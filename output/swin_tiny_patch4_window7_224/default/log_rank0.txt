[2024-12-02 11:38:26 swin_tiny_patch4_window7_224] (main.py 372): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 11:38:26 swin_tiny_patch4_window7_224] (main.py 373): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 11:38:26 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 11:38:26 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=1000, bias=True)
)
[2024-12-02 11:38:26 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1385002
[2024-12-02 11:38:26 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 11:38:26 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 11:38:26 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 11:38:26 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 11:44:06 swin_tiny_patch4_window7_224] (main.py 372): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 11:44:06 swin_tiny_patch4_window7_224] (main.py 373): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 11:44:06 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 11:44:06 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 11:44:06 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 11:44:06 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 11:44:06 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 11:44:06 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 11:44:06 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 11:49:00 swin_tiny_patch4_window7_224] (main.py 372): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 11:49:00 swin_tiny_patch4_window7_224] (main.py 373): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 11:49:00 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 11:49:00 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 11:49:00 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 11:49:00 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 11:49:00 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 11:49:00 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 11:49:00 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 11:51:36 swin_tiny_patch4_window7_224] (main.py 373): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 11:51:36 swin_tiny_patch4_window7_224] (main.py 374): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 11:51:36 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 11:51:36 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 11:51:36 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 11:51:36 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 11:51:36 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 11:51:36 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 11:51:36 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 11:53:44 swin_tiny_patch4_window7_224] (main.py 373): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 11:53:44 swin_tiny_patch4_window7_224] (main.py 374): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 11:53:44 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 11:53:44 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 11:53:44 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 11:53:44 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 11:53:44 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 11:53:44 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 11:53:44 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 11:57:19 swin_tiny_patch4_window7_224] (main.py 374): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 11:57:19 swin_tiny_patch4_window7_224] (main.py 375): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 11:57:19 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 11:57:19 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 11:57:19 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 11:57:19 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 11:57:19 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 11:57:19 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 11:57:19 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 11:57:48 swin_tiny_patch4_window7_224] (main.py 374): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 11:57:48 swin_tiny_patch4_window7_224] (main.py 375): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 11:57:48 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 11:57:48 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 11:57:48 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 11:57:48 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 11:57:48 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 11:57:48 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 11:57:48 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 11:59:17 swin_tiny_patch4_window7_224] (main.py 376): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 11:59:17 swin_tiny_patch4_window7_224] (main.py 377): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 11:59:17 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 11:59:17 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 11:59:17 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 11:59:17 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 11:59:17 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 11:59:17 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 11:59:17 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:01:21 swin_tiny_patch4_window7_224] (main.py 377): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:01:21 swin_tiny_patch4_window7_224] (main.py 378): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:01:21 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:01:21 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:01:21 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:01:21 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:01:21 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:01:21 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:01:21 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:04:04 swin_tiny_patch4_window7_224] (main.py 377): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:04:04 swin_tiny_patch4_window7_224] (main.py 378): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:04:04 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:04:04 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:04:04 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:04:04 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:04:04 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:04:04 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:04:04 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:04:35 swin_tiny_patch4_window7_224] (main.py 377): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:04:35 swin_tiny_patch4_window7_224] (main.py 378): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:04:35 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:04:35 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:04:35 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:04:35 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:04:35 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:04:35 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:04:35 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:05:19 swin_tiny_patch4_window7_224] (main.py 377): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:05:19 swin_tiny_patch4_window7_224] (main.py 378): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:05:19 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:05:19 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:05:19 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:05:19 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:05:19 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:05:19 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:05:19 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:07:08 swin_tiny_patch4_window7_224] (main.py 377): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:07:08 swin_tiny_patch4_window7_224] (main.py 378): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:07:08 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:07:08 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:07:08 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:07:08 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:07:08 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:07:08 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:07:08 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:07:31 swin_tiny_patch4_window7_224] (main.py 377): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:07:31 swin_tiny_patch4_window7_224] (main.py 378): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:07:31 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:07:31 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:07:31 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:07:31 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:07:31 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:07:31 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:07:31 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:07:45 swin_tiny_patch4_window7_224] (main.py 377): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:07:45 swin_tiny_patch4_window7_224] (main.py 378): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:07:45 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:07:45 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:07:45 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:07:45 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:07:45 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:07:45 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:07:45 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:08:29 swin_tiny_patch4_window7_224] (main.py 379): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:08:29 swin_tiny_patch4_window7_224] (main.py 380): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:08:29 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:08:29 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:08:29 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:08:29 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:08:29 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:08:29 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:08:29 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:09:08 swin_tiny_patch4_window7_224] (main.py 379): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:09:08 swin_tiny_patch4_window7_224] (main.py 380): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:09:08 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:09:08 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:09:08 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:09:08 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:09:08 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:09:08 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:09:08 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:09:40 swin_tiny_patch4_window7_224] (main.py 379): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:09:40 swin_tiny_patch4_window7_224] (main.py 380): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:09:40 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:09:40 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:09:40 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:09:40 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:09:40 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:09:40 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:09:40 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:12:54 swin_tiny_patch4_window7_224] (main.py 380): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:12:54 swin_tiny_patch4_window7_224] (main.py 381): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:12:54 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:12:54 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:12:54 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:12:54 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:12:54 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:12:54 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:12:54 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:15:22 swin_tiny_patch4_window7_224] (main.py 380): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:15:22 swin_tiny_patch4_window7_224] (main.py 381): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:15:22 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:15:22 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:15:22 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:15:22 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:15:22 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:15:22 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:15:22 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:17:38 swin_tiny_patch4_window7_224] (main.py 380): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:17:38 swin_tiny_patch4_window7_224] (main.py 381): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:17:38 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:17:38 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:17:38 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:17:38 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:17:38 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:17:38 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:17:38 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:17:39 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3154 (0.3154)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:17:41 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.2178 (0.1998)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:17:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.2686 (0.2007)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:17:45 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2460 (0.2180)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:17:46 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 0 training takes 0:00:07
[2024-12-02 12:18:50 swin_tiny_patch4_window7_224] (main.py 380): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:18:50 swin_tiny_patch4_window7_224] (main.py 381): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:18:50 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:18:50 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:18:50 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:18:50 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:18:50 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:18:50 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:18:50 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:18:50 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][0/34]	eta 0:00:09 lr 0.000000	 wd 0.0500	time 0.2705 (0.2705)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:18:52 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1765 (0.1869)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:18:54 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1732 (0.1817)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:18:55 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1673 (0.1796)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:18:56 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 0 training takes 0:00:06
[2024-12-02 12:20:05 swin_tiny_patch4_window7_224] (main.py 380): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:20:05 swin_tiny_patch4_window7_224] (main.py 381): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:20:05 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:20:05 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:20:05 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:20:05 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:20:05 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:20:05 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:20:05 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:20:05 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][0/34]	eta 0:00:08 lr 0.000000	 wd 0.0500	time 0.2639 (0.2639)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:20:07 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1684 (0.1933)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:20:09 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.2717 (0.1973)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:20:11 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2473 (0.2120)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:20:12 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 0 training takes 0:00:07
[2024-12-02 12:21:19 swin_tiny_patch4_window7_224] (main.py 380): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:21:19 swin_tiny_patch4_window7_224] (main.py 381): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:21:19 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:21:19 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:21:19 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:21:19 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:21:19 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:21:19 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:21:19 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:21:20 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][0/34]	eta 0:00:09 lr 0.000000	 wd 0.0500	time 0.2679 (0.2679)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:21:22 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.2087 (0.1937)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:21:24 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.2255 (0.2128)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:21:26 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1880 (0.2124)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:21:27 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 0 training takes 0:00:07
[2024-12-02 12:23:55 swin_tiny_patch4_window7_224] (main.py 382): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:23:55 swin_tiny_patch4_window7_224] (main.py 383): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:23:55 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:23:55 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:23:55 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:23:55 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:23:55 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:23:55 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:23:55 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:23:56 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3180 (0.3180)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:23:58 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.1862 (0.2198)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:24:00 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.2386 (0.2099)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:24:02 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2363 (0.2207)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:24:03 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 0 training takes 0:00:07
[2024-12-02 12:42:13 swin_tiny_patch4_window7_224] (main.py 382): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:42:13 swin_tiny_patch4_window7_224] (main.py 383): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:42:13 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:42:13 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:42:13 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:42:13 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:42:13 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:42:13 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:42:13 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:42:13 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3091 (0.3091)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:42:15 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1808 (0.1932)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:42:17 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1695 (0.1861)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:42:18 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1601 (0.1805)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:42:19 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 0 training takes 0:00:06
[2024-12-02 12:42:39 swin_tiny_patch4_window7_224] (main.py 382): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:42:39 swin_tiny_patch4_window7_224] (main.py 383): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:42:39 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:42:39 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:42:39 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:42:39 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:42:39 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:42:39 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:42:39 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:42:39 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3017 (0.3017)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:42:41 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1680 (0.1957)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:42:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1879 (0.1861)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:42:44 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1796 (0.1822)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:42:45 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 0 training takes 0:00:06
[2024-12-02 12:45:00 swin_tiny_patch4_window7_224] (main.py 382): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:45:00 swin_tiny_patch4_window7_224] (main.py 383): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:45:00 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:45:00 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:45:00 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:45:00 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:45:00 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:45:00 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:45:00 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:45:01 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][0/34]	eta 0:00:15 lr 0.000000	 wd 0.0500	time 0.4575 (0.4575)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:03 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.1717 (0.2234)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:05 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1634 (0.1990)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:06 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1882 (0.1927)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:07 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 0 training takes 0:00:06
[2024-12-02 12:45:28 swin_tiny_patch4_window7_224] (main.py 382): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:45:28 swin_tiny_patch4_window7_224] (main.py 383): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:45:28 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:45:28 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:45:28 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:45:28 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:45:28 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:45:28 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:45:28 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:45:28 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3026 (0.3026)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:30 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1721 (0.2030)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:32 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1620 (0.1872)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:34 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1674 (0.1866)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:34 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 0 training takes 0:00:06
[2024-12-02 12:45:35 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:45:35 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:45:35 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:45:35 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:45:36 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/300][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3455 (0.3455)	loss 1.3204 (1.3204)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:38 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/300][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.2387 (0.2397)	loss 1.2106 (1.1857)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:40 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/300][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.2119 (0.2417)	loss 1.3177 (1.1705)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:42 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2025 (0.2226)	loss 1.0078 (1.1956)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:43 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 1 training takes 0:00:07
[2024-12-02 12:45:44 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:45:44 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:45:44 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:45:44 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:45:44 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/300][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3129 (0.3129)	loss 0.9030 (0.9030)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:46 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/300][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.1865 (0.2097)	loss 1.3133 (1.1674)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:48 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1770 (0.1971)	loss 1.1163 (1.1561)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:49 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1798 (0.1885)	loss 1.3294 (1.1946)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:50 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 2 training takes 0:00:06
[2024-12-02 12:45:51 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:45:51 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:45:51 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:45:51 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:45:52 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/300][0/34]	eta 0:00:16 lr 0.000000	 wd 0.0500	time 0.4776 (0.4776)	loss 1.2180 (1.2180)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:54 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/300][10/34]	eta 0:00:06 lr 0.000000	 wd 0.0500	time 0.1760 (0.2638)	loss 0.8115 (1.1970)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:56 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/300][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1709 (0.2199)	loss 1.2076 (1.2151)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:58 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1808 (0.2068)	loss 1.2020 (1.1984)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:45:58 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 3 training takes 0:00:06
[2024-12-02 12:45:59 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:45:59 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:45:59 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:45:59 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:45:59 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/300][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3216 (0.3216)	loss 1.4127 (1.4127)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:01 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1844 (0.2060)	loss 1.1205 (1.1524)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:03 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1713 (0.1905)	loss 1.4133 (1.1858)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:05 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2636 (0.2010)	loss 1.0023 (1.1951)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:06 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 4 training takes 0:00:06
[2024-12-02 12:46:07 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:46:07 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:46:07 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:46:07 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:46:08 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/300][0/34]	eta 0:00:14 lr 0.000000	 wd 0.0500	time 0.4398 (0.4398)	loss 1.0050 (1.0050)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:10 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/300][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.1875 (0.2127)	loss 1.3082 (1.1669)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:12 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1748 (0.1926)	loss 1.4345 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:13 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1691 (0.1851)	loss 1.1244 (1.1977)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:14 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 5 training takes 0:00:06
[2024-12-02 12:46:15 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:46:15 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:46:15 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:46:15 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:46:15 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/300][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3208 (0.3208)	loss 1.2188 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:17 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/300][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.1810 (0.2091)	loss 0.8230 (1.1925)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:19 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/300][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.2686 (0.2208)	loss 1.2182 (1.1895)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:22 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1652 (0.2252)	loss 1.2178 (1.1813)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:22 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 6 training takes 0:00:07
[2024-12-02 12:46:23 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:46:23 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:46:23 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:46:23 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:46:23 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/300][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3549 (0.3549)	loss 1.0111 (1.0111)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:25 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1678 (0.2019)	loss 1.0072 (1.1654)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:27 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.2069 (0.1913)	loss 0.9054 (1.1399)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:29 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1727 (0.1859)	loss 1.2147 (1.1798)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:29 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 7 training takes 0:00:06
[2024-12-02 12:46:30 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:46:30 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:46:30 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:46:30 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:46:31 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/300][0/34]	eta 0:00:13 lr 0.000000	 wd 0.0500	time 0.3826 (0.3826)	loss 1.1182 (1.1182)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:33 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/300][10/34]	eta 0:00:06 lr 0.000000	 wd 0.0500	time 0.2633 (0.2697)	loss 1.0214 (1.2186)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:36 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/300][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1633 (0.2489)	loss 1.2093 (1.2085)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:37 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1705 (0.2229)	loss 1.0152 (1.1824)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:38 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 8 training takes 0:00:07
[2024-12-02 12:46:39 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:46:39 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:46:39 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:46:39 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:46:39 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/300][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3269 (0.3269)	loss 1.0264 (1.0264)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:41 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1760 (0.1993)	loss 1.3252 (1.1444)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1775 (0.1920)	loss 1.5173 (1.1804)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:45 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1684 (0.1854)	loss 1.5330 (1.1794)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:45 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 9 training takes 0:00:06
[2024-12-02 12:46:47 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:46:47 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:46:47 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:46:47 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:46:47 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/300][0/34]	eta 0:00:16 lr 0.000000	 wd 0.0500	time 0.4846 (0.4846)	loss 1.3284 (1.3284)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:49 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/300][10/34]	eta 0:00:06 lr 0.000000	 wd 0.0500	time 0.1696 (0.2661)	loss 1.1036 (1.1933)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:51 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/300][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1638 (0.2241)	loss 1.2124 (1.1887)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:53 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1805 (0.2083)	loss 1.1163 (1.1807)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:54 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 10 training takes 0:00:07
[2024-12-02 12:46:55 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:46:55 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:46:55 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:46:55 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:46:55 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/300][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3584 (0.3584)	loss 1.4163 (1.4163)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:57 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1778 (0.1934)	loss 0.8097 (1.2118)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:46:58 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1706 (0.1825)	loss 1.3154 (1.2331)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:01 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2678 (0.2007)	loss 0.7169 (1.1948)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:02 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 11 training takes 0:00:07
[2024-12-02 12:47:03 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:47:03 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:47:03 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:47:03 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:47:03 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [12/300][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3681 (0.3681)	loss 1.1152 (1.1152)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:05 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [12/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1887 (0.2067)	loss 1.3062 (1.2064)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:07 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [12/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1864 (0.1935)	loss 1.0982 (1.2008)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:09 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [12/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1659 (0.1881)	loss 1.4206 (1.1763)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:09 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 12 training takes 0:00:06
[2024-12-02 12:47:10 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:47:10 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:47:10 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:47:10 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:47:10 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [13/300][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3330 (0.3330)	loss 1.2212 (1.2212)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:12 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [13/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1899 (0.2080)	loss 1.2048 (1.1112)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:15 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [13/300][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.2862 (0.2268)	loss 1.3212 (1.1858)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:17 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [13/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1906 (0.2270)	loss 1.3129 (1.1824)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:18 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 13 training takes 0:00:07
[2024-12-02 12:47:19 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:47:19 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:47:19 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:47:19 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:47:19 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [14/300][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3552 (0.3552)	loss 1.3150 (1.3150)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:21 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [14/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1690 (0.2056)	loss 0.9079 (1.0414)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:23 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [14/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1661 (0.1875)	loss 1.5214 (1.1916)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:24 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [14/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1736 (0.1860)	loss 1.2292 (1.1842)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:25 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 14 training takes 0:00:06
[2024-12-02 12:47:26 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:47:26 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:47:26 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:47:26 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:47:26 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [15/300][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3530 (0.3530)	loss 0.8971 (0.8971)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:29 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [15/300][10/34]	eta 0:00:06 lr 0.000000	 wd 0.0500	time 0.2440 (0.2696)	loss 1.4212 (1.1306)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:31 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [15/300][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1711 (0.2396)	loss 1.3204 (1.1848)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:33 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [15/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1634 (0.2170)	loss 1.3373 (1.1890)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:33 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 15 training takes 0:00:07
[2024-12-02 12:47:34 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:47:34 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:47:34 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:47:34 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:47:35 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [16/300][0/34]	eta 0:00:13 lr 0.000000	 wd 0.0500	time 0.3867 (0.3867)	loss 1.0005 (1.0005)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:36 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [16/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1785 (0.2016)	loss 1.2136 (1.1193)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:38 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [16/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1748 (0.1928)	loss 0.8986 (1.1694)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:40 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [16/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2197 (0.1884)	loss 1.3288 (1.1791)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:41 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 16 training takes 0:00:06
[2024-12-02 12:47:42 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:47:42 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:47:42 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:47:42 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:47:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [17/300][0/34]	eta 0:00:16 lr 0.000000	 wd 0.0500	time 0.4970 (0.4970)	loss 1.4139 (1.4139)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:45 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [17/300][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.1757 (0.2327)	loss 1.3061 (1.2603)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:46 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [17/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.2002 (0.2085)	loss 1.2102 (1.1890)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:48 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [17/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1857 (0.1995)	loss 1.0017 (1.1820)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:49 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 17 training takes 0:00:06
[2024-12-02 12:47:50 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:47:50 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:47:50 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:47:50 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:47:50 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [18/300][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3379 (0.3379)	loss 1.3049 (1.3049)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:52 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [18/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1685 (0.2020)	loss 1.1154 (1.1879)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:54 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [18/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.2365 (0.1982)	loss 1.4091 (1.1868)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:57 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [18/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2319 (0.2161)	loss 1.0122 (1.1849)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:47:57 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 18 training takes 0:00:07
[2024-12-02 12:47:58 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:47:58 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:47:58 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:47:58 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:47:58 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [19/300][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3316 (0.3316)	loss 1.0286 (1.0286)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:00 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [19/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1746 (0.1940)	loss 1.1265 (1.2362)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:02 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [19/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1845 (0.1892)	loss 0.9085 (1.1835)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:04 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [19/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1798 (0.1864)	loss 1.2118 (1.1933)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:04 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 19 training takes 0:00:06
[2024-12-02 12:48:05 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:48:05 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:48:05 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:48:05 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:48:06 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [20/300][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3521 (0.3521)	loss 1.3092 (1.3092)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:08 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [20/300][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.2387 (0.2173)	loss 1.3186 (1.2343)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:10 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [20/300][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.2540 (0.2314)	loss 0.9077 (1.2009)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:12 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [20/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1921 (0.2163)	loss 1.1030 (1.2027)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:13 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 20 training takes 0:00:07
[2024-12-02 12:48:14 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:48:14 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:48:14 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:48:14 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:48:14 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [21/300][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3681 (0.3681)	loss 1.0226 (1.0226)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:16 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [21/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1653 (0.2021)	loss 1.2127 (1.1962)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:18 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [21/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1773 (0.1975)	loss 1.6450 (1.1826)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:19 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [21/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1601 (0.1905)	loss 1.4221 (1.1872)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:20 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 21 training takes 0:00:06
[2024-12-02 12:48:21 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:48:21 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:48:21 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:48:21 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:48:22 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [22/300][0/34]	eta 0:00:17 lr 0.000000	 wd 0.0500	time 0.5173 (0.5173)	loss 1.4136 (1.4136)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:24 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [22/300][10/34]	eta 0:00:06 lr 0.000000	 wd 0.0500	time 0.1859 (0.2666)	loss 1.1175 (1.1990)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:26 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [22/300][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1642 (0.2230)	loss 1.4164 (1.2163)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:28 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [22/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1810 (0.2061)	loss 1.3064 (1.1859)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:28 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 22 training takes 0:00:06
[2024-12-02 12:48:29 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:48:29 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:48:29 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:48:29 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:48:30 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [23/300][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3571 (0.3571)	loss 1.1186 (1.1186)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:32 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [23/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1744 (0.2072)	loss 0.8045 (1.1530)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:33 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [23/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1803 (0.1892)	loss 1.4216 (1.1972)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:36 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [23/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2296 (0.2039)	loss 1.1113 (1.1729)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:36 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 23 training takes 0:00:07
[2024-12-02 12:48:38 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:48:38 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:48:38 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:48:38 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:48:38 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [24/300][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3379 (0.3379)	loss 1.2105 (1.2105)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:40 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [24/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1875 (0.1981)	loss 1.1162 (1.1744)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:42 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [24/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1951 (0.1926)	loss 1.2089 (1.1896)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [24/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1648 (0.1876)	loss 1.3081 (1.2048)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:44 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 24 training takes 0:00:06
[2024-12-02 12:48:45 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:48:45 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:48:45 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:48:45 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:48:45 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [25/300][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3740 (0.3740)	loss 1.2229 (1.2229)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:47 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [25/300][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.1872 (0.2151)	loss 1.5144 (1.2567)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:50 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [25/300][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.2475 (0.2294)	loss 1.1144 (1.1926)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:52 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [25/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1905 (0.2219)	loss 1.3066 (1.1866)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:52 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 25 training takes 0:00:07
[2024-12-02 12:48:53 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:48:53 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:48:53 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:48:53 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:48:54 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [26/300][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3412 (0.3412)	loss 1.3199 (1.3199)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:56 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [26/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1590 (0.1921)	loss 1.0294 (1.1535)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:57 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [26/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1861 (0.1890)	loss 1.2143 (1.1770)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:48:59 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [26/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1702 (0.1855)	loss 1.0025 (1.1751)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:00 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 26 training takes 0:00:06
[2024-12-02 12:49:01 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:49:01 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:49:01 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:49:01 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:49:01 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [27/300][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3697 (0.3697)	loss 1.4119 (1.4119)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:04 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [27/300][10/34]	eta 0:00:06 lr 0.000000	 wd 0.0500	time 0.2708 (0.2711)	loss 1.5333 (1.1610)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:06 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [27/300][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1881 (0.2379)	loss 1.4040 (1.1913)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:07 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [27/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1702 (0.2192)	loss 1.4108 (1.1895)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:08 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 27 training takes 0:00:07
[2024-12-02 12:49:09 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:49:09 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:49:09 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:49:09 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:49:09 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [28/300][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3448 (0.3448)	loss 1.3144 (1.3144)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:11 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [28/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1744 (0.2005)	loss 1.2006 (1.1132)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:13 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [28/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1611 (0.1866)	loss 1.4167 (1.1531)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:15 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [28/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2476 (0.1920)	loss 1.1021 (1.1701)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:16 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 28 training takes 0:00:06
[2024-12-02 12:49:17 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:49:17 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:49:17 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:49:17 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:49:17 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [29/300][0/34]	eta 0:00:16 lr 0.000000	 wd 0.0500	time 0.4965 (0.4965)	loss 1.3227 (1.3227)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:19 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [29/300][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.1971 (0.2274)	loss 1.0086 (1.2451)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:21 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [29/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1687 (0.1997)	loss 1.0040 (1.2303)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:23 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [29/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1656 (0.1927)	loss 1.2243 (1.2024)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:23 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 29 training takes 0:00:06
[2024-12-02 12:49:25 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:49:25 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:49:25 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:49:25 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:49:25 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [30/300][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3316 (0.3316)	loss 1.1026 (1.1026)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:27 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [30/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1858 (0.2067)	loss 1.1117 (1.1223)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:29 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [30/300][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.2450 (0.2186)	loss 1.1088 (1.1668)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:32 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [30/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1983 (0.2260)	loss 1.4201 (1.1742)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:32 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 30 training takes 0:00:07
[2024-12-02 12:49:33 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:49:33 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:49:33 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:49:33 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:49:33 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [31/300][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3186 (0.3186)	loss 1.3124 (1.3124)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:35 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [31/300][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1933 (0.2050)	loss 1.3106 (1.2149)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:37 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [31/300][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1647 (0.1872)	loss 1.2093 (1.1914)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:39 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [31/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1830 (0.1855)	loss 0.7156 (1.1797)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:39 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 31 training takes 0:00:06
[2024-12-02 12:49:40 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:49:40 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:49:40 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:49:40 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:49:41 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [32/300][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3122 (0.3122)	loss 1.0058 (1.0058)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [32/300][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.2407 (0.2498)	loss 1.0125 (1.2167)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:46 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [32/300][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1893 (0.2482)	loss 1.0123 (1.1377)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:48 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [32/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1982 (0.2271)	loss 1.4166 (1.1854)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:48 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 32 training takes 0:00:07
[2024-12-02 12:49:49 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:49:49 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:49:49 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:49:49 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:49:49 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [33/300][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3593 (0.3593)	loss 1.1313 (1.1313)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:49:51 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [33/300][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.1883 (0.2095)	loss 0.6049 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:50:19 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [36/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1911 (0.1904)	loss 1.0131 (1.1773)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:50:20 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 36 training takes 0:00:06
[2024-12-02 12:50:21 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:50:21 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:50:21 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:50:21 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:50:21 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [37/300][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3289 (0.3289)	loss 1.3151 (1.3151)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:50:23 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [37/300][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.2409 (0.2201)	loss 1.1040 (1.2517)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:50:26 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [37/300][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.2192 (0.2264)	loss 0.7092 (1.2056)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:50:28 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [37/300][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1744 (0.2180)	loss 1.1129 (1.1746)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:50:28 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 37 training takes 0:00:07
[2024-12-02 12:55:43 swin_tiny_patch4_window7_224] (main.py 382): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:55:43 swin_tiny_patch4_window7_224] (main.py 383): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:55:43 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:55:43 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:55:43 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:55:43 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:55:43 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:55:43 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:55:43 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:55:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3092 (0.3092)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:55:45 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1838 (0.1922)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:55:46 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1690 (0.1804)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:55:48 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1626 (0.1760)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:55:49 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 0 training takes 0:00:06
[2024-12-02 12:55:50 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:55:50 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:55:50 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:55:50 swin_tiny_patch4_window7_224] (main.py 160): INFO config : AMP_ENABLE: False
AMP_OPT_LEVEL: 
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: no
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: True
  ZIP_MODE: True
ENABLE_AMP: False
EVAL_MODE: False
FUSED_LAYERNORM: False
FUSED_WINDOW_PROCESS: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: 
  RESUME: 
  SIMMIM:
    NORM_TARGET:
      ENABLE: False
      PATCH_SIZE: 47
  SWIN:
    APE: False
    DEPTHS: [2, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: False
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: False
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: False
    MLP_FC2_BIAS: True
    MLP_RATIO: 4.0
    MOE_BLOCKS: [[-1], [-1], [-1], [-1]]
    MOE_DROP: 0.0
    NORMALIZE_GATE: False
    NUM_HEADS: [3, 6, 12, 24]
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: True
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    QK_SCALE: None
    TOP_VALUE: 1
    USE_BPR: True
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
  SHUFFLE: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: True
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: False
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05, ignoring auto resume
[2024-12-02 12:55:50 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][0/34]	eta 0:00:14 lr 0.000000	 wd 0.0500	time 0.4290 (0.4290)	loss 1.3204 (1.3204)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:56:48 swin_tiny_patch4_window7_224] (main.py 382): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 12:56:48 swin_tiny_patch4_window7_224] (main.py 383): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 12:56:48 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 12:56:48 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 12:56:48 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 12:56:48 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 12:56:48 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 12:56:48 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 12:56:49 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3402 (0.3402)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:56:51 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.2671 (0.2097)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:56:53 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.2432 (0.2224)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:56:55 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1664 (0.2191)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:56:56 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 0 training takes 0:00:07
[2024-12-02 12:56:57 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:56:57 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:56:57 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:56:57 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3233 (0.3233)	loss 1.3204 (1.3204)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:56:59 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1869 (0.2017)	loss 1.2106 (1.1857)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:00 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1645 (0.1853)	loss 1.3177 (1.1705)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:02 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1616 (0.1788)	loss 1.0078 (1.1956)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:03 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 1 training takes 0:00:06
[2024-12-02 12:57:04 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:57:04 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:57:04 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:57:04 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3614 (0.3614)	loss 0.9030 (0.9030)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:07 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][10/34]	eta 0:00:06 lr 0.000000	 wd 0.0500	time 0.2489 (0.2734)	loss 1.3133 (1.1674)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:09 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1713 (0.2369)	loss 1.1163 (1.1561)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:10 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1915 (0.2159)	loss 1.3294 (1.1946)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:11 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 2 training takes 0:00:07
[2024-12-02 12:57:12 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:57:12 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:57:12 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:57:12 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3247 (0.3247)	loss 1.2180 (1.2180)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:14 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1835 (0.2027)	loss 0.8115 (1.1970)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:16 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1724 (0.1930)	loss 1.2076 (1.2151)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:18 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1777 (0.1901)	loss 1.2020 (1.1984)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:18 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 3 training takes 0:00:06
[2024-12-02 12:57:20 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:57:20 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:57:20 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:57:20 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][0/34]	eta 0:00:16 lr 0.000000	 wd 0.0500	time 0.4940 (0.4940)	loss 1.4127 (1.4127)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:22 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.1575 (0.2419)	loss 1.1205 (1.1524)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:24 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1868 (0.2076)	loss 1.4133 (1.1858)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:26 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1721 (0.1941)	loss 1.0023 (1.1951)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:26 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 4 training takes 0:00:06
[2024-12-02 12:57:27 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:57:27 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:57:27 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:57:27 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3145 (0.3145)	loss 1.0050 (1.0050)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:29 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1632 (0.1976)	loss 1.3082 (1.1669)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:31 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1787 (0.1865)	loss 1.4345 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:33 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2486 (0.2015)	loss 1.1244 (1.1977)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:34 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 5 training takes 0:00:06
[2024-12-02 12:57:35 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:57:35 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:57:35 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:57:36 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3546 (0.3546)	loss 1.2188 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:37 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1720 (0.1960)	loss 0.8230 (1.1925)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:39 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1713 (0.1808)	loss 1.2182 (1.1895)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:41 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1704 (0.1788)	loss 1.2178 (1.1813)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:41 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 6 training takes 0:00:06
[2024-12-02 12:57:42 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:57:42 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:57:42 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:57:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3436 (0.3436)	loss 1.0111 (1.0111)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:44 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1728 (0.1938)	loss 1.0072 (1.1654)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:47 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.2595 (0.1996)	loss 0.9054 (1.1399)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:49 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2187 (0.2144)	loss 1.2147 (1.1798)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:50 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 7 training takes 0:00:07
[2024-12-02 12:57:50 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:57:50 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:57:50 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:57:51 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3604 (0.3604)	loss 1.1182 (1.1182)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:53 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.1920 (0.2116)	loss 1.0214 (1.2186)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:55 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1770 (0.1984)	loss 1.2093 (1.2085)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:56 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1779 (0.1925)	loss 1.0152 (1.1824)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:57:57 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 8 training takes 0:00:06
[2024-12-02 12:57:58 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:57:58 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:57:58 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:57:58 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3339 (0.3339)	loss 1.0264 (1.0264)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:01 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.2502 (0.2489)	loss 1.3252 (1.1444)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:03 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.2158 (0.2469)	loss 1.5173 (1.1804)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:05 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1826 (0.2245)	loss 1.5330 (1.1794)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:05 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 9 training takes 0:00:07
[2024-12-02 12:58:06 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:58:06 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:58:06 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:58:07 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3242 (0.3242)	loss 1.3284 (1.3284)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:09 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1778 (0.1973)	loss 1.1036 (1.1933)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:10 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1568 (0.1827)	loss 1.2124 (1.1887)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:12 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1765 (0.1786)	loss 1.1163 (1.1807)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:12 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 10 training takes 0:00:06
[2024-12-02 12:58:14 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:58:14 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:58:14 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:58:14 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][0/34]	eta 0:00:15 lr 0.000000	 wd 0.0500	time 0.4682 (0.4682)	loss 1.4163 (1.4163)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:17 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][10/34]	eta 0:00:06 lr 0.000000	 wd 0.0500	time 0.2189 (0.2739)	loss 0.8097 (1.2118)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:18 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1819 (0.2317)	loss 1.3154 (1.2331)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:20 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1900 (0.2141)	loss 0.7169 (1.1948)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:21 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 11 training takes 0:00:07
[2024-12-02 12:58:22 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:58:22 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:58:22 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:58:22 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [12/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3428 (0.3428)	loss 1.1152 (1.1152)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:24 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [12/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1815 (0.2025)	loss 1.3062 (1.2064)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:26 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [12/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1708 (0.1870)	loss 1.0982 (1.2008)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:28 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [12/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2488 (0.1919)	loss 1.4206 (1.1763)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:29 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 12 training takes 0:00:06
[2024-12-02 12:58:30 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:58:30 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:58:30 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:58:30 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [13/50][0/34]	eta 0:00:15 lr 0.000000	 wd 0.0500	time 0.4652 (0.4652)	loss 1.2212 (1.2212)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:32 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [13/50][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.1941 (0.2099)	loss 1.2048 (1.1112)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:34 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [13/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1897 (0.1945)	loss 1.3212 (1.1858)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:36 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [13/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1628 (0.1862)	loss 1.3129 (1.1824)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:36 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 13 training takes 0:00:06
[2024-12-02 12:58:37 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:58:37 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:58:37 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:58:37 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [14/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3278 (0.3278)	loss 1.3150 (1.3150)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:39 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [14/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1770 (0.1998)	loss 0.9079 (1.0414)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:41 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [14/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.2521 (0.2047)	loss 1.5214 (1.1916)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:44 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [14/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2076 (0.2157)	loss 1.2292 (1.1842)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:44 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 14 training takes 0:00:07
[2024-12-02 12:58:45 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:58:45 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:58:45 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:58:46 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [15/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3294 (0.3294)	loss 0.8971 (0.8971)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:48 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [15/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1831 (0.2068)	loss 1.4212 (1.1306)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:49 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [15/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1953 (0.1935)	loss 1.3204 (1.1848)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:51 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [15/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1826 (0.1875)	loss 1.3373 (1.1890)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:52 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 15 training takes 0:00:06
[2024-12-02 12:58:53 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:58:53 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:58:53 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:58:53 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [16/50][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3689 (0.3689)	loss 1.0005 (1.0005)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:55 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [16/50][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.2606 (0.2442)	loss 1.2136 (1.1193)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:58:58 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [16/50][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.2396 (0.2457)	loss 0.8986 (1.1694)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:00 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [16/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1623 (0.2233)	loss 1.3288 (1.1791)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:00 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 16 training takes 0:00:07
[2024-12-02 12:59:01 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:59:01 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:59:01 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:59:01 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [17/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3503 (0.3503)	loss 1.4139 (1.4139)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:03 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [17/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1865 (0.2029)	loss 1.3061 (1.2603)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:05 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [17/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1678 (0.1889)	loss 1.2102 (1.1890)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:07 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [17/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1752 (0.1851)	loss 1.0017 (1.1820)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:07 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 17 training takes 0:00:06
[2024-12-02 12:59:08 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:59:08 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:59:08 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:59:09 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [18/50][0/34]	eta 0:00:16 lr 0.000000	 wd 0.0500	time 0.4823 (0.4823)	loss 1.3049 (1.3049)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:11 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [18/50][10/34]	eta 0:00:06 lr 0.000000	 wd 0.0500	time 0.2393 (0.2759)	loss 1.1154 (1.1879)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:13 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [18/50][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1617 (0.2292)	loss 1.4091 (1.1868)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:15 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [18/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1645 (0.2100)	loss 1.0122 (1.1849)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:15 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 18 training takes 0:00:07
[2024-12-02 12:59:16 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:59:16 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:59:16 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:59:17 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [19/50][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3608 (0.3608)	loss 1.0286 (1.0286)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:19 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [19/50][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.1643 (0.2109)	loss 1.1265 (1.2362)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:21 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [19/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1724 (0.1969)	loss 0.9085 (1.1835)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:23 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [19/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2402 (0.1990)	loss 1.2118 (1.1933)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:23 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 19 training takes 0:00:06
[2024-12-02 12:59:25 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:59:25 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:59:25 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:59:25 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [20/50][0/34]	eta 0:00:17 lr 0.000000	 wd 0.0500	time 0.5050 (0.5050)	loss 1.3092 (1.3092)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:27 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [20/50][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.1707 (0.2193)	loss 1.3186 (1.2343)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:29 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [20/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1791 (0.2014)	loss 0.9077 (1.2009)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:31 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [20/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1793 (0.1931)	loss 1.1030 (1.2027)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:31 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 20 training takes 0:00:06
[2024-12-02 12:59:32 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:59:32 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:59:32 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:59:33 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [21/50][0/34]	eta 0:00:13 lr 0.000000	 wd 0.0500	time 0.3844 (0.3844)	loss 1.0226 (1.0226)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:34 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [21/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1632 (0.1961)	loss 1.2127 (1.1962)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:36 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [21/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.2471 (0.1989)	loss 1.6450 (1.1826)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:39 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [21/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2608 (0.2135)	loss 1.4221 (1.1872)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:40 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 21 training takes 0:00:07
[2024-12-02 12:59:41 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:59:41 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:59:41 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:59:41 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [22/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3214 (0.3214)	loss 1.4136 (1.4136)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [22/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1616 (0.1926)	loss 1.1175 (1.1990)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:44 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [22/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1714 (0.1799)	loss 1.4164 (1.2163)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:46 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [22/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1746 (0.1760)	loss 1.3064 (1.1859)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:47 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 22 training takes 0:00:05
[2024-12-02 12:59:47 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:59:47 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:59:47 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:59:48 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [23/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3140 (0.3140)	loss 1.1186 (1.1186)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:50 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [23/50][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.2576 (0.2277)	loss 0.8045 (1.1530)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:52 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [23/50][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.2456 (0.2358)	loss 1.4216 (1.1972)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:54 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [23/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1895 (0.2231)	loss 1.1113 (1.1729)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:55 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 23 training takes 0:00:07
[2024-12-02 12:59:56 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 12:59:56 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 12:59:56 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 12:59:56 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [24/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3130 (0.3130)	loss 1.2105 (1.2105)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 12:59:58 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [24/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1950 (0.2009)	loss 1.1162 (1.1744)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:00 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [24/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.2355 (0.2086)	loss 1.2089 (1.1896)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:03 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [24/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2333 (0.2134)	loss 1.3081 (1.2048)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:03 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 24 training takes 0:00:07
[2024-12-02 13:00:05 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:00:05 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:00:05 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:00:05 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [25/50][0/34]	eta 0:00:16 lr 0.000000	 wd 0.0500	time 0.4725 (0.4725)	loss 1.2229 (1.2229)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:07 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [25/50][10/34]	eta 0:00:06 lr 0.000000	 wd 0.0500	time 0.1703 (0.2693)	loss 1.5144 (1.2567)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:09 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [25/50][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1853 (0.2222)	loss 1.1144 (1.1926)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:11 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [25/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1776 (0.2089)	loss 1.3066 (1.1866)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:12 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 25 training takes 0:00:07
[2024-12-02 13:00:12 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:00:12 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:00:12 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:00:13 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [26/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3452 (0.3452)	loss 1.3199 (1.3199)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:15 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [26/50][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.1739 (0.2102)	loss 1.0294 (1.1535)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:17 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [26/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1851 (0.1933)	loss 1.2143 (1.1770)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:19 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [26/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2425 (0.2030)	loss 1.0025 (1.1751)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:20 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 26 training takes 0:00:07
[2024-12-02 13:00:21 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:00:21 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:00:21 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:00:21 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [27/50][0/34]	eta 0:00:15 lr 0.000000	 wd 0.0500	time 0.4468 (0.4468)	loss 1.4119 (1.4119)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:23 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [27/50][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.1671 (0.2090)	loss 1.5333 (1.1610)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:25 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [27/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1590 (0.1882)	loss 1.4040 (1.1913)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:26 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [27/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1608 (0.1819)	loss 1.4108 (1.1895)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:27 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 27 training takes 0:00:06
[2024-12-02 13:00:28 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:00:28 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:00:28 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:00:28 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [28/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3220 (0.3220)	loss 1.3144 (1.3144)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:30 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [28/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1823 (0.1910)	loss 1.2006 (1.1132)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:32 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [28/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.2400 (0.1963)	loss 1.4167 (1.1531)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:34 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [28/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2360 (0.2099)	loss 1.1021 (1.1701)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:35 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 28 training takes 0:00:07
[2024-12-02 13:00:36 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:00:36 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:00:36 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:00:36 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [29/50][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3642 (0.3642)	loss 1.3227 (1.3227)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:38 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [29/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1675 (0.2057)	loss 1.0086 (1.2451)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:40 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [29/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1827 (0.2051)	loss 1.0040 (1.2303)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:42 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [29/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1834 (0.2009)	loss 1.2243 (1.2024)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:43 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 29 training takes 0:00:06
[2024-12-02 13:00:44 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:00:44 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:00:44 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:00:44 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [30/50][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3651 (0.3651)	loss 1.1026 (1.1026)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:48 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [30/50][10/34]	eta 0:00:08 lr 0.000000	 wd 0.0500	time 0.2559 (0.3428)	loss 1.1117 (1.1223)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:50 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [30/50][20/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1816 (0.3120)	loss 1.1088 (1.1668)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:52 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [30/50][30/34]	eta 0:00:01 lr 0.000000	 wd 0.0500	time 0.1657 (0.2674)	loss 1.4201 (1.1742)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:00:53 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 30 training takes 0:00:08
[2024-12-02 13:00:54 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:00:54 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:00:54 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:00:54 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [31/50][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3600 (0.3600)	loss 1.3124 (1.3124)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:16 swin_tiny_patch4_window7_224] (main.py 382): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.5625e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2024-12-02 13:01:16 swin_tiny_patch4_window7_224] (main.py 383): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 13:01:16 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 13:01:16 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 13:01:16 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 13:01:16 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 13:01:16 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 13:01:16 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 13:01:16 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][0/34]	eta 0:00:08 lr 0.000000	 wd 0.0500	time 0.2579 (0.2579)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:18 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1574 (0.1791)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:20 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1579 (0.1767)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:22 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1651 (0.1769)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:22 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 0 training takes 0:00:05
[2024-12-02 13:01:23 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:01:23 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:01:23 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:01:23 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3088 (0.3088)	loss 1.3204 (1.3204)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:26 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.2477 (0.2391)	loss 1.2106 (1.1857)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:28 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1643 (0.2335)	loss 1.3177 (1.1705)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:30 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1722 (0.2116)	loss 1.0078 (1.1956)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:30 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 1 training takes 0:00:07
[2024-12-02 13:01:31 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:01:31 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:01:31 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:01:31 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3103 (0.3103)	loss 0.9030 (0.9030)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:33 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1635 (0.1990)	loss 1.3133 (1.1674)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:35 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1597 (0.1873)	loss 1.1163 (1.1561)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:37 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1643 (0.1789)	loss 1.3294 (1.1946)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:37 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 2 training takes 0:00:06
[2024-12-02 13:01:38 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:01:38 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:01:38 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:01:39 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][0/34]	eta 0:00:16 lr 0.000000	 wd 0.0500	time 0.4938 (0.4938)	loss 1.2180 (1.2180)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:41 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][10/34]	eta 0:00:06 lr 0.000000	 wd 0.0500	time 0.2360 (0.2790)	loss 0.8115 (1.1970)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1642 (0.2273)	loss 1.2076 (1.2151)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:45 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1612 (0.2059)	loss 1.2020 (1.1984)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:45 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 3 training takes 0:00:06
[2024-12-02 13:01:46 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:01:46 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:01:46 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:01:46 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3595 (0.3595)	loss 1.4127 (1.4127)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:48 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1669 (0.1906)	loss 1.1205 (1.1524)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:50 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1600 (0.1772)	loss 1.4133 (1.1858)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:52 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2474 (0.1788)	loss 1.0023 (1.1951)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:52 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 4 training takes 0:00:06
[2024-12-02 13:01:54 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:01:54 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:01:54 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:01:54 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][0/34]	eta 0:00:14 lr 0.000000	 wd 0.0500	time 0.4377 (0.4377)	loss 1.0050 (1.0050)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:56 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.1594 (0.2097)	loss 1.3082 (1.1669)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:58 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1945 (0.1920)	loss 1.4345 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:01:59 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1629 (0.1875)	loss 1.1244 (1.1977)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:00 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 5 training takes 0:00:06
[2024-12-02 13:02:01 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:02:01 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:02:01 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:02:01 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3098 (0.3098)	loss 1.2188 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:03 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1867 (0.1943)	loss 0.8230 (1.1925)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:05 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1566 (0.1836)	loss 1.2182 (1.1895)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:07 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2405 (0.2008)	loss 1.2178 (1.1813)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:08 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 6 training takes 0:00:06
[2024-12-02 13:02:09 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:02:09 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:02:09 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:02:09 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3168 (0.3168)	loss 1.0111 (1.0111)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:11 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1810 (0.1961)	loss 1.0072 (1.1654)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:13 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1738 (0.1829)	loss 0.9054 (1.1399)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:14 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1693 (0.1814)	loss 1.2147 (1.1798)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:15 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 7 training takes 0:00:06
[2024-12-02 13:02:16 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:02:16 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:02:16 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:02:16 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3416 (0.3416)	loss 1.1182 (1.1182)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:18 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1727 (0.1941)	loss 1.0214 (1.2186)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:20 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.2345 (0.2147)	loss 1.2093 (1.2085)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:23 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1617 (0.2135)	loss 1.0152 (1.1824)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:23 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 8 training takes 0:00:07
[2024-12-02 13:02:24 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:02:24 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:02:24 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:02:24 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3422 (0.3422)	loss 1.0264 (1.0264)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:26 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1580 (0.1880)	loss 1.3252 (1.1444)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:28 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1799 (0.1806)	loss 1.5173 (1.1804)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:30 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1695 (0.1786)	loss 1.5330 (1.1794)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:30 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 9 training takes 0:00:06
[2024-12-02 13:02:31 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:02:31 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:02:31 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:02:31 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3106 (0.3106)	loss 1.3284 (1.3284)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:34 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.2535 (0.2464)	loss 1.1036 (1.1933)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:36 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1626 (0.2375)	loss 1.2124 (1.1887)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:38 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1631 (0.2144)	loss 1.1163 (1.1807)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:38 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 10 training takes 0:00:07
[2024-12-02 13:02:39 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:02:39 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:02:39 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:02:40 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3495 (0.3495)	loss 1.4163 (1.4163)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:42 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1609 (0.2012)	loss 0.8097 (1.2118)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1831 (0.1913)	loss 1.3154 (1.2331)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:45 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1656 (0.1826)	loss 0.7169 (1.1948)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:46 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 11 training takes 0:00:06
[2024-12-02 13:02:47 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:02:47 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:02:47 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:02:47 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [12/50][0/34]	eta 0:00:18 lr 0.000000	 wd 0.0500	time 0.5361 (0.5361)	loss 1.1152 (1.1152)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:50 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [12/50][10/34]	eta 0:00:06 lr 0.000000	 wd 0.0500	time 0.1590 (0.2588)	loss 1.3062 (1.2064)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:51 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [12/50][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1657 (0.2158)	loss 1.0982 (1.2008)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:53 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [12/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1763 (0.2008)	loss 1.4206 (1.1763)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:54 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 12 training takes 0:00:06
[2024-12-02 13:02:55 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:02:55 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:02:55 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:02:55 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [13/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.2998 (0.2998)	loss 1.2212 (1.2212)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:57 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [13/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1710 (0.1847)	loss 1.2048 (1.1112)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:02:58 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [13/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1746 (0.1834)	loss 1.3212 (1.1858)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:00 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [13/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2366 (0.1888)	loss 1.3129 (1.1824)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:01 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 13 training takes 0:00:06
[2024-12-02 13:03:02 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:03:02 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:03:02 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:03:03 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [14/50][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3617 (0.3617)	loss 1.3150 (1.3150)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:05 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [14/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1725 (0.2020)	loss 0.9079 (1.0414)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:06 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [14/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1762 (0.1891)	loss 1.5214 (1.1916)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:08 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [14/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1648 (0.1868)	loss 1.2292 (1.1842)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:09 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 14 training takes 0:00:06
[2024-12-02 13:03:10 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:03:10 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:03:10 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:03:10 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [15/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3041 (0.3041)	loss 0.8971 (0.8971)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:12 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [15/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1767 (0.1887)	loss 1.4212 (1.1306)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:14 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [15/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.2766 (0.1965)	loss 1.3204 (1.1848)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:16 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [15/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2503 (0.2092)	loss 1.3373 (1.1890)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:17 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 15 training takes 0:00:07
[2024-12-02 13:03:18 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:03:18 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:03:18 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:03:18 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [16/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3064 (0.3064)	loss 1.0005 (1.0005)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:20 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [16/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1736 (0.2055)	loss 1.2136 (1.1193)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:22 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [16/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1728 (0.1951)	loss 0.8986 (1.1694)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:24 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [16/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1646 (0.1889)	loss 1.3288 (1.1791)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:24 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 16 training takes 0:00:06
[2024-12-02 13:03:25 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:03:25 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:03:25 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:03:25 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [17/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3477 (0.3477)	loss 1.4139 (1.4139)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:28 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [17/50][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.2747 (0.2300)	loss 1.3061 (1.2603)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:30 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [17/50][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1661 (0.2358)	loss 1.2102 (1.1890)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:32 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [17/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1901 (0.2169)	loss 1.0017 (1.1820)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:32 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 17 training takes 0:00:07
[2024-12-02 13:03:33 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:03:33 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:03:33 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:03:34 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [18/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3026 (0.3026)	loss 1.3049 (1.3049)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:35 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [18/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1662 (0.1855)	loss 1.1154 (1.1879)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:37 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [18/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1846 (0.1811)	loss 1.4091 (1.1868)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:39 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [18/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1845 (0.1794)	loss 1.0122 (1.1849)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:39 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 18 training takes 0:00:06
[2024-12-02 13:03:41 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:03:41 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:03:41 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:03:41 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [19/50][0/34]	eta 0:00:16 lr 0.000000	 wd 0.0500	time 0.4812 (0.4812)	loss 1.0286 (1.0286)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [19/50][10/34]	eta 0:00:06 lr 0.000000	 wd 0.0500	time 0.1682 (0.2579)	loss 1.1265 (1.2362)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:45 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [19/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1610 (0.2130)	loss 0.9085 (1.1835)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:47 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [19/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1661 (0.1997)	loss 1.2118 (1.1933)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:47 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 19 training takes 0:00:06
[2024-12-02 13:03:48 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:03:48 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:03:48 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:03:48 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [20/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3040 (0.3040)	loss 1.3092 (1.3092)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:50 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [20/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1547 (0.1833)	loss 1.3186 (1.2343)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:52 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [20/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1605 (0.1745)	loss 0.9077 (1.2009)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:54 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [20/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2315 (0.1736)	loss 1.1030 (1.2027)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:54 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 20 training takes 0:00:06
[2024-12-02 13:03:56 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:03:56 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:03:56 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:03:56 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [21/50][0/34]	eta 0:00:14 lr 0.000000	 wd 0.0500	time 0.4247 (0.4247)	loss 1.0226 (1.0226)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:03:58 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [21/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1613 (0.2052)	loss 1.2127 (1.1962)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:00 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [21/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1654 (0.1894)	loss 1.6450 (1.1826)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:01 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [21/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1746 (0.1818)	loss 1.4221 (1.1872)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:02 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 21 training takes 0:00:06
[2024-12-02 13:04:03 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:04:03 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:04:03 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:04:03 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [22/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3003 (0.3003)	loss 1.4136 (1.4136)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:05 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [22/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1691 (0.2002)	loss 1.1175 (1.1990)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:07 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [22/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.2190 (0.1905)	loss 1.4164 (1.2163)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:09 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [22/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2067 (0.2032)	loss 1.3064 (1.1859)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:10 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 22 training takes 0:00:07
[2024-12-02 13:04:11 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:04:11 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:04:11 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:04:11 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [23/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3426 (0.3426)	loss 1.1186 (1.1186)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:13 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [23/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1636 (0.1879)	loss 0.8045 (1.1530)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:14 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [23/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1572 (0.1758)	loss 1.4216 (1.1972)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:16 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [23/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1585 (0.1709)	loss 1.1113 (1.1729)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:17 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 23 training takes 0:00:05
[2024-12-02 13:04:17 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:04:17 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:04:17 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:04:18 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [24/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3288 (0.3288)	loss 1.2105 (1.2105)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:20 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [24/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1740 (0.1899)	loss 1.1162 (1.1744)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:22 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [24/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.2392 (0.2043)	loss 1.2089 (1.1896)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:24 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [24/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1625 (0.2082)	loss 1.3081 (1.2048)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:24 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 24 training takes 0:00:06
[2024-12-02 13:04:25 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:04:25 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:04:25 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:04:26 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [25/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3161 (0.3161)	loss 1.2229 (1.2229)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:27 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [25/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1627 (0.1932)	loss 1.5144 (1.2567)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:29 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [25/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1900 (0.1877)	loss 1.1144 (1.1926)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:31 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [25/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1594 (0.1798)	loss 1.3066 (1.1866)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:31 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 25 training takes 0:00:06
[2024-12-02 13:04:32 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:04:32 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:04:32 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:04:33 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [26/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3529 (0.3529)	loss 1.3199 (1.3199)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:35 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [26/50][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.2359 (0.2314)	loss 1.0294 (1.1535)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:37 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [26/50][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1611 (0.2255)	loss 1.2143 (1.1770)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:39 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [26/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1815 (0.2110)	loss 1.0025 (1.1751)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:40 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 26 training takes 0:00:07
[2024-12-02 13:04:41 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:04:41 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:04:41 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:04:41 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [27/50][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3578 (0.3578)	loss 1.4119 (1.4119)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [27/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1672 (0.1975)	loss 1.5333 (1.1610)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:44 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [27/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1720 (0.1824)	loss 1.4040 (1.1913)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:46 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [27/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1682 (0.1812)	loss 1.4108 (1.1895)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:47 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 27 training takes 0:00:06
[2024-12-02 13:04:48 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:04:48 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:04:48 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:04:48 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [28/50][0/34]	eta 0:00:15 lr 0.000000	 wd 0.0500	time 0.4512 (0.4512)	loss 1.3144 (1.3144)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:51 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [28/50][10/34]	eta 0:00:06 lr 0.000000	 wd 0.0500	time 0.1775 (0.2635)	loss 1.2006 (1.1132)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:53 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [28/50][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1632 (0.2200)	loss 1.4167 (1.1531)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:54 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [28/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1709 (0.2016)	loss 1.1021 (1.1701)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:55 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 28 training takes 0:00:06
[2024-12-02 13:04:56 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:04:56 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:04:56 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:04:56 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [29/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3424 (0.3424)	loss 1.3227 (1.3227)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:04:58 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [29/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1590 (0.1872)	loss 1.0086 (1.2451)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:00 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [29/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1652 (0.1811)	loss 1.0040 (1.2303)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:02 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [29/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2341 (0.1864)	loss 1.2243 (1.2024)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:02 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 29 training takes 0:00:06
[2024-12-02 13:05:04 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:05:04 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:05:04 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:05:04 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [30/50][0/34]	eta 0:00:15 lr 0.000000	 wd 0.0500	time 0.4616 (0.4616)	loss 1.1026 (1.1026)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:06 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [30/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1901 (0.2010)	loss 1.1117 (1.1223)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:07 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [30/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1633 (0.1846)	loss 1.1088 (1.1668)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:09 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [30/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1687 (0.1805)	loss 1.4201 (1.1742)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:10 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 30 training takes 0:00:06
[2024-12-02 13:05:11 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:05:11 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:05:11 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:05:11 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [31/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3500 (0.3500)	loss 1.3124 (1.3124)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:13 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [31/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1826 (0.2034)	loss 1.3106 (1.2149)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:15 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [31/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.2409 (0.1970)	loss 1.2093 (1.1914)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:17 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [31/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2403 (0.2116)	loss 0.7156 (1.1797)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:18 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 31 training takes 0:00:07
[2024-12-02 13:05:19 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:05:19 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:05:19 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:05:19 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [32/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3202 (0.3202)	loss 1.0058 (1.0058)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:21 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [32/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1696 (0.1943)	loss 1.0125 (1.2167)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:23 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [32/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1595 (0.1803)	loss 1.0123 (1.1377)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:24 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [32/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1726 (0.1779)	loss 1.4166 (1.1854)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:25 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 32 training takes 0:00:06
[2024-12-02 13:05:26 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:05:26 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:05:26 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:05:26 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [33/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3059 (0.3059)	loss 1.1313 (1.1313)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:28 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [33/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.2505 (0.2037)	loss 0.6049 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:30 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [33/50][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.2515 (0.2225)	loss 1.3294 (1.1954)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:32 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [33/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1605 (0.2101)	loss 1.0163 (1.1927)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:33 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 33 training takes 0:00:07
[2024-12-02 13:05:34 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:05:34 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:05:34 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:05:34 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [34/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3523 (0.3523)	loss 1.1084 (1.1084)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:36 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [34/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1883 (0.2021)	loss 1.3212 (1.1620)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:38 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [34/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1645 (0.1845)	loss 1.3186 (1.1931)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:39 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [34/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1725 (0.1808)	loss 0.9284 (1.1845)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:40 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 34 training takes 0:00:06
[2024-12-02 13:05:41 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:05:41 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:05:41 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:05:41 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [35/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3148 (0.3148)	loss 1.1111 (1.1111)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [35/50][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.2363 (0.2431)	loss 0.8124 (1.1889)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:46 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [35/50][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1628 (0.2299)	loss 0.9057 (1.1603)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:47 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [35/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1613 (0.2088)	loss 1.6174 (1.1775)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:48 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 35 training takes 0:00:06
[2024-12-02 13:05:49 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:05:49 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:05:49 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:05:49 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [36/50][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3648 (0.3648)	loss 1.4135 (1.4135)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:51 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [36/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1651 (0.2027)	loss 1.3019 (1.1401)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:53 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [36/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1838 (0.1849)	loss 1.1178 (1.1869)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:54 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [36/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1752 (0.1841)	loss 1.0131 (1.1773)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:55 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 36 training takes 0:00:06
[2024-12-02 13:05:56 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:05:56 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:05:56 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:05:57 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [37/50][0/34]	eta 0:00:14 lr 0.000000	 wd 0.0500	time 0.4166 (0.4166)	loss 1.3151 (1.3151)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:05:59 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [37/50][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.1638 (0.2309)	loss 1.1040 (1.2517)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:01 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [37/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1895 (0.2058)	loss 0.7092 (1.2056)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:02 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [37/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1663 (0.1965)	loss 1.1129 (1.1746)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:03 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 37 training takes 0:00:06
[2024-12-02 13:06:04 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:06:04 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:06:04 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:06:04 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [38/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3521 (0.3521)	loss 0.9028 (0.9028)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:06 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [38/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1808 (0.1983)	loss 1.2352 (1.1704)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:08 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [38/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1869 (0.1914)	loss 1.4207 (1.1830)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:10 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [38/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2322 (0.2067)	loss 1.0141 (1.1747)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:11 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 38 training takes 0:00:07
[2024-12-02 13:06:12 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:06:12 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:06:12 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:06:13 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [39/50][0/34]	eta 0:00:12 lr 0.000000	 wd 0.0500	time 0.3666 (0.3666)	loss 0.9233 (0.9233)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:14 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [39/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1671 (0.2074)	loss 1.3170 (1.2329)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:16 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [39/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1642 (0.1887)	loss 1.3231 (1.2046)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:18 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [39/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1604 (0.1841)	loss 0.9093 (1.2043)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:18 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 39 training takes 0:00:06
[2024-12-02 13:06:19 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:06:19 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:06:19 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:06:20 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [40/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3114 (0.3114)	loss 1.1246 (1.1246)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:22 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [40/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1638 (0.1946)	loss 1.2323 (1.1760)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:24 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [40/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.2423 (0.2137)	loss 1.1203 (1.2085)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:26 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [40/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1634 (0.2135)	loss 1.0112 (1.2018)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:27 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 40 training takes 0:00:07
[2024-12-02 13:06:28 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:06:28 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:06:28 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:06:28 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [41/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3126 (0.3126)	loss 1.0274 (1.0274)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:30 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [41/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1712 (0.1962)	loss 1.1096 (1.2295)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:31 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [41/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1736 (0.1874)	loss 1.1239 (1.2008)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:33 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [41/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1630 (0.1800)	loss 1.2138 (1.2016)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:34 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 41 training takes 0:00:06
[2024-12-02 13:06:35 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:06:35 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:06:35 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:06:35 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [42/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3381 (0.3381)	loss 0.9176 (0.9176)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:37 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [42/50][10/34]	eta 0:00:06 lr 0.000000	 wd 0.0500	time 0.2586 (0.2513)	loss 1.3111 (1.1131)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:40 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [42/50][20/34]	eta 0:00:03 lr 0.000000	 wd 0.0500	time 0.1639 (0.2309)	loss 1.1106 (1.1564)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:41 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [42/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1777 (0.2134)	loss 1.0226 (1.1715)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:42 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 42 training takes 0:00:07
[2024-12-02 13:06:43 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:06:43 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:06:43 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:06:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [43/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3333 (0.3333)	loss 1.3139 (1.3139)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:45 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [43/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1660 (0.2037)	loss 0.7032 (1.1111)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:47 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [43/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1671 (0.1881)	loss 1.0167 (1.1435)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:48 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [43/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1766 (0.1853)	loss 1.3263 (1.1784)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:49 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 43 training takes 0:00:06
[2024-12-02 13:06:50 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:06:50 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:06:50 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:06:51 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [44/50][0/34]	eta 0:00:15 lr 0.000000	 wd 0.0500	time 0.4540 (0.4540)	loss 1.2082 (1.2082)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:53 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [44/50][10/34]	eta 0:00:05 lr 0.000000	 wd 0.0500	time 0.1704 (0.2465)	loss 1.1141 (1.1958)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:55 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [44/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1647 (0.2133)	loss 1.0141 (1.2047)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:57 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [44/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1693 (0.2013)	loss 1.4171 (1.2017)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:06:57 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 44 training takes 0:00:06
[2024-12-02 13:06:58 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:06:58 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:06:58 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:06:58 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [45/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3104 (0.3104)	loss 1.0091 (1.0091)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:00 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [45/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1644 (0.1895)	loss 1.0239 (1.1648)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:02 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [45/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1725 (0.1783)	loss 1.0019 (1.1559)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:04 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [45/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2397 (0.1921)	loss 1.2294 (1.1755)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:05 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 45 training takes 0:00:06
[2024-12-02 13:07:06 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:07:06 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:07:06 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:07:07 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [46/50][0/34]	eta 0:00:11 lr 0.000000	 wd 0.0500	time 0.3386 (0.3386)	loss 1.0175 (1.0175)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:08 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [46/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1602 (0.1924)	loss 1.2075 (1.2443)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:10 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [46/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1784 (0.1836)	loss 1.4277 (1.2311)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:12 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [46/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1703 (0.1782)	loss 1.1107 (1.1996)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:12 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 46 training takes 0:00:06
[2024-12-02 13:07:13 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:07:13 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:07:13 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:07:13 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [47/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3066 (0.3066)	loss 1.3101 (1.3101)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:15 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [47/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1657 (0.1919)	loss 1.1094 (1.1374)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:17 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [47/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.2454 (0.1991)	loss 1.1018 (1.1581)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:20 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [47/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2072 (0.2078)	loss 1.3091 (1.1700)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:20 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 47 training takes 0:00:06
[2024-12-02 13:07:21 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:07:21 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:07:21 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:07:21 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [48/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3064 (0.3064)	loss 1.1175 (1.1175)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:23 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [48/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.1562 (0.1849)	loss 0.9134 (1.1412)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:25 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [48/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.1832 (0.1779)	loss 1.2073 (1.2090)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:26 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [48/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1599 (0.1756)	loss 1.2137 (1.1842)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:27 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 48 training takes 0:00:05
[2024-12-02 13:07:28 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:07:28 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:07:28 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:07:28 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [49/50][0/34]	eta 0:00:10 lr 0.000000	 wd 0.0500	time 0.3014 (0.3014)	loss 1.1293 (1.1293)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:30 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [49/50][10/34]	eta 0:00:04 lr 0.000000	 wd 0.0500	time 0.2077 (0.1859)	loss 1.1128 (1.2068)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:32 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [49/50][20/34]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.2239 (0.2077)	loss 1.0198 (1.1625)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:34 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [49/50][30/34]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1593 (0.2009)	loss 1.2172 (1.1699)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:07:35 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 49 training takes 0:00:06
[2024-12-02 13:07:35 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:07:35 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:07:35 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:07:36 swin_tiny_patch4_window7_224] (main.py 175): INFO Training time 0:06:19
[2024-12-02 13:14:11 swin_tiny_patch4_window7_224] (main.py 382): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 3.125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.125e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 3.125e-06
  WEIGHT_DECAY: 0.05

[2024-12-02 13:14:11 swin_tiny_patch4_window7_224] (main.py 383): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 13:14:11 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 13:14:11 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 13:14:11 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 13:14:11 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 13:14:11 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 13:14:11 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 13:14:12 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][0/34]	eta 0:00:09 lr 0.000003	 wd 0.0500	time 0.2794 (0.2794)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:13 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1583 (0.1794)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:15 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1692 (0.1700)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:17 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2440 (0.1757)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:17 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 0 training takes 0:00:06
[2024-12-02 13:14:19 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:14:19 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:14:19 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:14:19 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][0/34]	eta 0:00:17 lr 0.000003	 wd 0.0500	time 0.5050 (0.5050)	loss 1.3204 (1.3204)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:21 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1792 (0.2257)	loss 1.2106 (1.1857)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:23 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1714 (0.2015)	loss 1.3177 (1.1705)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:25 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1703 (0.1928)	loss 1.0078 (1.1956)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:25 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 1 training takes 0:00:06
[2024-12-02 13:14:26 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:14:26 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:14:26 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:14:27 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3452 (0.3452)	loss 0.9030 (0.9030)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:28 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1729 (0.2001)	loss 1.3133 (1.1674)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:30 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.2139 (0.1849)	loss 1.1163 (1.1561)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:32 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2363 (0.2010)	loss 1.3294 (1.1946)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:33 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 2 training takes 0:00:06
[2024-12-02 13:14:34 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:14:34 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:14:34 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:14:34 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3061 (0.3061)	loss 1.2180 (1.2180)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:36 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1725 (0.1848)	loss 0.8115 (1.1970)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:38 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1628 (0.1789)	loss 1.2076 (1.2151)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:40 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1727 (0.1749)	loss 1.2020 (1.1984)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:40 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 3 training takes 0:00:05
[2024-12-02 13:14:41 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:14:41 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:14:41 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:14:41 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3335 (0.3335)	loss 1.4127 (1.4127)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1699 (0.1952)	loss 1.1205 (1.1524)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:45 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.2457 (0.2038)	loss 1.4133 (1.1858)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:48 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1773 (0.2093)	loss 1.0023 (1.1951)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:48 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 4 training takes 0:00:07
[2024-12-02 13:14:49 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:14:49 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:14:49 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:14:49 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3062 (0.3062)	loss 1.0050 (1.0050)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:51 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1605 (0.1928)	loss 1.3082 (1.1669)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:53 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1599 (0.1789)	loss 1.4345 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:54 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1729 (0.1748)	loss 1.1244 (1.1977)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:55 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 5 training takes 0:00:06
[2024-12-02 13:14:56 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:14:56 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:14:56 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:14:56 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3397 (0.3397)	loss 1.2188 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:14:59 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.2340 (0.2398)	loss 0.8230 (1.1925)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:01 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.2328 (0.2385)	loss 1.2182 (1.1895)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:03 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1607 (0.2199)	loss 1.2178 (1.1813)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:03 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 6 training takes 0:00:07
[2024-12-02 13:15:04 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:15:04 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:15:04 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:15:05 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3451 (0.3451)	loss 1.0111 (1.0111)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:06 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1612 (0.1871)	loss 1.0072 (1.1654)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:08 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1805 (0.1842)	loss 0.9054 (1.1399)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:10 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1643 (0.1787)	loss 1.2147 (1.1798)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:10 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 7 training takes 0:00:06
[2024-12-02 13:15:11 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:15:11 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:15:11 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:15:12 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][0/34]	eta 0:00:14 lr 0.000003	 wd 0.0500	time 0.4225 (0.4225)	loss 1.1182 (1.1182)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:14 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][10/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 0.2263 (0.2680)	loss 1.0214 (1.2186)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:16 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1582 (0.2209)	loss 1.2093 (1.2085)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:18 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1864 (0.2027)	loss 1.0152 (1.1824)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:18 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 8 training takes 0:00:06
[2024-12-02 13:15:19 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:15:19 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:15:19 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:15:19 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3071 (0.3071)	loss 1.0264 (1.0264)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:21 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1626 (0.1901)	loss 1.3252 (1.1444)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:23 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1620 (0.1759)	loss 1.5173 (1.1804)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:24 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1875 (0.1728)	loss 1.5330 (1.1794)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:25 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 9 training takes 0:00:06
[2024-12-02 13:15:27 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:15:27 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:15:27 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:15:27 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][0/34]	eta 0:00:17 lr 0.000003	 wd 0.0500	time 0.5040 (0.5040)	loss 1.3284 (1.3284)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:29 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1581 (0.2222)	loss 1.1036 (1.1933)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:31 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1566 (0.1971)	loss 1.2124 (1.1887)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:32 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1712 (0.1881)	loss 1.1163 (1.1807)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:33 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 10 training takes 0:00:06
[2024-12-02 13:15:34 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:15:34 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:15:34 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:15:34 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3440 (0.3440)	loss 1.4163 (1.4163)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:36 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1718 (0.1956)	loss 0.8097 (1.2118)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:38 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1771 (0.1817)	loss 1.3154 (1.2331)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:40 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2620 (0.1991)	loss 0.7169 (1.1948)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:41 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 11 training takes 0:00:06
[2024-12-02 13:15:42 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:15:42 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:15:42 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:15:42 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [12/50][0/34]	eta 0:00:14 lr 0.000003	 wd 0.0500	time 0.4294 (0.4294)	loss 1.1152 (1.1152)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:44 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [12/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1723 (0.1993)	loss 1.3062 (1.2064)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:46 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [12/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1837 (0.1843)	loss 1.0982 (1.2008)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:47 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [12/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1609 (0.1786)	loss 1.4206 (1.1763)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:48 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 12 training takes 0:00:06
[2024-12-02 13:15:49 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:15:49 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:15:49 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:15:49 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [13/50][0/34]	eta 0:00:12 lr 0.000003	 wd 0.0500	time 0.3680 (0.3680)	loss 1.2212 (1.2212)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:51 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [13/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1578 (0.1885)	loss 1.2048 (1.1112)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:53 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [13/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.2403 (0.2069)	loss 1.3212 (1.1858)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:56 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [13/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1698 (0.2118)	loss 1.3129 (1.1824)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:56 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 13 training takes 0:00:07
[2024-12-02 13:15:57 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:15:57 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:15:57 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:15:57 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [14/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3484 (0.3484)	loss 1.3150 (1.3150)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:15:59 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [14/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1611 (0.2003)	loss 0.9079 (1.0414)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:01 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [14/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1658 (0.1832)	loss 1.5214 (1.1916)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:03 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [14/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1594 (0.1802)	loss 1.2292 (1.1842)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:03 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 14 training takes 0:00:06
[2024-12-02 13:16:04 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:16:04 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:16:04 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:16:04 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [15/50][0/34]	eta 0:00:12 lr 0.000003	 wd 0.0500	time 0.3717 (0.3717)	loss 0.8971 (0.8971)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:07 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [15/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.2322 (0.2467)	loss 1.4212 (1.1306)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:09 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [15/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1739 (0.2371)	loss 1.3204 (1.1848)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:11 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [15/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1753 (0.2168)	loss 1.3373 (1.1890)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:11 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 15 training takes 0:00:07
[2024-12-02 13:16:12 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:16:12 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:16:12 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:16:13 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [16/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3337 (0.3337)	loss 1.0005 (1.0005)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:14 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [16/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1663 (0.1976)	loss 1.2136 (1.1193)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:16 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [16/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1678 (0.1854)	loss 0.8986 (1.1694)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:18 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [16/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1788 (0.1806)	loss 1.3288 (1.1791)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:18 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 16 training takes 0:00:06
[2024-12-02 13:16:20 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:16:20 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:16:20 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:16:20 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [17/50][0/34]	eta 0:00:13 lr 0.000003	 wd 0.0500	time 0.4050 (0.4050)	loss 1.4139 (1.4139)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:22 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [17/50][10/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 0.1694 (0.2513)	loss 1.3061 (1.2603)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:24 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [17/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1746 (0.2158)	loss 1.2102 (1.1890)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:26 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [17/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1581 (0.2027)	loss 1.0017 (1.1820)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:27 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 17 training takes 0:00:06
[2024-12-02 13:16:28 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:16:28 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:16:28 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:16:28 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [18/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3484 (0.3484)	loss 1.3049 (1.3049)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:30 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [18/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1540 (0.1895)	loss 1.1154 (1.1879)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:31 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [18/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1698 (0.1786)	loss 1.4091 (1.1868)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:33 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [18/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2317 (0.1898)	loss 1.0122 (1.1849)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:34 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 18 training takes 0:00:06
[2024-12-02 13:16:35 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:16:35 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:16:35 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:16:36 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [19/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3040 (0.3040)	loss 1.0286 (1.0286)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:38 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [19/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1708 (0.1881)	loss 1.1265 (1.2362)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:39 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [19/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1820 (0.1763)	loss 0.9085 (1.1835)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:41 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [19/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1700 (0.1744)	loss 1.2118 (1.1933)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:41 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 19 training takes 0:00:05
[2024-12-02 13:16:42 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:16:42 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:16:42 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:16:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [20/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3250 (0.3250)	loss 1.3092 (1.3092)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:44 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [20/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1770 (0.1843)	loss 1.3186 (1.2343)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:46 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [20/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.2467 (0.1846)	loss 0.9077 (1.2009)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:49 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [20/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2305 (0.2052)	loss 1.1030 (1.2027)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:49 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 20 training takes 0:00:06
[2024-12-02 13:16:50 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:16:50 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:16:50 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:16:51 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [21/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3517 (0.3517)	loss 1.0226 (1.0226)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:53 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [21/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1637 (0.1994)	loss 1.2127 (1.1962)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:54 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [21/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1687 (0.1859)	loss 1.6450 (1.1826)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:56 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [21/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1703 (0.1845)	loss 1.4221 (1.1872)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:16:57 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 21 training takes 0:00:06
[2024-12-02 13:16:58 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:16:58 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:16:58 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:16:58 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [22/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3124 (0.3124)	loss 1.4136 (1.4136)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:00 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [22/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.2546 (0.2235)	loss 1.1175 (1.1990)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:02 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [22/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.2179 (0.2322)	loss 1.4164 (1.2163)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:04 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [22/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1784 (0.2206)	loss 1.3064 (1.1859)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:05 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 22 training takes 0:00:07
[2024-12-02 13:17:06 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:17:06 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:17:06 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:17:06 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [23/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3434 (0.3434)	loss 1.1186 (1.1186)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:08 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [23/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1860 (0.2076)	loss 0.8045 (1.1530)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:10 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [23/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1706 (0.1923)	loss 1.4216 (1.1972)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:12 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [23/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1692 (0.1843)	loss 1.1113 (1.1729)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:12 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 23 training takes 0:00:06
[2024-12-02 13:17:14 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:17:14 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:17:14 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:17:14 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [24/50][0/34]	eta 0:00:14 lr 0.000003	 wd 0.0500	time 0.4310 (0.4310)	loss 1.2105 (1.2105)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:16 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [24/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1696 (0.2402)	loss 1.1162 (1.1744)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:18 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [24/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1815 (0.2111)	loss 1.2089 (1.1896)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:20 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [24/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1743 (0.1991)	loss 1.3081 (1.2048)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:20 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 24 training takes 0:00:06
[2024-12-02 13:17:21 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:17:21 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:17:21 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:17:22 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [25/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3488 (0.3488)	loss 1.2229 (1.2229)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:23 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [25/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1716 (0.1941)	loss 1.5144 (1.2567)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:25 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [25/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1776 (0.1873)	loss 1.1144 (1.1926)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:28 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [25/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2656 (0.2055)	loss 1.3066 (1.1866)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:29 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 25 training takes 0:00:07
[2024-12-02 13:17:30 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:17:30 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:17:30 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:17:30 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [26/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3514 (0.3514)	loss 1.3199 (1.3199)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:32 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [26/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1589 (0.1872)	loss 1.0294 (1.1535)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:34 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [26/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1631 (0.1851)	loss 1.2143 (1.1770)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:35 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [26/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1573 (0.1775)	loss 1.0025 (1.1751)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:36 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 26 training takes 0:00:06
[2024-12-02 13:17:37 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:17:37 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:17:37 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:17:37 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [27/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3371 (0.3371)	loss 1.4119 (1.4119)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:39 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [27/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1586 (0.1911)	loss 1.5333 (1.1610)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:41 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [27/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.2580 (0.2125)	loss 1.4040 (1.1913)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [27/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1565 (0.2112)	loss 1.4108 (1.1895)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:44 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 27 training takes 0:00:07
[2024-12-02 13:17:45 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:17:45 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:17:45 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:17:45 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [28/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3292 (0.3292)	loss 1.3144 (1.3144)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:47 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [28/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1591 (0.1877)	loss 1.2006 (1.1132)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:48 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [28/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1643 (0.1768)	loss 1.4167 (1.1531)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:50 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [28/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1956 (0.1761)	loss 1.1021 (1.1701)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:51 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 28 training takes 0:00:06
[2024-12-02 13:17:52 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:17:52 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:17:52 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:17:52 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [29/50][0/34]	eta 0:00:12 lr 0.000003	 wd 0.0500	time 0.3650 (0.3650)	loss 1.3227 (1.3227)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:55 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [29/50][10/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 0.2393 (0.2638)	loss 1.0086 (1.2451)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:57 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [29/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1812 (0.2424)	loss 1.0040 (1.2303)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:59 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [29/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1669 (0.2232)	loss 1.2243 (1.2024)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:17:59 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 29 training takes 0:00:07
[2024-12-02 13:18:00 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:18:00 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:18:00 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:18:01 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [30/50][0/34]	eta 0:00:12 lr 0.000003	 wd 0.0500	time 0.3748 (0.3748)	loss 1.1026 (1.1026)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:03 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [30/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.2140 (0.2174)	loss 1.1117 (1.1223)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:04 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [30/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1627 (0.1979)	loss 1.1088 (1.1668)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:06 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [30/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1827 (0.1909)	loss 1.4201 (1.1742)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:07 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 30 training takes 0:00:06
[2024-12-02 13:18:08 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:18:08 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:18:08 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:18:09 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [31/50][0/34]	eta 0:00:14 lr 0.000003	 wd 0.0500	time 0.4306 (0.4306)	loss 1.3124 (1.3124)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:11 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [31/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1817 (0.2314)	loss 1.3106 (1.2149)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:13 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [31/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1767 (0.2071)	loss 1.2093 (1.1914)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:14 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [31/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1684 (0.1936)	loss 0.7156 (1.1797)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:15 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 31 training takes 0:00:06
[2024-12-02 13:18:16 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:18:16 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:18:16 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:18:16 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [32/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3138 (0.3138)	loss 1.0058 (1.0058)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:18 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [32/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1795 (0.1918)	loss 1.0125 (1.2167)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:20 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [32/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.2277 (0.1854)	loss 1.0123 (1.1377)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:22 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [32/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2544 (0.2078)	loss 1.4166 (1.1854)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:23 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 32 training takes 0:00:07
[2024-12-02 13:18:24 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:18:24 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:18:24 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:18:24 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [33/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3462 (0.3462)	loss 1.1313 (1.1313)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:26 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [33/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1618 (0.1976)	loss 0.6049 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:28 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [33/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1734 (0.1855)	loss 1.3294 (1.1954)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:29 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [33/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1740 (0.1801)	loss 1.0163 (1.1927)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:30 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 33 training takes 0:00:06
[2024-12-02 13:18:31 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:18:31 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:18:31 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:18:31 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [34/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3139 (0.3139)	loss 1.1084 (1.1084)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:33 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [34/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.2489 (0.2040)	loss 1.3212 (1.1620)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:36 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [34/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.2388 (0.2250)	loss 1.3186 (1.1931)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:37 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [34/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1708 (0.2085)	loss 0.9284 (1.1845)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:38 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 34 training takes 0:00:07
[2024-12-02 13:18:39 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:18:39 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:18:39 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:18:39 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [35/50][0/34]	eta 0:00:12 lr 0.000003	 wd 0.0500	time 0.3530 (0.3530)	loss 1.1111 (1.1111)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:41 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [35/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1608 (0.1939)	loss 0.8124 (1.1889)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [35/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1598 (0.1785)	loss 0.9057 (1.1603)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:44 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [35/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1739 (0.1772)	loss 1.6174 (1.1775)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:45 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 35 training takes 0:00:06
[2024-12-02 13:18:46 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:18:46 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:18:46 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:18:46 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [36/50][0/34]	eta 0:00:13 lr 0.000003	 wd 0.0500	time 0.3867 (0.3867)	loss 1.4135 (1.4135)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:49 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [36/50][10/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 0.2241 (0.2614)	loss 1.3019 (1.1401)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:51 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [36/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1741 (0.2280)	loss 1.1178 (1.1869)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:52 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [36/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1565 (0.2070)	loss 1.0131 (1.1773)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:53 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 36 training takes 0:00:06
[2024-12-02 13:18:54 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:18:54 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:18:54 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:18:54 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [37/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3275 (0.3275)	loss 1.3151 (1.3151)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:56 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [37/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1702 (0.1947)	loss 1.1040 (1.2517)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:57 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [37/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1637 (0.1800)	loss 0.7092 (1.2056)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:18:59 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [37/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1750 (0.1744)	loss 1.1129 (1.1746)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:00 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 37 training takes 0:00:06
[2024-12-02 13:19:01 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:19:01 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:19:01 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:19:02 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [38/50][0/34]	eta 0:00:16 lr 0.000003	 wd 0.0500	time 0.4940 (0.4940)	loss 0.9028 (0.9028)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:04 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [38/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1749 (0.2350)	loss 1.2352 (1.1704)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:06 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [38/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1706 (0.2082)	loss 1.4207 (1.1830)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:07 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [38/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1797 (0.1984)	loss 1.0141 (1.1747)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:08 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 38 training takes 0:00:06
[2024-12-02 13:19:09 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:19:09 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:19:09 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:19:09 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [39/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3104 (0.3104)	loss 0.9233 (0.9233)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:11 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [39/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1621 (0.1902)	loss 1.3170 (1.2329)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:13 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [39/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1666 (0.1772)	loss 1.3231 (1.2046)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:15 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [39/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2489 (0.1929)	loss 0.9093 (1.2043)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:16 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 39 training takes 0:00:06
[2024-12-02 13:19:17 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:19:17 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:19:17 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:19:17 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [40/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3223 (0.3223)	loss 1.1246 (1.1246)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:19 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [40/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1768 (0.1943)	loss 1.2323 (1.1760)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:21 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [40/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1804 (0.1858)	loss 1.1203 (1.2085)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:22 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [40/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1659 (0.1821)	loss 1.0112 (1.2018)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:23 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 40 training takes 0:00:06
[2024-12-02 13:19:24 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:19:24 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:19:24 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:19:24 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [41/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3488 (0.3488)	loss 1.0274 (1.0274)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:26 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [41/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.2150 (0.2079)	loss 1.1096 (1.2295)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:29 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [41/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.2664 (0.2265)	loss 1.1239 (1.2008)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:30 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [41/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1728 (0.2149)	loss 1.2138 (1.2016)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:31 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 41 training takes 0:00:07
[2024-12-02 13:19:32 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:19:32 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:19:32 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:19:32 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [42/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3030 (0.3030)	loss 0.9176 (0.9176)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:34 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [42/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1707 (0.1927)	loss 1.3111 (1.1131)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:36 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [42/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1599 (0.1816)	loss 1.1106 (1.1564)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:37 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [42/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1612 (0.1754)	loss 1.0226 (1.1715)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:38 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 42 training takes 0:00:05
[2024-12-02 13:19:39 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:19:39 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:19:39 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:19:39 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [43/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3377 (0.3377)	loss 1.3139 (1.3139)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:42 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [43/50][10/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 0.2663 (0.2541)	loss 0.7032 (1.1111)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:44 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [43/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1588 (0.2266)	loss 1.0167 (1.1435)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:45 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [43/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1892 (0.2096)	loss 1.3263 (1.1784)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:46 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 43 training takes 0:00:07
[2024-12-02 13:19:47 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:19:47 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:19:47 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:19:47 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [44/50][0/34]	eta 0:00:12 lr 0.000003	 wd 0.0500	time 0.3551 (0.3551)	loss 1.2082 (1.2082)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:49 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [44/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1669 (0.1973)	loss 1.1141 (1.1958)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:51 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [44/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1739 (0.1869)	loss 1.0141 (1.2047)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:53 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [44/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2021 (0.1855)	loss 1.4171 (1.2017)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:53 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 44 training takes 0:00:06
[2024-12-02 13:19:55 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:19:55 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:19:55 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:19:56 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [45/50][0/34]	eta 0:00:16 lr 0.000003	 wd 0.0500	time 0.4885 (0.4885)	loss 1.0091 (1.0091)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:58 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [45/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1762 (0.2262)	loss 1.0239 (1.1648)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:19:59 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [45/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1587 (0.1992)	loss 1.0019 (1.1559)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:20:01 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [45/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1689 (0.1911)	loss 1.2294 (1.1755)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:20:02 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 45 training takes 0:00:06
[2024-12-02 13:20:03 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:20:03 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:20:03 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:20:03 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [46/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3364 (0.3364)	loss 1.0175 (1.0175)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:20:23 swin_tiny_patch4_window7_224] (main.py 382): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 3.125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.125e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 3.125e-06
  WEIGHT_DECAY: 0.05

[2024-12-02 13:20:23 swin_tiny_patch4_window7_224] (main.py 383): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 13:20:23 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 13:20:23 swin_tiny_patch4_window7_224] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(1, 96, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(14, 14), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=192, out_features=2, bias=True)
)
[2024-12-02 13:20:23 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 13:20:23 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 13:20:23 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 13:20:23 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 13:20:23 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][0/34]	eta 0:00:09 lr 0.000003	 wd 0.0500	time 0.2934 (0.2934)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:20:25 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1582 (0.1858)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:20:27 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1733 (0.1815)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:20:28 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1675 (0.1787)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:20:29 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 0 training takes 0:00:06
[2024-12-02 13:20:30 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:20:30 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:20:30 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:20:30 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3160 (0.3160)	loss 1.3204 (1.3204)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:20:32 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.2331 (0.2075)	loss 1.2106 (1.1857)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:20:34 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.2287 (0.2193)	loss 1.3177 (1.1705)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:20:36 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1707 (0.2048)	loss 1.0078 (1.1956)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:20:37 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 1 training takes 0:00:06
[2024-12-02 13:20:38 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:20:38 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:20:38 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:20:38 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][0/34]	eta 0:00:12 lr 0.000003	 wd 0.0500	time 0.3552 (0.3552)	loss 0.9030 (0.9030)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:20:40 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1702 (0.1939)	loss 1.3133 (1.1674)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:20:42 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1627 (0.1846)	loss 1.1163 (1.1561)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:20:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1932 (0.1806)	loss 1.3294 (1.1946)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:20:44 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 2 training takes 0:00:06
[2024-12-02 13:20:45 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:20:45 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:20:45 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:20:45 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][0/34]	eta 0:00:16 lr 0.000003	 wd 0.0500	time 0.4892 (0.4892)	loss 1.2180 (1.2180)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:23 swin_tiny_patch4_window7_224] (main.py 382): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 3.125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.125e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 3.125e-06
  WEIGHT_DECAY: 0.05

[2024-12-02 13:22:23 swin_tiny_patch4_window7_224] (main.py 383): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 13:22:23 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 13:22:23 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 13:22:23 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 13:22:23 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 13:22:23 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 13:22:23 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][0/34]	eta 0:00:13 lr 0.000003	 wd 0.0500	time 0.3870 (0.3870)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:25 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1559 (0.1870)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:26 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1912 (0.1775)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:28 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1765 (0.1796)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:29 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 0 training takes 0:00:06
[2024-12-02 13:22:30 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:22:30 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:22:30 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:22:30 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3404 (0.3404)	loss 1.3204 (1.3204)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:32 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1743 (0.1920)	loss 1.2106 (1.1857)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:34 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.2502 (0.1908)	loss 1.3177 (1.1705)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:36 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2335 (0.2098)	loss 1.0078 (1.1956)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:37 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 1 training takes 0:00:07
[2024-12-02 13:22:38 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:22:38 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:22:38 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:22:38 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3178 (0.3178)	loss 0.9030 (0.9030)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:40 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1862 (0.2023)	loss 1.3133 (1.1674)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:42 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1844 (0.1897)	loss 1.1163 (1.1561)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1741 (0.1855)	loss 1.3294 (1.1946)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:44 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 2 training takes 0:00:06
[2024-12-02 13:22:45 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:22:45 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:22:45 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:22:45 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3158 (0.3158)	loss 1.2180 (1.2180)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:47 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.2259 (0.2072)	loss 0.8115 (1.1970)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:50 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.2386 (0.2213)	loss 1.2076 (1.2151)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:51 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1558 (0.2028)	loss 1.2020 (1.1984)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:52 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 3 training takes 0:00:06
[2024-12-02 13:22:53 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:22:53 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:22:53 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:22:53 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3458 (0.3458)	loss 1.4127 (1.4127)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:55 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1710 (0.1902)	loss 1.1205 (1.1524)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:56 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1862 (0.1835)	loss 1.4133 (1.1858)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:58 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1596 (0.1801)	loss 1.0023 (1.1951)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:22:59 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 4 training takes 0:00:06
[2024-12-02 13:23:00 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:23:00 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:23:00 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:23:00 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][0/34]	eta 0:00:15 lr 0.000003	 wd 0.0500	time 0.4617 (0.4617)	loss 1.0050 (1.0050)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:03 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][10/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 0.2333 (0.2822)	loss 1.3082 (1.1669)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:05 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1564 (0.2282)	loss 1.4345 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:06 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1676 (0.2095)	loss 1.1244 (1.1977)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:07 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 5 training takes 0:00:07
[2024-12-02 13:23:08 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:23:08 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:23:08 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:23:08 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3254 (0.3254)	loss 1.2188 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:10 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1653 (0.1905)	loss 0.8230 (1.1925)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:12 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1726 (0.1830)	loss 1.2182 (1.1895)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:13 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2155 (0.1792)	loss 1.2178 (1.1813)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:14 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 6 training takes 0:00:06
[2024-12-02 13:23:15 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:23:15 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:23:15 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:23:16 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][0/34]	eta 0:00:16 lr 0.000003	 wd 0.0500	time 0.4800 (0.4800)	loss 1.0111 (1.0111)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:18 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1566 (0.2091)	loss 1.0072 (1.1654)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:19 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1615 (0.1861)	loss 0.9054 (1.1399)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:21 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1579 (0.1775)	loss 1.2147 (1.1798)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:21 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 7 training takes 0:00:06
[2024-12-02 13:23:22 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:23:22 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:23:22 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:23:23 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3218 (0.3218)	loss 1.1182 (1.1182)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:24 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1788 (0.1899)	loss 1.0214 (1.2186)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:26 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1685 (0.1796)	loss 1.2093 (1.2085)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:28 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2387 (0.1957)	loss 1.0152 (1.1824)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:29 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 8 training takes 0:00:06
[2024-12-02 13:23:30 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:23:30 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:23:30 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:23:30 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3022 (0.3022)	loss 1.0264 (1.0264)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:32 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1594 (0.1818)	loss 1.3252 (1.1444)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:34 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1685 (0.1727)	loss 1.5173 (1.1804)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:35 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1718 (0.1710)	loss 1.5330 (1.1794)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:36 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 9 training takes 0:00:05
[2024-12-02 13:23:37 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:23:37 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:23:37 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:23:37 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3398 (0.3398)	loss 1.3284 (1.3284)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:39 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1793 (0.1941)	loss 1.1036 (1.1933)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:41 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.2488 (0.2077)	loss 1.2124 (1.1887)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:43 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1606 (0.2061)	loss 1.1163 (1.1807)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:44 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 10 training takes 0:00:06
[2024-12-02 13:23:45 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:23:45 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:23:45 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:23:45 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3026 (0.3026)	loss 1.4163 (1.4163)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:47 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1727 (0.1905)	loss 0.8097 (1.2118)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:49 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1661 (0.1819)	loss 1.3154 (1.2331)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:50 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1848 (0.1834)	loss 0.7169 (1.1948)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:23:51 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 11 training takes 0:00:06
[2024-12-02 13:23:53 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:23:53 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:23:53 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:23:53 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [12/50][0/34]	eta 0:00:13 lr 0.000003	 wd 0.0500	time 0.4046 (0.4046)	loss 1.1152 (1.1152)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:24:38 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 13:24:38 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 13:24:38 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 13:24:38 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 13:24:38 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 13:24:38 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][0/34]	eta 0:00:09 lr 0.000003	 wd 0.0500	time 0.2687 (0.2687)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:24:40 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1709 (0.1824)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:24:42 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.2567 (0.1858)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:24:44 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [0/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2644 (0.2041)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:24:45 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 0 training takes 0:00:07
[2024-12-02 13:24:46 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:24:46 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:24:46 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:24:47 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3354 (0.3354)	loss 1.3204 (1.3204)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:24:48 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1870 (0.2003)	loss 1.2106 (1.1857)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:24:50 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1820 (0.1899)	loss 1.3177 (1.1705)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:24:52 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [1/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1840 (0.1868)	loss 1.0078 (1.1956)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:24:53 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 1 training takes 0:00:06
[2024-12-02 13:24:54 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:24:54 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:24:54 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:24:54 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3298 (0.3298)	loss 0.9030 (0.9030)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:24:56 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.2138 (0.2109)	loss 1.3133 (1.1674)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:24:58 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.2340 (0.2260)	loss 1.1163 (1.1561)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:00 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [2/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1758 (0.2134)	loss 1.3294 (1.1946)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:01 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 2 training takes 0:00:07
[2024-12-02 13:25:02 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:25:02 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:25:02 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:25:02 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3153 (0.3153)	loss 1.2180 (1.2180)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:04 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1673 (0.1879)	loss 0.8115 (1.1970)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:05 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1814 (0.1815)	loss 1.2076 (1.2151)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:07 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [3/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1651 (0.1769)	loss 1.2020 (1.1984)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:08 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 3 training takes 0:00:06
[2024-12-02 13:25:09 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:25:09 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:25:09 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:25:09 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3129 (0.3129)	loss 1.4127 (1.4127)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:11 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][10/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 0.2596 (0.2643)	loss 1.1205 (1.1524)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:13 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1794 (0.2333)	loss 1.4133 (1.1858)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:15 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [4/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1670 (0.2119)	loss 1.0023 (1.1951)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:16 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 4 training takes 0:00:07
[2024-12-02 13:25:17 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:25:17 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:25:17 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:25:17 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3062 (0.3062)	loss 1.0050 (1.0050)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:19 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1634 (0.1837)	loss 1.3082 (1.1669)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:20 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1628 (0.1746)	loss 1.4345 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:22 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [5/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1784 (0.1739)	loss 1.1244 (1.1977)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:22 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 5 training takes 0:00:05
[2024-12-02 13:25:24 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:25:24 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:25:24 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:25:24 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][0/34]	eta 0:00:15 lr 0.000003	 wd 0.0500	time 0.4642 (0.4642)	loss 1.2188 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:27 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1740 (0.2445)	loss 0.8230 (1.1925)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:28 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1815 (0.2152)	loss 1.2182 (1.1895)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:30 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [6/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1709 (0.2045)	loss 1.2178 (1.1813)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:31 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 6 training takes 0:00:06
[2024-12-02 13:25:32 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:25:32 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:25:32 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:25:32 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3468 (0.3468)	loss 1.0111 (1.0111)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:34 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1672 (0.1967)	loss 1.0072 (1.1654)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:36 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1775 (0.1885)	loss 0.9054 (1.1399)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:38 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [7/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2577 (0.2101)	loss 1.2147 (1.1798)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:39 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 7 training takes 0:00:07
[2024-12-02 13:25:40 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:25:40 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:25:40 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:25:40 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][0/34]	eta 0:00:12 lr 0.000003	 wd 0.0500	time 0.3538 (0.3538)	loss 1.1182 (1.1182)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:42 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1674 (0.2012)	loss 1.0214 (1.2186)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:44 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1820 (0.1902)	loss 1.2093 (1.2085)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:46 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [8/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1832 (0.1889)	loss 1.0152 (1.1824)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:46 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 8 training takes 0:00:06
[2024-12-02 13:25:47 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:25:47 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:25:47 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:25:48 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][0/34]	eta 0:00:12 lr 0.000003	 wd 0.0500	time 0.3755 (0.3755)	loss 1.0264 (1.0264)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:50 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.2259 (0.2208)	loss 1.3252 (1.1444)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:52 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.2529 (0.2310)	loss 1.5173 (1.1804)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:54 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [9/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1859 (0.2186)	loss 1.5330 (1.1794)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:55 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 9 training takes 0:00:07
[2024-12-02 13:25:56 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:25:56 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:25:56 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:25:56 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3436 (0.3436)	loss 1.3284 (1.3284)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:25:58 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1830 (0.2014)	loss 1.1036 (1.1933)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:26:00 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1660 (0.1898)	loss 1.2124 (1.1887)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:26:01 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [10/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1943 (0.1867)	loss 1.1163 (1.1807)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:26:02 swin_tiny_patch4_window7_224] (main.py 256): INFO EPOCH 10 training takes 0:00:06
[2024-12-02 13:26:03 swin_tiny_patch4_window7_224] (main.py 305): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:26:03 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:26:03 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:26:03 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][0/34]	eta 0:00:16 lr 0.000003	 wd 0.0500	time 0.4960 (0.4960)	loss 1.4163 (1.4163)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:26:06 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][10/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 0.2228 (0.2891)	loss 0.8097 (1.2118)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:26:08 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1844 (0.2353)	loss 1.3154 (1.2331)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:26:10 swin_tiny_patch4_window7_224] (main.py 247): INFO Train: [11/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1683 (0.2139)	loss 0.7169 (1.1948)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:31:19 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 13:31:19 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 13:31:19 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 13:31:19 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 13:31:19 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 13:33:10 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 13:33:10 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 13:33:10 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 13:33:10 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 13:33:10 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 13:33:11 swin_tiny_patch4_window7_224] (main.py 248): INFO Train: [0/50][0/34]	eta 0:00:13 lr 0.000003	 wd 0.0500	time 0.3889 (0.3889)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:13 swin_tiny_patch4_window7_224] (main.py 248): INFO Train: [0/50][10/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 0.2168 (0.2603)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:15 swin_tiny_patch4_window7_224] (main.py 248): INFO Train: [0/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1752 (0.2279)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:17 swin_tiny_patch4_window7_224] (main.py 248): INFO Train: [0/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1789 (0.2100)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:17 swin_tiny_patch4_window7_224] (main.py 257): INFO EPOCH 0 training takes 0:00:07
[2024-12-02 13:33:18 swin_tiny_patch4_window7_224] (main.py 306): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:33:18 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:33:18 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:33:19 swin_tiny_patch4_window7_224] (main.py 248): INFO Train: [1/50][0/34]	eta 0:00:12 lr 0.000003	 wd 0.0500	time 0.3546 (0.3546)	loss 1.3204 (1.3204)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:20 swin_tiny_patch4_window7_224] (main.py 248): INFO Train: [1/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1741 (0.1969)	loss 1.2106 (1.1857)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:22 swin_tiny_patch4_window7_224] (main.py 248): INFO Train: [1/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1597 (0.1852)	loss 1.3177 (1.1705)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:24 swin_tiny_patch4_window7_224] (main.py 248): INFO Train: [1/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1672 (0.1817)	loss 1.0078 (1.1956)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:25 swin_tiny_patch4_window7_224] (main.py 257): INFO EPOCH 1 training takes 0:00:06
[2024-12-02 13:33:26 swin_tiny_patch4_window7_224] (main.py 306): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:33:26 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:33:26 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:33:27 swin_tiny_patch4_window7_224] (main.py 248): INFO Train: [2/50][0/34]	eta 0:00:16 lr 0.000003	 wd 0.0500	time 0.4936 (0.4936)	loss 0.9030 (0.9030)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:29 swin_tiny_patch4_window7_224] (main.py 248): INFO Train: [2/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1680 (0.2348)	loss 1.3133 (1.1674)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:30 swin_tiny_patch4_window7_224] (main.py 248): INFO Train: [2/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1653 (0.2034)	loss 1.1163 (1.1561)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:32 swin_tiny_patch4_window7_224] (main.py 248): INFO Train: [2/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1655 (0.1943)	loss 1.3294 (1.1946)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:33 swin_tiny_patch4_window7_224] (main.py 257): INFO EPOCH 2 training takes 0:00:06
[2024-12-02 13:33:34 swin_tiny_patch4_window7_224] (main.py 306): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:33:34 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:33:34 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:33:34 swin_tiny_patch4_window7_224] (main.py 248): INFO Train: [3/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3426 (0.3426)	loss 1.2180 (1.2180)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:36 swin_tiny_patch4_window7_224] (main.py 248): INFO Train: [3/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1686 (0.2157)	loss 0.8115 (1.1970)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:38 swin_tiny_patch4_window7_224] (main.py 248): INFO Train: [3/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.2353 (0.2024)	loss 1.2076 (1.2151)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:40 swin_tiny_patch4_window7_224] (main.py 248): INFO Train: [3/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2237 (0.2156)	loss 1.2020 (1.1984)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:41 swin_tiny_patch4_window7_224] (main.py 257): INFO EPOCH 3 training takes 0:00:07
[2024-12-02 13:33:42 swin_tiny_patch4_window7_224] (main.py 306): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:33:42 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:33:42 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:33:42 swin_tiny_patch4_window7_224] (main.py 248): INFO Train: [4/50][0/34]	eta 0:00:12 lr 0.000003	 wd 0.0500	time 0.3715 (0.3715)	loss 1.4127 (1.4127)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:50 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 13:33:50 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 13:33:50 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 13:33:50 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 13:33:50 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 13:33:51 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [0/50][0/34]	eta 0:00:09 lr 0.000003	 wd 0.0500	time 0.2737 (0.2737)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:53 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [0/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.2182 (0.2387)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:55 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [0/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1783 (0.2264)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:57 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [0/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1726 (0.2112)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:33:57 swin_tiny_patch4_window7_224] (main.py 258): INFO EPOCH 0 training takes 0:00:07
[2024-12-02 13:33:58 swin_tiny_patch4_window7_224] (main.py 307): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:33:58 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:33:58 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:33:59 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [1/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3098 (0.3098)	loss 1.3204 (1.3204)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:00 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [1/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1759 (0.1892)	loss 1.2106 (1.1857)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:02 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [1/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1599 (0.1777)	loss 1.3177 (1.1705)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:04 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [1/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1616 (0.1761)	loss 1.0078 (1.1956)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:04 swin_tiny_patch4_window7_224] (main.py 258): INFO EPOCH 1 training takes 0:00:06
[2024-12-02 13:34:06 swin_tiny_patch4_window7_224] (main.py 307): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:34:06 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:34:06 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:34:06 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [2/50][0/34]	eta 0:00:16 lr 0.000003	 wd 0.0500	time 0.4722 (0.4722)	loss 0.9030 (0.9030)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:08 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [2/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1695 (0.2472)	loss 1.3133 (1.1674)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:10 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [2/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1689 (0.2133)	loss 1.1163 (1.1561)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:12 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [2/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1709 (0.2012)	loss 1.3294 (1.1946)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:12 swin_tiny_patch4_window7_224] (main.py 258): INFO EPOCH 2 training takes 0:00:06
[2024-12-02 13:34:13 swin_tiny_patch4_window7_224] (main.py 307): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:34:13 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:34:13 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:34:14 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [3/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3240 (0.3240)	loss 1.2180 (1.2180)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:15 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [3/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1675 (0.1919)	loss 0.8115 (1.1970)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:17 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [3/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1776 (0.1868)	loss 1.2076 (1.2151)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:20 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [3/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2403 (0.2040)	loss 1.2020 (1.1984)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:20 swin_tiny_patch4_window7_224] (main.py 258): INFO EPOCH 3 training takes 0:00:07
[2024-12-02 13:34:21 swin_tiny_patch4_window7_224] (main.py 307): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:34:21 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:34:21 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:34:22 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [4/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3389 (0.3389)	loss 1.4127 (1.4127)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:24 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [4/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1763 (0.1993)	loss 1.1205 (1.1524)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:25 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [4/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1864 (0.1901)	loss 1.4133 (1.1858)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:27 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [4/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1728 (0.1836)	loss 1.0023 (1.1951)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:28 swin_tiny_patch4_window7_224] (main.py 258): INFO EPOCH 4 training takes 0:00:06
[2024-12-02 13:34:29 swin_tiny_patch4_window7_224] (main.py 307): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:34:29 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:34:29 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:34:29 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [5/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3375 (0.3375)	loss 1.0050 (1.0050)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:31 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [5/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.2148 (0.2106)	loss 1.3082 (1.1669)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:34 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [5/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.2552 (0.2303)	loss 1.4345 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:35 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [5/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1716 (0.2184)	loss 1.1244 (1.1977)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:36 swin_tiny_patch4_window7_224] (main.py 258): INFO EPOCH 5 training takes 0:00:07
[2024-12-02 13:34:37 swin_tiny_patch4_window7_224] (main.py 307): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:34:37 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:34:37 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:34:37 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [6/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3145 (0.3145)	loss 1.2188 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:39 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [6/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1802 (0.1949)	loss 0.8230 (1.1925)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:41 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [6/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1824 (0.1875)	loss 1.2182 (1.1895)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:43 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [6/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1747 (0.1849)	loss 1.2178 (1.1813)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:43 swin_tiny_patch4_window7_224] (main.py 258): INFO EPOCH 6 training takes 0:00:06
[2024-12-02 13:34:44 swin_tiny_patch4_window7_224] (main.py 307): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:34:44 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:34:44 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:34:45 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [7/50][0/34]	eta 0:00:16 lr 0.000003	 wd 0.0500	time 0.4918 (0.4918)	loss 1.0111 (1.0111)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:47 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [7/50][10/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 0.2446 (0.2808)	loss 1.0072 (1.1654)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:49 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [7/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1803 (0.2367)	loss 0.9054 (1.1399)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:51 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [7/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1766 (0.2177)	loss 1.2147 (1.1798)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:52 swin_tiny_patch4_window7_224] (main.py 258): INFO EPOCH 7 training takes 0:00:07
[2024-12-02 13:34:53 swin_tiny_patch4_window7_224] (main.py 307): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:34:53 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:34:53 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:34:53 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [8/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3237 (0.3237)	loss 1.1182 (1.1182)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:55 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [8/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1768 (0.1885)	loss 1.0214 (1.2186)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:56 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [8/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1578 (0.1813)	loss 1.2093 (1.2085)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:58 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [8/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2160 (0.1828)	loss 1.0152 (1.1824)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:34:59 swin_tiny_patch4_window7_224] (main.py 258): INFO EPOCH 8 training takes 0:00:06
[2024-12-02 13:35:00 swin_tiny_patch4_window7_224] (main.py 307): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:35:00 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:35:00 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:35:01 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [9/50][0/34]	eta 0:00:14 lr 0.000003	 wd 0.0500	time 0.4408 (0.4408)	loss 1.0264 (1.0264)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:35:03 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [9/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1608 (0.2147)	loss 1.3252 (1.1444)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:35:04 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [9/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1757 (0.1944)	loss 1.5173 (1.1804)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:35:06 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [9/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1899 (0.1899)	loss 1.5330 (1.1794)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:35:07 swin_tiny_patch4_window7_224] (main.py 258): INFO EPOCH 9 training takes 0:00:06
[2024-12-02 13:35:08 swin_tiny_patch4_window7_224] (main.py 307): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:35:08 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:35:08 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:35:08 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [10/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3159 (0.3159)	loss 1.3284 (1.3284)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:35:10 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [10/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1745 (0.1910)	loss 1.1036 (1.1933)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:36:04 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 13:36:04 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 13:36:04 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 13:36:04 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 13:36:04 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 13:36:04 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [0/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3095 (0.3095)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:36:06 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [0/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1644 (0.1904)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:37:18 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 13:37:18 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 13:37:18 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 13:37:18 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 13:37:18 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 13:37:19 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [0/50][0/34]	eta 0:00:25 lr 0.000003	 wd 0.0500	time 0.7443 (0.7443)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:37:25 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [0/50][10/34]	eta 0:00:14 lr 0.000003	 wd 0.0500	time 0.6175 (0.6094)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:37:31 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [0/50][20/34]	eta 0:00:08 lr 0.000003	 wd 0.0500	time 0.5018 (0.6023)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:37:37 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [0/50][30/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.6188 (0.5961)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:37:39 swin_tiny_patch4_window7_224] (main.py 258): INFO EPOCH 0 training takes 0:00:20
[2024-12-02 13:37:40 swin_tiny_patch4_window7_224] (main.py 307): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:37:40 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:37:40 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:37:41 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [1/50][0/34]	eta 0:00:28 lr 0.000003	 wd 0.0500	time 0.8325 (0.8325)	loss 1.3204 (1.3204)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:37:47 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [1/50][10/34]	eta 0:00:14 lr 0.000003	 wd 0.0500	time 0.6303 (0.6235)	loss 1.2106 (1.1857)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:37:53 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [1/50][20/34]	eta 0:00:08 lr 0.000003	 wd 0.0500	time 0.6245 (0.6123)	loss 1.3177 (1.1705)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:37:59 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [1/50][30/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.6150 (0.6033)	loss 1.0078 (1.1956)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:38:01 swin_tiny_patch4_window7_224] (main.py 258): INFO EPOCH 1 training takes 0:00:20
[2024-12-02 13:38:01 swin_tiny_patch4_window7_224] (main.py 307): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:38:01 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:38:01 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:38:02 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [2/50][0/34]	eta 0:00:23 lr 0.000003	 wd 0.0500	time 0.6797 (0.6797)	loss 0.9030 (0.9030)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:38:08 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [2/50][10/34]	eta 0:00:15 lr 0.000003	 wd 0.0500	time 0.6137 (0.6335)	loss 1.3133 (1.1674)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:38:14 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [2/50][20/34]	eta 0:00:08 lr 0.000003	 wd 0.0500	time 0.5222 (0.6057)	loss 1.1163 (1.1561)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:38:20 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [2/50][30/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.6142 (0.6024)	loss 1.3294 (1.1946)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:38:22 swin_tiny_patch4_window7_224] (main.py 258): INFO EPOCH 2 training takes 0:00:20
[2024-12-02 13:38:23 swin_tiny_patch4_window7_224] (main.py 307): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:38:23 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:38:23 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:38:24 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [3/50][0/34]	eta 0:00:24 lr 0.000003	 wd 0.0500	time 0.7196 (0.7196)	loss 1.2180 (1.2180)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:38:29 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [3/50][10/34]	eta 0:00:14 lr 0.000003	 wd 0.0500	time 0.6168 (0.5962)	loss 0.8115 (1.1970)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:38:36 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [3/50][20/34]	eta 0:00:08 lr 0.000003	 wd 0.0500	time 0.5205 (0.6017)	loss 1.2076 (1.2151)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:38:41 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [3/50][30/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.6135 (0.5895)	loss 1.2020 (1.1984)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:38:43 swin_tiny_patch4_window7_224] (main.py 258): INFO EPOCH 3 training takes 0:00:20
[2024-12-02 13:38:44 swin_tiny_patch4_window7_224] (main.py 307): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:38:45 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:38:45 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:38:45 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [4/50][0/34]	eta 0:00:31 lr 0.000003	 wd 0.0500	time 0.9320 (0.9320)	loss 1.4127 (1.4127)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:38:51 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [4/50][10/34]	eta 0:00:15 lr 0.000003	 wd 0.0500	time 0.5104 (0.6318)	loss 1.1205 (1.1524)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:38:57 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [4/50][20/34]	eta 0:00:08 lr 0.000003	 wd 0.0500	time 0.6083 (0.5854)	loss 1.4133 (1.1858)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:39:03 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [4/50][30/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.6158 (0.5951)	loss 1.0023 (1.1951)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:39:05 swin_tiny_patch4_window7_224] (main.py 258): INFO EPOCH 4 training takes 0:00:20
[2024-12-02 13:39:06 swin_tiny_patch4_window7_224] (main.py 307): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:39:06 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:39:06 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:39:06 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [5/50][0/34]	eta 0:00:22 lr 0.000003	 wd 0.0500	time 0.6740 (0.6740)	loss 1.0050 (1.0050)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:39:12 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [5/50][10/34]	eta 0:00:14 lr 0.000003	 wd 0.0500	time 0.5145 (0.6042)	loss 1.3082 (1.1669)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:39:18 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [5/50][20/34]	eta 0:00:08 lr 0.000003	 wd 0.0500	time 0.6093 (0.5993)	loss 1.4345 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:39:24 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [5/50][30/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.7147 (0.6015)	loss 1.1244 (1.1977)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:39:26 swin_tiny_patch4_window7_224] (main.py 258): INFO EPOCH 5 training takes 0:00:20
[2024-12-02 13:39:27 swin_tiny_patch4_window7_224] (main.py 307): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:39:27 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:39:27 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:39:28 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [6/50][0/34]	eta 0:00:25 lr 0.000003	 wd 0.0500	time 0.7548 (0.7548)	loss 1.2188 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:39:34 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [6/50][10/34]	eta 0:00:15 lr 0.000003	 wd 0.0500	time 0.6094 (0.6361)	loss 0.8230 (1.1925)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:39:40 swin_tiny_patch4_window7_224] (main.py 249): INFO Train: [6/50][20/34]	eta 0:00:08 lr 0.000003	 wd 0.0500	time 0.6140 (0.6222)	loss 1.2182 (1.1895)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:55:37 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 13:55:37 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 13:55:37 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 13:55:37 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 13:55:37 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 13:55:38 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][0/34]	eta 0:00:15 lr 0.000003	 wd 0.0500	time 0.4615 (0.4615)	loss 0.9096 (0.9096)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:55:40 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][10/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 0.2632 (0.2733)	loss 1.1134 (1.1430)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:55:42 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1695 (0.2277)	loss 1.1165 (1.1598)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:55:44 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1616 (0.2075)	loss 0.9104 (1.1777)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:55:44 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 0 training takes 0:00:06
[2024-12-02 13:55:45 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:55:45 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:55:45 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:55:46 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3288 (0.3288)	loss 1.3204 (1.3204)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:55:47 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1634 (0.1855)	loss 1.2106 (1.1857)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:55:49 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1579 (0.1758)	loss 1.3177 (1.1705)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:55:51 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1860 (0.1742)	loss 1.0078 (1.1956)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:55:51 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 1 training takes 0:00:06
[2024-12-02 13:55:53 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:55:53 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:55:53 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:55:53 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][0/34]	eta 0:00:14 lr 0.000003	 wd 0.0500	time 0.4226 (0.4226)	loss 0.9030 (0.9030)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:55:55 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1749 (0.2222)	loss 1.3133 (1.1674)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:55:57 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1773 (0.2008)	loss 1.1163 (1.1561)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:55:59 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1626 (0.1948)	loss 1.3294 (1.1946)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:55:59 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 2 training takes 0:00:06
[2024-12-02 13:56:00 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:56:00 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:56:00 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:56:00 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3254 (0.3254)	loss 1.2180 (1.2180)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:02 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1776 (0.2011)	loss 0.8115 (1.1970)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:04 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.2386 (0.1937)	loss 1.2076 (1.2151)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:07 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2340 (0.2105)	loss 1.2020 (1.1984)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:07 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 3 training takes 0:00:07
[2024-12-02 13:56:08 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:56:08 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:56:08 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:56:09 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3201 (0.3201)	loss 1.4127 (1.4127)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:10 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1659 (0.1869)	loss 1.1205 (1.1524)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:12 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1631 (0.1780)	loss 1.4133 (1.1858)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:14 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1825 (0.1752)	loss 1.0023 (1.1951)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:14 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 4 training takes 0:00:05
[2024-12-02 13:56:15 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:56:15 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:56:15 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:56:16 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [5/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3271 (0.3271)	loss 1.0050 (1.0050)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:17 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [5/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1799 (0.1881)	loss 1.3082 (1.1669)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:20 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [5/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.2458 (0.2077)	loss 1.4345 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:22 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [5/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1720 (0.2066)	loss 1.1244 (1.1977)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:22 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 5 training takes 0:00:06
[2024-12-02 13:56:23 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:56:23 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:56:23 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:56:24 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [6/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3445 (0.3445)	loss 1.2188 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:25 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [6/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1644 (0.1982)	loss 0.8230 (1.1925)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:27 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [6/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1680 (0.1829)	loss 1.2182 (1.1895)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:29 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [6/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1752 (0.1804)	loss 1.2178 (1.1813)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:29 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 6 training takes 0:00:06
[2024-12-02 13:56:30 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:56:30 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:56:30 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:56:31 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [7/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3110 (0.3110)	loss 1.0111 (1.0111)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:33 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [7/50][10/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 0.2811 (0.2615)	loss 1.0072 (1.1654)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:35 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [7/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1616 (0.2380)	loss 0.9054 (1.1399)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:37 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [7/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1628 (0.2142)	loss 1.2147 (1.1798)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:38 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 7 training takes 0:00:07
[2024-12-02 13:56:38 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:56:38 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:56:38 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:56:39 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [8/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3519 (0.3519)	loss 1.1182 (1.1182)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:41 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [8/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1724 (0.2020)	loss 1.0214 (1.2186)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:43 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [8/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1786 (0.1945)	loss 1.2093 (1.2085)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:44 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [8/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1651 (0.1903)	loss 1.0152 (1.1824)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:45 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 8 training takes 0:00:06
[2024-12-02 13:56:46 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:56:46 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:56:46 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:56:47 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [9/50][0/34]	eta 0:00:16 lr 0.000003	 wd 0.0500	time 0.4733 (0.4733)	loss 1.0264 (1.0264)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:49 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [9/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1730 (0.2444)	loss 1.3252 (1.1444)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:51 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [9/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1744 (0.2105)	loss 1.5173 (1.1804)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:52 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [9/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1975 (0.1974)	loss 1.5330 (1.1794)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:53 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 9 training takes 0:00:06
[2024-12-02 13:56:54 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:56:54 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:56:54 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:56:54 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [10/50][0/34]	eta 0:00:12 lr 0.000003	 wd 0.0500	time 0.3596 (0.3596)	loss 1.3284 (1.3284)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:56 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [10/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1612 (0.2021)	loss 1.1036 (1.1933)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:56:58 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [10/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1667 (0.1925)	loss 1.2124 (1.1887)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:57:00 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [10/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2707 (0.2091)	loss 1.1163 (1.1807)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:57:01 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 10 training takes 0:00:07
[2024-12-02 13:57:02 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:57:02 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:57:02 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:57:03 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [11/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3376 (0.3376)	loss 1.4163 (1.4163)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:57:04 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [11/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1785 (0.1952)	loss 0.8097 (1.2118)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:57:06 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [11/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1807 (0.1904)	loss 1.3154 (1.2331)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:57:08 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [11/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1716 (0.1883)	loss 0.7169 (1.1948)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:57:09 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 11 training takes 0:00:06
[2024-12-02 13:57:09 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:57:09 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:57:09 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:57:10 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [12/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3064 (0.3064)	loss 1.1152 (1.1152)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:57:12 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [12/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1708 (0.1880)	loss 1.3062 (1.2064)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:57:14 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [12/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.2238 (0.2175)	loss 1.0982 (1.2008)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:57:16 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [12/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1667 (0.2106)	loss 1.4206 (1.1763)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:57:17 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 12 training takes 0:00:07
[2024-12-02 13:57:17 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:57:17 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:57:17 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:57:18 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [13/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3056 (0.3056)	loss 1.2212 (1.2212)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:57:20 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [13/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1968 (0.1919)	loss 1.2048 (1.1112)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:57:21 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [13/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1855 (0.1854)	loss 1.3212 (1.1858)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:57:23 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [13/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1766 (0.1851)	loss 1.3129 (1.1824)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:57:24 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 13 training takes 0:00:06
[2024-12-02 13:57:25 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:57:25 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:57:25 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:57:25 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [14/50][0/34]	eta 0:00:11 lr 0.000003	 wd 0.0500	time 0.3514 (0.3514)	loss 1.3150 (1.3150)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:57:28 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [14/50][10/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 0.2934 (0.2704)	loss 0.9079 (1.0414)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:57:30 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [14/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1775 (0.2465)	loss 1.5214 (1.1916)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:57:32 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [14/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1693 (0.2218)	loss 1.2292 (1.1842)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:57:32 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 14 training takes 0:00:07
[2024-12-02 13:57:33 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:57:33 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:57:33 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:57:33 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [15/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3184 (0.3184)	loss 0.8971 (0.8971)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:57:35 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [15/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1740 (0.1916)	loss 1.4212 (1.1306)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:58:02 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 13:58:02 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 932368
[2024-12-02 13:58:02 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.064430944
[2024-12-02 13:58:02 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 13:58:02 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 13:59:02 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 13:59:02 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 2084144
[2024-12-02 13:59:02 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.141317952
[2024-12-02 13:59:02 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 13:59:02 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 13:59:02 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][0/34]	eta 0:00:15 lr 0.000003	 wd 0.0500	time 0.4673 (0.4673)	loss 1.2547 (1.2547)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:59:05 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][10/34]	eta 0:00:07 lr 0.000003	 wd 0.0500	time 0.2908 (0.2917)	loss 1.0736 (1.0871)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:59:08 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.2117 (0.2763)	loss 1.0678 (1.0528)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:59:10 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][30/34]	eta 0:00:01 lr 0.000003	 wd 0.0500	time 0.2140 (0.2527)	loss 1.1589 (1.0432)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:59:10 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 0 training takes 0:00:08
[2024-12-02 13:59:11 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:59:11 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:59:11 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:59:12 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][0/34]	eta 0:00:14 lr 0.000003	 wd 0.0500	time 0.4230 (0.4230)	loss 1.2463 (1.2463)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:59:14 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1957 (0.2423)	loss 1.0759 (1.1097)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:59:16 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.2143 (0.2227)	loss 1.1593 (1.0530)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:59:19 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2794 (0.2338)	loss 1.0626 (1.0476)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:59:20 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 1 training takes 0:00:08
[2024-12-02 13:59:21 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:59:21 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:59:21 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:59:21 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][0/34]	eta 0:00:12 lr 0.000003	 wd 0.0500	time 0.3713 (0.3713)	loss 0.7966 (0.7966)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:59:24 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.2317 (0.2445)	loss 0.8903 (1.0130)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:59:26 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.2164 (0.2294)	loss 0.7086 (1.0191)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:59:28 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1991 (0.2240)	loss 1.0663 (1.0413)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:59:29 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 2 training takes 0:00:07
[2024-12-02 13:59:30 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:59:30 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:59:30 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:59:30 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][0/34]	eta 0:00:13 lr 0.000003	 wd 0.0500	time 0.3907 (0.3907)	loss 0.8912 (0.8912)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:59:33 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][10/34]	eta 0:00:07 lr 0.000003	 wd 0.0500	time 0.2964 (0.3091)	loss 0.9796 (1.0458)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:59:36 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1976 (0.2799)	loss 1.1585 (1.0320)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:59:38 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][30/34]	eta 0:00:01 lr 0.000003	 wd 0.0500	time 0.2308 (0.2561)	loss 0.8080 (1.0410)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 13:59:38 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 3 training takes 0:00:08
[2024-12-02 13:59:39 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 13:59:39 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 13:59:39 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 13:59:40 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][0/34]	eta 0:00:12 lr 0.000003	 wd 0.0500	time 0.3726 (0.3726)	loss 1.1704 (1.1704)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:00:54 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 14:02:16 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 14:02:16 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 2087504
[2024-12-02 14:02:16 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.15238176
[2024-12-02 14:02:16 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 14:02:16 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 14:02:16 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3140 (0.3140)	loss 1.1433 (1.1433)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:18 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1936 (0.1927)	loss 0.8821 (1.0330)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:20 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.2467 (0.2083)	loss 1.2331 (1.0198)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:23 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1613 (0.2178)	loss 0.9722 (1.0234)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:23 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 0 training takes 0:00:07
[2024-12-02 14:02:24 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 26.923 Acc@5 26.923
[2024-12-02 14:02:24 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 26.9%
[2024-12-02 14:02:24 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 26.92%
[2024-12-02 14:02:25 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][0/34]	eta 0:00:13 lr 0.000003	 wd 0.0500	time 0.4007 (0.4007)	loss 0.7903 (0.7903)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:27 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1782 (0.2275)	loss 0.9707 (1.0090)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:29 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1580 (0.1994)	loss 0.7930 (1.0609)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:30 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1902 (0.1917)	loss 0.8810 (1.0397)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:31 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 1 training takes 0:00:06
[2024-12-02 14:02:32 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 26.923 Acc@5 26.923
[2024-12-02 14:02:32 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 26.9%
[2024-12-02 14:02:32 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 26.92%
[2024-12-02 14:02:32 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][0/34]	eta 0:00:14 lr 0.000003	 wd 0.0500	time 0.4170 (0.4170)	loss 1.0604 (1.0604)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:35 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][10/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 0.3004 (0.2907)	loss 0.9719 (1.0647)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:37 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1909 (0.2614)	loss 1.1456 (1.0404)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:39 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1621 (0.2319)	loss 1.1440 (1.0395)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:40 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 2 training takes 0:00:07
[2024-12-02 14:02:41 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 26.923 Acc@5 26.923
[2024-12-02 14:02:41 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 26.9%
[2024-12-02 14:02:41 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 26.92%
[2024-12-02 14:02:41 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][0/34]	eta 0:00:12 lr 0.000003	 wd 0.0500	time 0.3612 (0.3612)	loss 1.0531 (1.0531)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:43 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1591 (0.1950)	loss 1.2340 (0.9926)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:44 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1609 (0.1798)	loss 1.1475 (1.0145)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:46 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1767 (0.1800)	loss 0.8800 (1.0370)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:47 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 3 training takes 0:00:06
[2024-12-02 14:02:48 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 26.923 Acc@5 26.923
[2024-12-02 14:02:48 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 26.9%
[2024-12-02 14:02:48 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 26.92%
[2024-12-02 14:02:49 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][0/34]	eta 0:00:18 lr 0.000003	 wd 0.0500	time 0.5482 (0.5482)	loss 0.7950 (0.7950)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:51 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][10/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 0.1826 (0.2511)	loss 1.0584 (1.0254)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:53 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1736 (0.2216)	loss 0.9674 (1.0320)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:55 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1815 (0.2064)	loss 0.8863 (1.0265)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:55 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 4 training takes 0:00:06
[2024-12-02 14:02:56 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 26.923 Acc@5 26.923
[2024-12-02 14:02:56 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 26.9%
[2024-12-02 14:02:56 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 26.92%
[2024-12-02 14:02:57 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [5/50][0/34]	eta 0:00:14 lr 0.000003	 wd 0.0500	time 0.4143 (0.4143)	loss 1.2266 (1.2266)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:02:58 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [5/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1594 (0.2026)	loss 1.0533 (1.0571)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:03:00 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [5/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.2848 (0.1946)	loss 1.1440 (1.0492)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:03:03 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [5/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2593 (0.2178)	loss 0.9711 (1.0345)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:03:04 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 5 training takes 0:00:07
[2024-12-02 14:03:05 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 26.923 Acc@5 26.923
[2024-12-02 14:03:05 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 26.9%
[2024-12-02 14:03:05 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 26.92%
[2024-12-02 14:03:05 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [6/50][0/34]	eta 0:00:13 lr 0.000003	 wd 0.0500	time 0.4111 (0.4111)	loss 1.0567 (1.0567)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:03:07 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [6/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1995 (0.2251)	loss 1.4112 (1.0577)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:03:09 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [6/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1802 (0.2074)	loss 0.9705 (1.0447)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:03:11 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [6/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1830 (0.2004)	loss 1.0579 (1.0343)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:03:11 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 6 training takes 0:00:06
[2024-12-02 14:03:31 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 14:03:31 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 2087504
[2024-12-02 14:03:31 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.15238176
[2024-12-02 14:03:31 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 14:03:31 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 14:03:32 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][0/34]	eta 0:00:12 lr 0.000003	 wd 0.0500	time 0.3620 (0.3620)	loss 1.1433 (1.1433)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:03:34 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1715 (0.2071)	loss 0.8821 (1.0330)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:03:35 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1619 (0.1875)	loss 1.2331 (1.0198)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:03:37 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1635 (0.1828)	loss 0.9722 (1.0234)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:03:38 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 0 training takes 0:00:06
[2024-12-02 14:03:39 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 26.923 Acc@5 26.923
[2024-12-02 14:03:39 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 26.9%
[2024-12-02 14:03:39 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 26.92%
[2024-12-02 14:03:40 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][0/34]	eta 0:00:17 lr 0.000003	 wd 0.0500	time 0.5015 (0.5015)	loss 0.7903 (0.7903)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:03:42 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1799 (0.2342)	loss 0.9707 (1.0090)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:03:44 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1624 (0.2040)	loss 0.7930 (1.0609)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:03:45 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1747 (0.1927)	loss 0.8810 (1.0397)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:03:46 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 1 training takes 0:00:06
[2024-12-02 14:03:47 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 26.923 Acc@5 26.923
[2024-12-02 14:03:47 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 26.9%
[2024-12-02 14:03:47 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 26.92%
[2024-12-02 14:03:47 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][0/34]	eta 0:00:13 lr 0.000003	 wd 0.0500	time 0.3847 (0.3847)	loss 1.0604 (1.0604)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:03:49 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1778 (0.2138)	loss 0.9719 (1.0647)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:03:51 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.2652 (0.2050)	loss 1.1456 (1.0404)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:03:54 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2542 (0.2291)	loss 1.1440 (1.0395)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:04:20 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 14:04:20 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 2087504
[2024-12-02 14:04:20 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.15238176
[2024-12-02 14:04:20 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 14:04:20 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 14:04:21 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][0/34]	eta 0:00:10 lr 0.000003	 wd 0.0500	time 0.3231 (0.3231)	loss 1.1433 (1.1433)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:04:22 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][10/34]	eta 0:00:04 lr 0.000003	 wd 0.0500	time 0.1788 (0.1900)	loss 0.8821 (1.0330)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:04:24 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1885 (0.1886)	loss 1.2331 (1.0198)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:04:26 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1790 (0.1881)	loss 0.9722 (1.0234)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:04:27 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 0 training takes 0:00:06
[2024-12-02 14:04:28 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 26.923 Acc@5 26.923
[2024-12-02 14:04:28 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 26.9%
[2024-12-02 14:04:28 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 26.92%
[2024-12-02 14:04:28 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][0/34]	eta 0:00:12 lr 0.000003	 wd 0.0500	time 0.3730 (0.3730)	loss 0.7903 (0.7903)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:04:30 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.2478 (0.2333)	loss 0.9707 (1.0090)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:04:33 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.2854 (0.2539)	loss 0.7930 (1.0609)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:04:35 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1839 (0.2337)	loss 0.8810 (1.0397)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:04:36 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 1 training takes 0:00:07
[2024-12-02 14:04:37 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 26.923 Acc@5 26.923
[2024-12-02 14:04:37 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 26.9%
[2024-12-02 14:04:37 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 26.92%
[2024-12-02 14:04:37 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][0/34]	eta 0:00:13 lr 0.000003	 wd 0.0500	time 0.3999 (0.3999)	loss 1.0604 (1.0604)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:04:39 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1969 (0.2255)	loss 0.9719 (1.0647)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:04:41 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.2035 (0.2118)	loss 1.1456 (1.0404)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:04:43 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1742 (0.2057)	loss 1.1440 (1.0395)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:04:44 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 2 training takes 0:00:07
[2024-12-02 14:04:45 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 26.923 Acc@5 26.923
[2024-12-02 14:04:45 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 26.9%
[2024-12-02 14:04:45 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 26.92%
[2024-12-02 14:04:46 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][0/34]	eta 0:00:20 lr 0.000003	 wd 0.0500	time 0.5893 (0.5893)	loss 1.0531 (1.0531)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:04:48 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1641 (0.2448)	loss 1.2340 (0.9926)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:04:50 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.1980 (0.2168)	loss 1.1475 (1.0145)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:04:52 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1818 (0.2057)	loss 0.8800 (1.0370)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:04:52 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 3 training takes 0:00:06
[2024-12-02 14:04:53 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 26.923 Acc@5 26.923
[2024-12-02 14:04:53 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 26.9%
[2024-12-02 14:04:53 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 26.92%
[2024-12-02 14:04:54 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][0/34]	eta 0:00:13 lr 0.000003	 wd 0.0500	time 0.4072 (0.4072)	loss 0.7950 (0.7950)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:04:56 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1671 (0.2161)	loss 1.0584 (1.0254)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:04:58 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.2495 (0.2106)	loss 0.9674 (1.0320)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:05:00 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2688 (0.2287)	loss 0.8863 (1.0265)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:05:01 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 4 training takes 0:00:07
[2024-12-02 14:05:02 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 26.923 Acc@5 26.923
[2024-12-02 14:05:02 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 26.9%
[2024-12-02 14:05:02 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 26.92%
[2024-12-02 14:05:02 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [5/50][0/34]	eta 0:00:14 lr 0.000003	 wd 0.0500	time 0.4236 (0.4236)	loss 1.2266 (1.2266)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:05:04 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [5/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.1887 (0.2129)	loss 1.0533 (1.0571)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:05:06 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [5/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1806 (0.1940)	loss 1.1440 (1.0492)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:05:08 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [5/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1817 (0.1935)	loss 0.9711 (1.0345)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:05:09 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 5 training takes 0:00:06
[2024-12-02 14:05:10 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 26.923 Acc@5 26.923
[2024-12-02 14:05:10 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 26.9%
[2024-12-02 14:05:10 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 26.92%
[2024-12-02 14:05:10 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [6/50][0/34]	eta 0:00:14 lr 0.000003	 wd 0.0500	time 0.4137 (0.4137)	loss 1.0567 (1.0567)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:05:13 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [6/50][10/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 0.2525 (0.2787)	loss 1.4112 (1.0577)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:05:15 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [6/50][20/34]	eta 0:00:03 lr 0.000003	 wd 0.0500	time 0.2222 (0.2531)	loss 0.9705 (1.0447)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:05:17 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [6/50][30/34]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1934 (0.2306)	loss 1.0579 (1.0343)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:05:17 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 6 training takes 0:00:07
[2024-12-02 14:05:19 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 26.923 Acc@5 26.923
[2024-12-02 14:05:19 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 26.9%
[2024-12-02 14:05:19 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 26.92%
[2024-12-02 14:05:19 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [7/50][0/34]	eta 0:00:13 lr 0.000003	 wd 0.0500	time 0.3943 (0.3943)	loss 0.9673 (0.9673)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:05:21 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [7/50][10/34]	eta 0:00:05 lr 0.000003	 wd 0.0500	time 0.2088 (0.2191)	loss 1.0532 (1.0878)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:05:23 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [7/50][20/34]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.1753 (0.1983)	loss 1.0546 (1.0612)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:12:42 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 14:12:43 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 33097328
[2024-12-02 14:12:43 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 2.213553408
[2024-12-02 14:12:43 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 14:12:43 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 14:12:45 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][0/34]	eta 0:01:28 lr 0.000003	 wd 0.0500	time 2.5953 (2.5953)	loss 1.0855 (1.0855)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:13:01 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][10/34]	eta 0:00:40 lr 0.000003	 wd 0.0500	time 1.3936 (1.6810)	loss 1.1843 (1.1228)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:13:16 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][20/34]	eta 0:00:22 lr 0.000003	 wd 0.0500	time 1.3147 (1.5902)	loss 0.9865 (1.0646)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:13:31 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][30/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 1.3608 (1.5549)	loss 0.9035 (1.0670)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:13:35 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 0 training takes 0:00:52
[2024-12-02 14:13:43 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 14:13:43 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 14:13:43 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 14:13:45 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][0/34]	eta 0:01:08 lr 0.000003	 wd 0.0500	time 2.0192 (2.0192)	loss 0.9955 (0.9955)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:14:01 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][10/34]	eta 0:00:38 lr 0.000003	 wd 0.0500	time 1.4162 (1.5969)	loss 1.0919 (1.0669)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:14:16 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][20/34]	eta 0:00:21 lr 0.000003	 wd 0.0500	time 1.3667 (1.5489)	loss 1.1782 (1.0572)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:14:31 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][30/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 1.2971 (1.5331)	loss 1.0761 (1.0571)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:14:36 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 1 training takes 0:00:52
[2024-12-02 14:14:43 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 14:14:43 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 14:14:43 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 14:14:45 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][0/34]	eta 0:01:05 lr 0.000003	 wd 0.0500	time 1.9126 (1.9126)	loss 0.6413 (0.6413)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:15:01 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][10/34]	eta 0:00:39 lr 0.000003	 wd 0.0500	time 2.1980 (1.6515)	loss 1.1778 (1.0326)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:15:54 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 14:15:54 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 33093968
[2024-12-02 14:15:54 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 2.169298176
[2024-12-02 14:15:54 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 14:15:54 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 14:15:56 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][0/34]	eta 0:01:03 lr 0.000003	 wd 0.0500	time 1.8636 (1.8636)	loss 1.0172 (1.0172)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:16:13 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][10/34]	eta 0:00:41 lr 0.000003	 wd 0.0500	time 1.4495 (1.7265)	loss 0.7282 (1.0100)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:16:30 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][20/34]	eta 0:00:23 lr 0.000003	 wd 0.0500	time 1.6189 (1.6855)	loss 1.3203 (1.0919)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:16:48 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][30/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 1.8877 (1.7118)	loss 1.0182 (1.0872)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:16:52 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 0 training takes 0:00:57
[2024-12-02 14:17:01 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 14:17:01 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 14:17:01 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 14:17:03 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][0/34]	eta 0:01:08 lr 0.000003	 wd 0.0500	time 2.0139 (2.0139)	loss 1.0932 (1.0932)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:17:19 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][10/34]	eta 0:00:40 lr 0.000003	 wd 0.0500	time 1.4603 (1.6868)	loss 0.9209 (1.0959)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:17:36 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][20/34]	eta 0:00:23 lr 0.000003	 wd 0.0500	time 1.5230 (1.6666)	loss 0.7312 (1.1031)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:17:53 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][30/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 2.2995 (1.6854)	loss 0.9296 (1.0890)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:17:58 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 1 training takes 0:00:57
[2024-12-02 14:18:06 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 14:18:06 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 14:18:06 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 14:18:09 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][0/34]	eta 0:01:27 lr 0.000003	 wd 0.0500	time 2.5799 (2.5799)	loss 1.1311 (1.1311)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:18:25 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][10/34]	eta 0:00:41 lr 0.000003	 wd 0.0500	time 1.4599 (1.7352)	loss 0.8617 (1.0170)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:18:42 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][20/34]	eta 0:00:23 lr 0.000003	 wd 0.0500	time 1.5391 (1.6746)	loss 1.1192 (1.0790)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:18:58 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][30/34]	eta 0:00:06 lr 0.000003	 wd 0.0500	time 1.5188 (1.6676)	loss 0.9964 (1.0860)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:19:04 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 2 training takes 0:00:57
[2024-12-02 14:19:11 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 14:19:11 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 14:19:11 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 14:19:14 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][0/34]	eta 0:01:40 lr 0.000003	 wd 0.0500	time 2.9573 (2.9573)	loss 1.2001 (1.2001)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:19:31 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][10/34]	eta 0:00:43 lr 0.000003	 wd 0.0500	time 1.5469 (1.7997)	loss 0.9975 (1.0888)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:19:48 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][20/34]	eta 0:00:24 lr 0.000003	 wd 0.0500	time 1.5525 (1.7323)	loss 1.2027 (1.1058)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:20:22 swin_tiny_patch4_window7_224] (main.py 387): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 4
    EMBED_DIM: 384
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 14:20:22 swin_tiny_patch4_window7_224] (main.py 388): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 14:20:22 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 14:20:23 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 33093968
[2024-12-02 14:20:23 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 2.169298176
[2024-12-02 14:20:23 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 14:20:23 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 14:20:25 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][0/34]	eta 0:01:04 lr 0.000004	 wd 0.0001	time 1.8863 (1.8863)	loss 1.0172 (1.0172)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:20:41 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][10/34]	eta 0:00:40 lr 0.000004	 wd 0.0001	time 1.5662 (1.6819)	loss 0.7282 (1.0100)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:20:58 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][20/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 1.5246 (1.6580)	loss 1.3203 (1.0919)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:21:15 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 2.0374 (1.6975)	loss 1.0182 (1.0872)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:21:20 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 0 training takes 0:00:57
[2024-12-02 14:21:29 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 14:21:29 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 14:21:29 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 14:21:31 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][0/34]	eta 0:01:13 lr 0.000004	 wd 0.0001	time 2.1631 (2.1631)	loss 1.0932 (1.0932)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:21:47 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][10/34]	eta 0:00:40 lr 0.000004	 wd 0.0001	time 1.5409 (1.6963)	loss 0.9209 (1.0959)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:22:07 swin_tiny_patch4_window7_224] (main.py 387): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 4
    EMBED_DIM: 384
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 14
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 14:22:07 swin_tiny_patch4_window7_224] (main.py 388): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 14:22:07 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 14:22:08 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 33097328
[2024-12-02 14:22:08 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 2.213553408
[2024-12-02 14:22:08 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 14:22:08 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 14:22:10 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][0/34]	eta 0:01:10 lr 0.000004	 wd 0.0001	time 2.0774 (2.0774)	loss 1.0855 (1.0855)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:22:25 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][10/34]	eta 0:00:37 lr 0.000004	 wd 0.0001	time 1.3279 (1.5678)	loss 1.1843 (1.1228)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:22:40 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.4116 (1.5179)	loss 0.9865 (1.0646)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:22:54 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3579 (1.5069)	loss 0.9035 (1.0670)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:22:58 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 0 training takes 0:00:50
[2024-12-02 14:23:06 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 14:23:06 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 14:23:06 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 14:23:08 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][0/34]	eta 0:01:04 lr 0.000004	 wd 0.0001	time 1.8869 (1.8869)	loss 0.9955 (0.9955)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:23:24 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][10/34]	eta 0:00:37 lr 0.000004	 wd 0.0001	time 1.2967 (1.5572)	loss 1.0919 (1.0669)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:23:38 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.3497 (1.5043)	loss 1.1782 (1.0572)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:23:53 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][30/34]	eta 0:00:05 lr 0.000004	 wd 0.0001	time 1.3750 (1.4949)	loss 1.0761 (1.0571)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:23:58 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 1 training takes 0:00:51
[2024-12-02 14:24:05 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 14:24:05 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 14:24:05 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 14:24:07 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][0/34]	eta 0:01:03 lr 0.000004	 wd 0.0001	time 1.8703 (1.8703)	loss 0.6413 (0.6413)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:24:23 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][10/34]	eta 0:00:38 lr 0.000004	 wd 0.0001	time 2.0407 (1.6169)	loss 1.1778 (1.0326)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:24:38 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][20/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 1.8845 (1.5806)	loss 1.2703 (1.0401)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:24:53 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3588 (1.5479)	loss 0.9743 (1.0522)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:24:57 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 2 training takes 0:00:52
[2024-12-02 14:25:05 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 14:25:05 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 14:25:05 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 14:25:07 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][0/34]	eta 0:01:05 lr 0.000004	 wd 0.0001	time 1.9355 (1.9355)	loss 1.0090 (1.0090)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:25:22 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][10/34]	eta 0:00:37 lr 0.000004	 wd 0.0001	time 1.2839 (1.5582)	loss 0.8220 (0.9971)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:25:37 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.3366 (1.5168)	loss 1.3722 (1.0442)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:25:52 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3962 (1.5012)	loss 1.1834 (1.0703)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:25:56 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 3 training takes 0:00:50
[2024-12-02 14:26:04 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 14:26:04 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 14:26:04 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 14:26:06 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][0/34]	eta 0:01:02 lr 0.000004	 wd 0.0001	time 1.8411 (1.8411)	loss 0.9931 (0.9931)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:26:21 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][10/34]	eta 0:00:38 lr 0.000004	 wd 0.0001	time 1.3022 (1.5847)	loss 0.8166 (1.0769)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:26:37 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.3689 (1.5538)	loss 1.0840 (1.0796)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:26:53 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 2.2055 (1.5746)	loss 0.7354 (1.0622)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:26:57 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 4 training takes 0:00:53
[2024-12-02 14:27:05 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 14:27:05 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 14:27:05 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 14:27:07 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [5/50][0/34]	eta 0:01:37 lr 0.000004	 wd 0.0001	time 2.8764 (2.8764)	loss 1.1726 (1.1726)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:27:23 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [5/50][10/34]	eta 0:00:39 lr 0.000004	 wd 0.0001	time 1.4300 (1.6663)	loss 1.0808 (1.0423)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:27:38 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [5/50][20/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 1.3167 (1.5742)	loss 1.0906 (1.0516)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:27:49 swin_tiny_patch4_window7_224] (main.py 387): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 4
    - 8
    EMBED_DIM: 128
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 14
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 14
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 14:27:49 swin_tiny_patch4_window7_224] (main.py 388): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 14:27:49 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 14:27:49 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 7264962
[2024-12-02 14:27:49 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.518428352
[2024-12-02 14:27:49 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 14:27:49 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 14:29:28 swin_tiny_patch4_window7_224] (main.py 387): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 4
    - 8
    EMBED_DIM: 128
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 14
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 14
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 14:29:28 swin_tiny_patch4_window7_224] (main.py 388): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 14:29:28 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 14:29:29 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 7262046
[2024-12-02 14:29:29 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.517813696
[2024-12-02 14:29:29 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 14:29:29 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 14:29:49 swin_tiny_patch4_window7_224] (main.py 387): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 4
    EMBED_DIM: 128
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 14
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 14
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 14:29:49 swin_tiny_patch4_window7_224] (main.py 388): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 14:29:49 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 14:29:49 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 3698032
[2024-12-02 14:29:49 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.262199904
[2024-12-02 14:29:49 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 14:29:49 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 14:30:10 swin_tiny_patch4_window7_224] (main.py 387): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 4
    EMBED_DIM: 128
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 14
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 14:30:10 swin_tiny_patch4_window7_224] (main.py 388): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 14:30:10 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 14:30:10 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 3698032
[2024-12-02 14:30:10 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.262199904
[2024-12-02 14:30:10 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 14:30:10 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 14:30:32 swin_tiny_patch4_window7_224] (main.py 387): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 4
    EMBED_DIM: 384
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 14
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 14:30:32 swin_tiny_patch4_window7_224] (main.py 388): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 14:30:32 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 14:30:32 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 33097328
[2024-12-02 14:30:32 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 2.213553408
[2024-12-02 14:30:32 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 14:30:32 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 14:30:34 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][0/34]	eta 0:01:02 lr 0.000004	 wd 0.0001	time 1.8303 (1.8303)	loss 1.0855 (1.0855)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:30:51 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][10/34]	eta 0:00:39 lr 0.000004	 wd 0.0001	time 2.1992 (1.6582)	loss 1.1843 (1.1228)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:31:06 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][20/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 1.6966 (1.6097)	loss 0.9865 (1.0646)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:31:21 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.4004 (1.5774)	loss 0.9035 (1.0670)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:31:25 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 0 training takes 0:00:53
[2024-12-02 14:31:34 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 14:31:34 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 14:31:34 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 14:31:36 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][0/34]	eta 0:01:06 lr 0.000004	 wd 0.0001	time 1.9683 (1.9683)	loss 0.9955 (0.9955)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:31:51 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][10/34]	eta 0:00:37 lr 0.000004	 wd 0.0001	time 1.3759 (1.5720)	loss 1.0919 (1.0669)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:32:06 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.3470 (1.5326)	loss 1.1782 (1.0572)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:32:21 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3113 (1.5205)	loss 1.0761 (1.0571)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:32:26 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 1 training takes 0:00:52
[2024-12-02 14:32:33 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 14:32:33 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 14:32:33 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 14:32:35 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][0/34]	eta 0:01:08 lr 0.000004	 wd 0.0001	time 2.0122 (2.0122)	loss 0.6413 (0.6413)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:32:51 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][10/34]	eta 0:00:38 lr 0.000004	 wd 0.0001	time 1.4149 (1.5850)	loss 1.1778 (1.0326)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 14:33:06 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][20/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 2.0865 (1.5838)	loss 1.2703 (1.0401)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:32:00 swin_tiny_patch4_window7_224] (main.py 387): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 4
    EMBED_DIM: 384
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 14
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 15:32:00 swin_tiny_patch4_window7_224] (main.py 388): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 15:32:00 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 15:32:01 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 33097328
[2024-12-02 15:32:01 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 2.213553408
[2024-12-02 15:32:01 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 15:32:01 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 15:32:04 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][0/34]	eta 0:01:18 lr 0.000004	 wd 0.0001	time 2.3109 (2.3109)	loss 1.0855 (1.0855)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:32:20 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][10/34]	eta 0:00:40 lr 0.000004	 wd 0.0001	time 1.3863 (1.6733)	loss 1.1843 (1.1228)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:32:37 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][20/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 1.3164 (1.6782)	loss 0.9865 (1.0646)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:32:51 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3817 (1.6167)	loss 0.9035 (1.0670)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:32:57 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 0 training takes 0:00:55
[2024-12-02 15:33:05 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 15:33:05 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 15:33:05 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 15:33:07 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][0/34]	eta 0:01:20 lr 0.000004	 wd 0.0001	time 2.3670 (2.3670)	loss 0.9955 (0.9955)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:33:23 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][10/34]	eta 0:00:40 lr 0.000004	 wd 0.0001	time 1.5761 (1.6870)	loss 1.0919 (1.0669)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:34:02 swin_tiny_patch4_window7_224] (main.py 387): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 4
    EMBED_DIM: 384
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 14
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 15:34:02 swin_tiny_patch4_window7_224] (main.py 388): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 15:34:02 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 15:34:03 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 33097328
[2024-12-02 15:34:03 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 2.213553408
[2024-12-02 15:34:03 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 15:34:03 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 15:34:04 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][0/34]	eta 0:00:55 lr 0.000004	 wd 0.0001	time 1.6445 (1.6445)	loss 1.0855 (1.0855)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:34:20 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][10/34]	eta 0:00:37 lr 0.000004	 wd 0.0001	time 1.3305 (1.5616)	loss 1.1843 (1.1228)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:34:35 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.3098 (1.5249)	loss 0.9865 (1.0646)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:34:50 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [0/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3841 (1.5119)	loss 0.9035 (1.0670)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:34:55 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 0 training takes 0:00:52
[2024-12-02 15:35:01 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 15:35:01 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 15:35:01 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 15:35:03 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][0/34]	eta 0:01:07 lr 0.000004	 wd 0.0001	time 1.9776 (1.9776)	loss 0.9955 (0.9955)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:35:20 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][10/34]	eta 0:00:39 lr 0.000004	 wd 0.0001	time 2.1569 (1.6475)	loss 1.0919 (1.0669)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:35:35 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][20/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 1.8689 (1.5946)	loss 1.1782 (1.0572)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:35:50 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [1/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3684 (1.5528)	loss 1.0761 (1.0571)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:35:54 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 1 training takes 0:00:52
[2024-12-02 15:36:02 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 15:36:02 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 15:36:02 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 15:36:04 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][0/34]	eta 0:01:05 lr 0.000004	 wd 0.0001	time 1.9219 (1.9219)	loss 0.6413 (0.6413)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:36:19 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][10/34]	eta 0:00:36 lr 0.000004	 wd 0.0001	time 1.3131 (1.5341)	loss 1.1778 (1.0326)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:36:33 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.3538 (1.5102)	loss 1.2703 (1.0401)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:36:50 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [2/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3953 (1.5448)	loss 0.9743 (1.0522)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:36:55 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 2 training takes 0:00:53
[2024-12-02 15:37:02 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 15:37:02 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 15:37:02 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 15:37:04 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][0/34]	eta 0:01:03 lr 0.000004	 wd 0.0001	time 1.8777 (1.8777)	loss 1.0090 (1.0090)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:37:19 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][10/34]	eta 0:00:37 lr 0.000004	 wd 0.0001	time 1.4257 (1.5680)	loss 0.8220 (0.9971)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:37:35 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 2.2641 (1.5679)	loss 1.3722 (1.0442)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:37:50 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [3/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.7470 (1.5581)	loss 1.1834 (1.0703)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:37:54 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 3 training takes 0:00:52
[2024-12-02 15:38:02 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 15:38:02 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 15:38:02 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 15:38:04 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][0/34]	eta 0:01:13 lr 0.000004	 wd 0.0001	time 2.1604 (2.1604)	loss 0.9931 (0.9931)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:38:19 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][10/34]	eta 0:00:37 lr 0.000004	 wd 0.0001	time 1.3642 (1.5738)	loss 0.8166 (1.0769)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:38:34 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.3390 (1.5297)	loss 1.0840 (1.0796)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:38:49 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [4/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3587 (1.5167)	loss 0.7354 (1.0622)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:38:53 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 4 training takes 0:00:51
[2024-12-02 15:39:01 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 15:39:01 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 15:39:01 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 15:39:03 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [5/50][0/34]	eta 0:01:05 lr 0.000004	 wd 0.0001	time 1.9322 (1.9322)	loss 1.1726 (1.1726)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:39:19 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [5/50][10/34]	eta 0:00:37 lr 0.000004	 wd 0.0001	time 1.4031 (1.5635)	loss 1.0808 (1.0423)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:39:33 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [5/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.3958 (1.5142)	loss 1.0906 (1.0516)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:39:49 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [5/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.8325 (1.5216)	loss 1.3398 (1.0448)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:39:54 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 5 training takes 0:00:52
[2024-12-02 15:40:01 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 15:40:01 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 15:40:01 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 15:40:04 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [6/50][0/34]	eta 0:01:38 lr 0.000004	 wd 0.0001	time 2.9002 (2.9002)	loss 1.0040 (1.0040)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:40:19 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [6/50][10/34]	eta 0:00:40 lr 0.000004	 wd 0.0001	time 1.4587 (1.6968)	loss 1.1713 (1.1394)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:40:34 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [6/50][20/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 1.3399 (1.5987)	loss 1.0901 (1.0770)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:40:49 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [6/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.4083 (1.5625)	loss 1.0077 (1.0444)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:40:53 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 6 training takes 0:00:52
[2024-12-02 15:41:01 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 15:41:01 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 15:41:01 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 15:41:03 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [7/50][0/34]	eta 0:01:07 lr 0.000004	 wd 0.0001	time 1.9843 (1.9843)	loss 1.1839 (1.1839)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:41:23 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [7/50][10/34]	eta 0:00:47 lr 0.000004	 wd 0.0001	time 1.4634 (1.9588)	loss 1.0938 (1.0275)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:41:40 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [7/50][20/34]	eta 0:00:26 lr 0.000004	 wd 0.0001	time 2.0614 (1.8596)	loss 0.9914 (1.0582)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:41:58 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [7/50][30/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 2.3389 (1.8352)	loss 1.1679 (1.0450)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:42:03 swin_tiny_patch4_window7_224] (main.py 261): INFO EPOCH 7 training takes 0:01:01
[2024-12-02 15:42:11 swin_tiny_patch4_window7_224] (main.py 310): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 15:42:11 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 15:42:11 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 15:42:13 swin_tiny_patch4_window7_224] (main.py 252): INFO Train: [8/50][0/34]	eta 0:01:09 lr 0.000004	 wd 0.0001	time 2.0577 (2.0577)	loss 1.2868 (1.2868)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	mem 0MB
[2024-12-02 15:42:52 swin_tiny_patch4_window7_224] (main.py 388): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 4
    EMBED_DIM: 384
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 14
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 15:42:52 swin_tiny_patch4_window7_224] (main.py 389): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 15:42:52 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 15:42:53 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 33097328
[2024-12-02 15:42:53 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 2.213553408
[2024-12-02 15:42:53 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 15:42:53 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 15:42:55 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][0/34]	eta 0:01:10 lr 0.000004	 wd 0.0001	time 2.0861 (2.0861)	loss 1.0855 (1.0855)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.750
[2024-12-02 15:43:11 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][10/34]	eta 0:00:40 lr 0.000004	 wd 0.0001	time 2.1125 (1.6853)	loss 1.1843 (1.1228)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.812
[2024-12-02 15:43:26 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][20/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 1.4819 (1.5951)	loss 0.9865 (1.0646)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.641
[2024-12-02 15:43:41 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3398 (1.5614)	loss 0.9035 (1.0670)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-02 15:43:45 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 0 training takes 0:00:52
[2024-12-02 15:43:54 swin_tiny_patch4_window7_224] (main.py 311): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 15:43:54 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 15:43:54 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 15:43:55 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][0/34]	eta 0:01:02 lr 0.000004	 wd 0.0001	time 1.8337 (1.8337)	loss 0.9955 (0.9955)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.664
[2024-12-02 15:44:10 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][10/34]	eta 0:00:36 lr 0.000004	 wd 0.0001	time 1.3041 (1.5251)	loss 1.0919 (1.0669)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.750
[2024-12-02 15:44:25 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][20/34]	eta 0:00:20 lr 0.000004	 wd 0.0001	time 1.3205 (1.4965)	loss 1.1782 (1.0572)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.812
[2024-12-02 15:44:40 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][30/34]	eta 0:00:05 lr 0.000004	 wd 0.0001	time 1.3286 (1.4908)	loss 1.0761 (1.0571)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-02 15:44:45 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 1 training takes 0:00:51
[2024-12-02 15:44:52 swin_tiny_patch4_window7_224] (main.py 311): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 15:44:52 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 15:44:52 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 15:44:54 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [2/50][0/34]	eta 0:01:02 lr 0.000004	 wd 0.0001	time 1.8506 (1.8506)	loss 0.6413 (0.6413)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.438
[2024-12-02 15:45:09 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [2/50][10/34]	eta 0:00:37 lr 0.000004	 wd 0.0001	time 1.3872 (1.5536)	loss 1.1778 (1.0326)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.812
[2024-12-02 15:45:24 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [2/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.7682 (1.5325)	loss 1.2703 (1.0401)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.828
[2024-12-02 15:45:40 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [2/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 2.1171 (1.5331)	loss 0.9743 (1.0522)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-02 15:45:44 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 2 training takes 0:00:51
[2024-12-02 15:45:51 swin_tiny_patch4_window7_224] (main.py 311): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 15:45:51 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 15:45:51 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 15:45:54 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [3/50][0/34]	eta 0:01:29 lr 0.000004	 wd 0.0001	time 2.6452 (2.6452)	loss 1.0090 (1.0090)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.664
[2024-12-02 15:46:09 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [3/50][10/34]	eta 0:00:39 lr 0.000004	 wd 0.0001	time 1.3513 (1.6289)	loss 0.8220 (0.9971)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.555
[2024-12-02 15:46:24 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [3/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.3287 (1.5645)	loss 1.3722 (1.0442)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.883
[2024-12-02 15:46:39 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [3/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.2976 (1.5361)	loss 1.1834 (1.0703)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.812
[2024-12-02 15:46:43 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 3 training takes 0:00:51
[2024-12-02 15:46:51 swin_tiny_patch4_window7_224] (main.py 311): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 15:46:51 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 15:46:51 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 15:46:53 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [4/50][0/34]	eta 0:01:07 lr 0.000004	 wd 0.0001	time 1.9956 (1.9956)	loss 0.9931 (0.9931)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.664
[2024-12-02 15:47:09 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [4/50][10/34]	eta 0:00:38 lr 0.000004	 wd 0.0001	time 1.3283 (1.6011)	loss 0.8166 (1.0769)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.555
[2024-12-02 15:47:23 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [4/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.3753 (1.5333)	loss 1.0840 (1.0796)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.719
[2024-12-02 15:47:38 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [4/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.4287 (1.5135)	loss 0.7354 (1.0622)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 15:47:43 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 4 training takes 0:00:52
[2024-12-02 15:47:50 swin_tiny_patch4_window7_224] (main.py 311): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 15:47:50 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 15:47:50 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 15:47:52 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [5/50][0/34]	eta 0:01:15 lr 0.000004	 wd 0.0001	time 2.2293 (2.2293)	loss 1.1726 (1.1726)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.773
[2024-12-02 15:48:09 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [5/50][10/34]	eta 0:00:40 lr 0.000004	 wd 0.0001	time 1.9213 (1.6749)	loss 1.0808 (1.0423)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.750
[2024-12-02 15:48:23 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [5/50][20/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 1.3217 (1.5796)	loss 1.0906 (1.0516)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.719
[2024-12-02 15:48:39 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [5/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3538 (1.5905)	loss 1.3398 (1.0448)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.883
[2024-12-02 15:48:44 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 5 training takes 0:00:53
[2024-12-02 15:48:52 swin_tiny_patch4_window7_224] (main.py 311): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 15:48:52 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 15:48:52 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 15:48:54 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [6/50][0/34]	eta 0:01:05 lr 0.000004	 wd 0.0001	time 1.9119 (1.9119)	loss 1.0040 (1.0040)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-02 15:49:09 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [6/50][10/34]	eta 0:00:37 lr 0.000004	 wd 0.0001	time 1.2958 (1.5827)	loss 1.1713 (1.1394)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.734
[2024-12-02 15:49:25 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [6/50][20/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 1.3045 (1.5769)	loss 1.0901 (1.0770)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.750
[2024-12-02 15:49:43 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [6/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 2.6924 (1.6531)	loss 1.0077 (1.0444)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.664
[2024-12-02 15:49:48 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 6 training takes 0:00:56
[2024-12-02 15:49:55 swin_tiny_patch4_window7_224] (main.py 311): INFO  * Acc@1 73.077 Acc@5 73.077
[2024-12-02 15:49:55 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 73.1%
[2024-12-02 15:49:55 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 73.08%
[2024-12-02 15:49:58 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [7/50][0/34]	eta 0:01:28 lr 0.000004	 wd 0.0001	time 2.6046 (2.6046)	loss 1.1839 (1.1839)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.812
[2024-12-02 15:50:14 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [7/50][10/34]	eta 0:00:41 lr 0.000004	 wd 0.0001	time 1.6885 (1.7232)	loss 1.0938 (1.0275)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.750
[2024-12-02 15:51:19 swin_tiny_patch4_window7_224] (main.py 390): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 4
    EMBED_DIM: 384
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 14
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 15:51:19 swin_tiny_patch4_window7_224] (main.py 391): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 15:51:19 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 15:51:20 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 33097328
[2024-12-02 15:51:20 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 2.213553408
[2024-12-02 15:51:20 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 15:51:20 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 15:51:22 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][0/34]	eta 0:01:00 lr 0.000004	 wd 0.0001	time 1.7794 (1.7794)	loss 1.0855 (1.0855)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.750
[2024-12-02 15:51:40 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][10/34]	eta 0:00:42 lr 0.000004	 wd 0.0001	time 3.7131 (1.7799)	loss 1.1843 (1.1228)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.812
[2024-12-02 15:51:55 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][20/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 1.4337 (1.6575)	loss 0.9865 (1.0646)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.641
[2024-12-02 15:52:10 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3590 (1.6102)	loss 0.9035 (1.0670)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-02 15:52:14 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 0 training takes 0:00:54
[2024-12-02 15:52:23 swin_tiny_patch4_window7_224] (main.py 313): INFO  * Accuracy validation@ 0.000 
[2024-12-02 15:52:23 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 15:52:23 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 15:52:25 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][0/34]	eta 0:01:06 lr 0.000004	 wd 0.0001	time 1.9647 (1.9647)	loss 0.9955 (0.9955)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.664
[2024-12-02 15:52:40 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][10/34]	eta 0:00:37 lr 0.000004	 wd 0.0001	time 1.2951 (1.5740)	loss 1.0919 (1.0669)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.750
[2024-12-02 15:52:54 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.3570 (1.5209)	loss 1.1782 (1.0572)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.812
[2024-12-02 15:53:09 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3705 (1.5030)	loss 1.0761 (1.0571)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-02 15:53:15 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 1 training takes 0:00:52
[2024-12-02 15:53:24 swin_tiny_patch4_window7_224] (main.py 313): INFO  * Accuracy validation@ 0.000 
[2024-12-02 15:53:24 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 15:53:24 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 15:53:29 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [2/50][0/34]	eta 0:02:55 lr 0.000004	 wd 0.0001	time 5.1710 (5.1710)	loss 0.6413 (0.6413)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.438
[2024-12-02 15:53:46 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [2/50][10/34]	eta 0:00:48 lr 0.000004	 wd 0.0001	time 1.3913 (2.0391)	loss 1.1778 (1.0326)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.812
[2024-12-02 15:54:05 swin_tiny_patch4_window7_224] (main.py 392): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 4
    EMBED_DIM: 384
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 14
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 15:54:05 swin_tiny_patch4_window7_224] (main.py 393): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 15:54:05 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 15:54:06 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 33097328
[2024-12-02 15:54:06 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 2.213553408
[2024-12-02 15:54:06 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 15:54:06 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 15:54:08 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][0/34]	eta 0:01:31 lr 0.000004	 wd 0.0001	time 2.6870 (2.6870)	loss 1.0855 (1.0855)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.750
[2024-12-02 15:54:24 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][10/34]	eta 0:00:39 lr 0.000004	 wd 0.0001	time 1.5250 (1.6404)	loss 1.1843 (1.1228)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.812
[2024-12-02 15:54:40 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][20/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 1.3472 (1.6258)	loss 0.9865 (1.0646)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.641
[2024-12-02 15:54:54 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3555 (1.5763)	loss 0.9035 (1.0670)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-02 15:54:59 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 0 training takes 0:00:52
[2024-12-02 15:55:08 swin_tiny_patch4_window7_224] (main.py 315): INFO  * Accuracy validation@ 0.000 
[2024-12-02 15:55:08 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 15:55:08 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 15:55:10 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][0/34]	eta 0:01:14 lr 0.000004	 wd 0.0001	time 2.2053 (2.2053)	loss 0.9955 (0.9955)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.664
[2024-12-02 15:55:25 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][10/34]	eta 0:00:37 lr 0.000004	 wd 0.0001	time 1.3423 (1.5540)	loss 1.0919 (1.0669)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.750
[2024-12-02 15:55:40 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.3610 (1.5198)	loss 1.1782 (1.0572)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.812
[2024-12-02 15:55:55 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.7933 (1.5242)	loss 1.0761 (1.0571)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-02 15:56:00 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 1 training takes 0:00:52
[2024-12-02 15:56:07 swin_tiny_patch4_window7_224] (main.py 315): INFO  * Accuracy validation@ 0.000 
[2024-12-02 15:56:07 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 15:56:07 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 15:56:09 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [2/50][0/34]	eta 0:01:28 lr 0.000004	 wd 0.0001	time 2.5891 (2.5891)	loss 0.6413 (0.6413)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.438
[2024-12-02 15:56:25 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [2/50][10/34]	eta 0:00:40 lr 0.000004	 wd 0.0001	time 1.7427 (1.6729)	loss 1.1778 (1.0326)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.812
[2024-12-02 15:56:40 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [2/50][20/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 1.3636 (1.5854)	loss 1.2703 (1.0401)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.828
[2024-12-02 15:56:55 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [2/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3054 (1.5521)	loss 0.9743 (1.0522)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-02 15:56:59 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 2 training takes 0:00:52
[2024-12-02 15:57:07 swin_tiny_patch4_window7_224] (main.py 315): INFO  * Accuracy validation@ 0.000 
[2024-12-02 15:57:07 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 15:57:07 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 15:57:09 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [3/50][0/34]	eta 0:01:06 lr 0.000004	 wd 0.0001	time 1.9666 (1.9666)	loss 1.0090 (1.0090)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.664
[2024-12-02 16:03:14 swin_tiny_patch4_window7_224] (main.py 392): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 4
    EMBED_DIM: 384
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 14
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 16:03:14 swin_tiny_patch4_window7_224] (main.py 393): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 16:03:14 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 16:03:15 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 33097328
[2024-12-02 16:03:15 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 2.213553408
[2024-12-02 16:03:15 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 16:03:15 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 16:03:16 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][0/34]	eta 0:00:57 lr 0.000004	 wd 0.0001	time 1.6900 (1.6900)	loss 1.0855 (1.0855)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.750
[2024-12-02 16:03:31 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][10/34]	eta 0:00:36 lr 0.000004	 wd 0.0001	time 1.3558 (1.5231)	loss 1.1843 (1.1228)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.812
[2024-12-02 16:03:46 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.3913 (1.5009)	loss 0.9865 (1.0646)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.641
[2024-12-02 16:04:01 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][30/34]	eta 0:00:05 lr 0.000004	 wd 0.0001	time 1.3894 (1.4975)	loss 0.9035 (1.0670)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-02 16:04:05 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 0 training takes 0:00:50
[2024-12-02 16:04:13 swin_tiny_patch4_window7_224] (main.py 315): INFO  * Accuracy validation@ 0.731 
[2024-12-02 16:04:13 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 16:04:13 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 16:04:15 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][0/34]	eta 0:01:08 lr 0.000004	 wd 0.0001	time 2.0005 (2.0005)	loss 0.9955 (0.9955)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.664
[2024-12-02 16:04:30 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][10/34]	eta 0:00:37 lr 0.000004	 wd 0.0001	time 1.2977 (1.5507)	loss 1.0919 (1.0669)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.750
[2024-12-02 16:04:45 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.3127 (1.5160)	loss 1.1782 (1.0572)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.812
[2024-12-02 16:05:00 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.6828 (1.5109)	loss 1.0761 (1.0571)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-02 16:05:05 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 1 training takes 0:00:51
[2024-12-02 16:05:12 swin_tiny_patch4_window7_224] (main.py 315): INFO  * Accuracy validation@ 0.731 
[2024-12-02 16:05:12 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 16:05:12 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 16:05:15 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [2/50][0/34]	eta 0:01:29 lr 0.000004	 wd 0.0001	time 2.6377 (2.6377)	loss 0.6413 (0.6413)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.438
[2024-12-02 16:05:30 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [2/50][10/34]	eta 0:00:39 lr 0.000004	 wd 0.0001	time 1.6838 (1.6445)	loss 1.1778 (1.0326)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.812
[2024-12-02 16:05:45 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [2/50][20/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 1.4975 (1.5942)	loss 1.2703 (1.0401)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.828
[2024-12-02 16:06:01 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [2/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3720 (1.5701)	loss 0.9743 (1.0522)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-02 16:06:05 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 2 training takes 0:00:52
[2024-12-02 16:06:13 swin_tiny_patch4_window7_224] (main.py 315): INFO  * Accuracy validation@ 0.731 
[2024-12-02 16:06:13 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 16:06:13 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 16:06:15 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [3/50][0/34]	eta 0:01:10 lr 0.000004	 wd 0.0001	time 2.0736 (2.0736)	loss 1.0090 (1.0090)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.664
[2024-12-02 16:06:31 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [3/50][10/34]	eta 0:00:39 lr 0.000004	 wd 0.0001	time 1.3745 (1.6482)	loss 0.8220 (0.9971)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.555
[2024-12-02 16:06:46 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [3/50][20/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 1.3947 (1.5817)	loss 1.3722 (1.0442)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.883
[2024-12-02 16:07:02 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [3/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.9859 (1.5780)	loss 1.1834 (1.0703)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.812
[2024-12-02 16:07:07 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 3 training takes 0:00:53
[2024-12-02 16:07:14 swin_tiny_patch4_window7_224] (main.py 315): INFO  * Accuracy validation@ 0.731 
[2024-12-02 16:07:14 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 16:07:14 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 16:07:16 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [4/50][0/34]	eta 0:01:30 lr 0.000004	 wd 0.0001	time 2.6694 (2.6694)	loss 0.9931 (0.9931)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.664
[2024-12-02 16:07:32 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [4/50][10/34]	eta 0:00:39 lr 0.000004	 wd 0.0001	time 1.6347 (1.6408)	loss 0.8166 (1.0769)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.555
[2024-12-02 16:07:46 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [4/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.3631 (1.5655)	loss 1.0840 (1.0796)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.719
[2024-12-02 16:08:01 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [4/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3755 (1.5384)	loss 0.7354 (1.0622)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 16:08:05 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 4 training takes 0:00:51
[2024-12-02 16:08:14 swin_tiny_patch4_window7_224] (main.py 315): INFO  * Accuracy validation@ 0.731 
[2024-12-02 16:08:14 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 16:08:14 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 16:08:16 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [5/50][0/34]	eta 0:01:01 lr 0.000004	 wd 0.0001	time 1.8058 (1.8058)	loss 1.1726 (1.1726)	grad_norm 0.0000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.773
[2024-12-02 16:08:33 swin_tiny_patch4_window7_224] (main.py 392): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 4
    EMBED_DIM: 384
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 14
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 16:08:33 swin_tiny_patch4_window7_224] (main.py 393): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 16:08:33 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 16:08:33 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 33097328
[2024-12-02 16:08:33 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 2.213553408
[2024-12-02 16:08:33 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 16:08:33 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 16:08:35 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][0/34]	eta 0:01:06 lr 0.000004	 wd 0.0001	time 1.9662 (1.9662)	loss 1.0855 (1.0855)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.750
[2024-12-02 16:08:52 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][10/34]	eta 0:00:39 lr 0.000004	 wd 0.0001	time 2.1667 (1.6524)	loss 1.1843 (1.1228)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.812
[2024-12-02 16:09:06 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][20/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 1.5468 (1.5734)	loss 0.9865 (1.0646)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.641
[2024-12-02 16:09:21 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [0/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3847 (1.5491)	loss 0.9035 (1.0670)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-02 16:09:25 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 0 training takes 0:00:52
[2024-12-02 16:09:34 swin_tiny_patch4_window7_224] (main.py 315): INFO  * Accuracy validation@ 0.731 
[2024-12-02 16:09:34 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 16:09:34 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 16:09:36 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][0/34]	eta 0:01:06 lr 0.000004	 wd 0.0001	time 1.9637 (1.9637)	loss 0.9955 (0.9955)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.664
[2024-12-02 16:09:51 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][10/34]	eta 0:00:38 lr 0.000004	 wd 0.0001	time 1.3504 (1.6010)	loss 1.0919 (1.0669)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.750
[2024-12-02 16:10:07 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.4348 (1.5711)	loss 1.1782 (1.0572)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.812
[2024-12-02 16:10:21 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [1/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3046 (1.5411)	loss 1.0761 (1.0571)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-02 16:10:27 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 1 training takes 0:00:53
[2024-12-02 16:10:34 swin_tiny_patch4_window7_224] (main.py 315): INFO  * Accuracy validation@ 0.731 
[2024-12-02 16:10:34 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 16:10:34 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 16:10:36 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [2/50][0/34]	eta 0:01:09 lr 0.000004	 wd 0.0001	time 2.0329 (2.0329)	loss 0.6413 (0.6413)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.438
[2024-12-02 16:10:52 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [2/50][10/34]	eta 0:00:40 lr 0.000004	 wd 0.0001	time 2.1580 (1.6698)	loss 1.1778 (1.0326)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.812
[2024-12-02 16:11:08 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [2/50][20/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 1.8349 (1.6225)	loss 1.2703 (1.0401)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.828
[2024-12-02 16:11:23 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [2/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3138 (1.5729)	loss 0.9743 (1.0522)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-02 16:11:27 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 2 training takes 0:00:52
[2024-12-02 16:11:35 swin_tiny_patch4_window7_224] (main.py 315): INFO  * Accuracy validation@ 0.731 
[2024-12-02 16:11:35 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 16:11:35 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 16:11:37 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [3/50][0/34]	eta 0:01:02 lr 0.000004	 wd 0.0001	time 1.8242 (1.8242)	loss 1.0090 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.664
[2024-12-02 16:11:52 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [3/50][10/34]	eta 0:00:37 lr 0.000004	 wd 0.0001	time 1.2909 (1.5481)	loss 0.8220 (0.9971)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.555
[2024-12-02 16:12:07 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [3/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.3006 (1.5061)	loss 1.3722 (1.0442)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.883
[2024-12-02 16:12:23 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [3/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.3964 (1.5408)	loss 1.1834 (1.0703)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.812
[2024-12-02 16:12:28 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 3 training takes 0:00:53
[2024-12-02 16:12:35 swin_tiny_patch4_window7_224] (main.py 315): INFO  * Accuracy validation@ 0.731 
[2024-12-02 16:12:35 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 16:12:35 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 16:12:37 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [4/50][0/34]	eta 0:01:07 lr 0.000004	 wd 0.0001	time 1.9877 (1.9877)	loss 0.9931 (0.9931)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.664
[2024-12-02 16:12:53 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [4/50][10/34]	eta 0:00:39 lr 0.000004	 wd 0.0001	time 1.9388 (1.6403)	loss 0.8166 (1.0769)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.555
[2024-12-02 16:13:08 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [4/50][20/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 2.0414 (1.5816)	loss 1.0840 (1.0796)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.719
[2024-12-02 16:13:23 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [4/50][30/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 1.4158 (1.5497)	loss 0.7354 (1.0622)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 16:13:27 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 4 training takes 0:00:52
[2024-12-02 16:13:35 swin_tiny_patch4_window7_224] (main.py 315): INFO  * Accuracy validation@ 0.731 
[2024-12-02 16:13:35 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 16:13:35 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 16:13:37 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [5/50][0/34]	eta 0:01:01 lr 0.000004	 wd 0.0001	time 1.8180 (1.8180)	loss 1.1726 (1.1726)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.773
[2024-12-02 16:13:52 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [5/50][10/34]	eta 0:00:37 lr 0.000004	 wd 0.0001	time 1.3659 (1.5457)	loss 1.0808 (1.0423)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.750
[2024-12-02 16:14:07 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [5/50][20/34]	eta 0:00:21 lr 0.000004	 wd 0.0001	time 1.3010 (1.5087)	loss 1.0906 (1.0516)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.719
[2024-12-02 16:14:22 swin_tiny_patch4_window7_224] (main.py 253): INFO Train: [5/50][30/34]	eta 0:00:05 lr 0.000004	 wd 0.0001	time 1.3596 (1.4953)	loss 1.3398 (1.0448)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.883
[2024-12-02 16:14:26 swin_tiny_patch4_window7_224] (main.py 262): INFO EPOCH 5 training takes 0:00:50
[2024-12-02 16:16:19 swin_tiny_patch4_window7_224] (main.py 398): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 4
    EMBED_DIM: 384
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 14
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 16:16:19 swin_tiny_patch4_window7_224] (main.py 399): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 16:16:19 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 16:16:20 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 33097328
[2024-12-02 16:16:20 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 2.213553408
[2024-12-02 16:16:20 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 16:16:20 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 16:16:25 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][0/34]	eta 0:02:52 lr 0.000004	 wd 0.0001	time 5.0671 (5.0671)	loss 1.0855 (1.0855)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.750
[2024-12-02 16:17:12 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][10/34]	eta 0:01:54 lr 0.000004	 wd 0.0001	time 5.5139 (4.7789)	loss 1.1368 (1.0901)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.227
[2024-12-02 16:17:59 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][20/34]	eta 0:01:05 lr 0.000004	 wd 0.0001	time 4.2828 (4.7026)	loss 0.9583 (1.0269)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-02 16:18:47 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][30/34]	eta 0:00:18 lr 0.000004	 wd 0.0001	time 4.9615 (4.7457)	loss 0.8699 (1.0292)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.422
[2024-12-02 16:19:01 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 0 training takes 0:02:40
[2024-12-02 16:19:08 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.308 
[2024-12-02 16:19:08 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-02 16:19:08 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.31%
[2024-12-02 16:19:14 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][0/34]	eta 0:03:24 lr 0.000004	 wd 0.0001	time 6.0236 (6.0236)	loss 0.9538 (0.9538)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.336
[2024-12-02 16:20:01 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][10/34]	eta 0:01:55 lr 0.000004	 wd 0.0001	time 4.3849 (4.8284)	loss 1.0421 (1.0254)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 16:20:48 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][20/34]	eta 0:01:06 lr 0.000004	 wd 0.0001	time 4.9678 (4.7664)	loss 1.1268 (1.0161)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 16:21:35 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][30/34]	eta 0:00:19 lr 0.000004	 wd 0.0001	time 4.3400 (4.7514)	loss 1.0401 (1.0156)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 16:21:49 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 1 training takes 0:02:41
[2024-12-02 16:21:58 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.423 
[2024-12-02 16:21:58 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 16:21:58 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.42%
[2024-12-02 16:22:03 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][0/34]	eta 0:03:03 lr 0.000004	 wd 0.0001	time 5.4115 (5.4115)	loss 0.6066 (0.6066)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 16:22:50 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][10/34]	eta 0:01:54 lr 0.000004	 wd 0.0001	time 5.0534 (4.7697)	loss 1.1267 (0.9849)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.383
[2024-12-02 16:23:36 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][20/34]	eta 0:01:05 lr 0.000004	 wd 0.0001	time 4.4085 (4.6816)	loss 1.2148 (0.9947)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.734
[2024-12-02 16:24:24 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][30/34]	eta 0:00:18 lr 0.000004	 wd 0.0001	time 4.2634 (4.7148)	loss 0.9533 (1.0093)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.570
[2024-12-02 16:24:38 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 2 training takes 0:02:40
[2024-12-02 16:24:46 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.308 
[2024-12-02 16:24:46 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-02 16:24:46 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.42%
[2024-12-02 16:24:51 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][0/34]	eta 0:03:05 lr 0.000004	 wd 0.0001	time 5.4632 (5.4632)	loss 0.9532 (0.9532)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.430
[2024-12-02 16:25:40 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][10/34]	eta 0:01:58 lr 0.000004	 wd 0.0001	time 4.1181 (4.9465)	loss 0.7800 (0.9534)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-02 16:26:27 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][20/34]	eta 0:01:07 lr 0.000004	 wd 0.0001	time 4.6404 (4.8004)	loss 1.2998 (0.9987)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.555
[2024-12-02 16:27:14 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][30/34]	eta 0:00:19 lr 0.000004	 wd 0.0001	time 4.1480 (4.7746)	loss 1.1266 (1.0232)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.227
[2024-12-02 16:27:28 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 3 training takes 0:02:41
[2024-12-02 16:27:36 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.628 
[2024-12-02 16:27:36 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 16:27:36 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.63%
[2024-12-02 16:27:41 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][0/34]	eta 0:03:12 lr 0.000004	 wd 0.0001	time 5.6685 (5.6685)	loss 0.9533 (0.9533)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-02 16:28:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][10/34]	eta 0:01:54 lr 0.000004	 wd 0.0001	time 4.4010 (4.7518)	loss 0.7799 (1.0320)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.516
[2024-12-02 16:29:17 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][20/34]	eta 0:01:07 lr 0.000004	 wd 0.0001	time 4.1627 (4.8243)	loss 1.0400 (1.0316)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 16:30:07 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][30/34]	eta 0:00:19 lr 0.000004	 wd 0.0001	time 5.6098 (4.8784)	loss 0.6933 (1.0148)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 16:30:21 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 4 training takes 0:02:45
[2024-12-02 16:30:28 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.718 
[2024-12-02 16:30:28 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 16:30:28 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 16:30:35 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][0/34]	eta 0:03:38 lr 0.000004	 wd 0.0001	time 6.4288 (6.4288)	loss 1.1265 (1.1265)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.695
[2024-12-02 16:31:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][10/34]	eta 0:01:56 lr 0.000004	 wd 0.0001	time 4.3196 (4.8455)	loss 1.0398 (1.0004)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.625
[2024-12-02 16:32:09 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][20/34]	eta 0:01:07 lr 0.000004	 wd 0.0001	time 5.4445 (4.8139)	loss 1.0398 (1.0069)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-02 16:33:41 swin_tiny_patch4_window7_224] (main.py 398): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 4
    EMBED_DIM: 384
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 16:33:41 swin_tiny_patch4_window7_224] (main.py 399): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 16:33:41 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 16:33:42 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 33093968
[2024-12-02 16:33:42 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 2.169298176
[2024-12-02 16:33:42 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 16:33:42 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 16:33:48 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][0/34]	eta 0:03:40 lr 0.000004	 wd 0.0001	time 6.4921 (6.4921)	loss 1.0172 (1.0172)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-02 16:34:40 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][10/34]	eta 0:02:06 lr 0.000004	 wd 0.0001	time 4.6475 (5.2781)	loss 0.6941 (0.9714)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 16:35:30 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][20/34]	eta 0:01:11 lr 0.000004	 wd 0.0001	time 4.4512 (5.1338)	loss 1.2187 (1.0363)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.828
[2024-12-02 16:36:21 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][30/34]	eta 0:00:20 lr 0.000004	 wd 0.0001	time 5.5603 (5.1157)	loss 0.9556 (1.0275)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.641
[2024-12-02 16:36:35 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 0 training takes 0:02:52
[2024-12-02 16:36:43 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.436 
[2024-12-02 16:36:43 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 16:36:43 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.44%
[2024-12-02 16:36:49 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][0/34]	eta 0:03:28 lr 0.000004	 wd 0.0001	time 6.1270 (6.1270)	loss 1.0404 (1.0404)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.375
[2024-12-02 16:37:39 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][10/34]	eta 0:02:02 lr 0.000004	 wd 0.0001	time 6.0951 (5.0856)	loss 0.8679 (1.0264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 16:38:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][20/34]	eta 0:01:09 lr 0.000004	 wd 0.0001	time 4.4318 (4.9841)	loss 0.6934 (1.0337)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 16:39:16 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][30/34]	eta 0:00:19 lr 0.000004	 wd 0.0001	time 4.2357 (4.9490)	loss 0.8670 (1.0220)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-02 16:39:31 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 1 training takes 0:02:47
[2024-12-02 16:39:40 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.321 
[2024-12-02 16:39:40 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-02 16:39:40 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.44%
[2024-12-02 16:39:45 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][0/34]	eta 0:03:07 lr 0.000004	 wd 0.0001	time 5.5137 (5.5137)	loss 1.0417 (1.0417)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-02 16:40:35 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][10/34]	eta 0:02:01 lr 0.000004	 wd 0.0001	time 4.5113 (5.0560)	loss 0.7800 (0.9460)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 16:41:25 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][20/34]	eta 0:01:10 lr 0.000004	 wd 0.0001	time 5.0401 (5.0172)	loss 1.0405 (1.0074)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 16:42:15 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][30/34]	eta 0:00:20 lr 0.000004	 wd 0.0001	time 5.5577 (5.0248)	loss 0.9532 (1.0152)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 16:42:30 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 2 training takes 0:02:50
[2024-12-02 16:42:38 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.372 
[2024-12-02 16:42:38 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 16:42:38 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.44%
[2024-12-02 16:42:44 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][0/34]	eta 0:03:36 lr 0.000004	 wd 0.0001	time 6.3609 (6.3609)	loss 1.1271 (1.1271)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.422
[2024-12-02 16:43:34 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][10/34]	eta 0:02:01 lr 0.000004	 wd 0.0001	time 5.6836 (5.0632)	loss 0.9535 (1.0167)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.383
[2024-12-02 16:44:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][20/34]	eta 0:01:08 lr 0.000004	 wd 0.0001	time 4.4797 (4.9245)	loss 1.1271 (1.0321)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-02 16:45:11 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][30/34]	eta 0:00:19 lr 0.000004	 wd 0.0001	time 4.6294 (4.9227)	loss 1.1269 (1.0067)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.422
[2024-12-02 16:45:25 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 3 training takes 0:02:47
[2024-12-02 16:45:34 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.731 
[2024-12-02 16:45:34 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 16:45:34 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 16:45:41 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][0/34]	eta 0:03:53 lr 0.000004	 wd 0.0001	time 6.8638 (6.8638)	loss 0.9533 (0.9533)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 16:46:31 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][10/34]	eta 0:02:04 lr 0.000004	 wd 0.0001	time 4.4268 (5.1930)	loss 0.9531 (1.0007)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 16:47:20 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][20/34]	eta 0:01:10 lr 0.000004	 wd 0.0001	time 4.9037 (5.0579)	loss 1.0400 (1.0194)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 16:48:11 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][30/34]	eta 0:00:20 lr 0.000004	 wd 0.0001	time 5.5240 (5.0527)	loss 0.8669 (1.0177)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.422
[2024-12-02 16:48:26 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 4 training takes 0:02:51
[2024-12-02 16:48:34 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.731 
[2024-12-02 16:48:34 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 16:48:34 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 16:48:40 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][0/34]	eta 0:03:37 lr 0.000004	 wd 0.0001	time 6.3840 (6.3840)	loss 0.8673 (0.8673)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.516
[2024-12-02 16:49:29 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][10/34]	eta 0:02:01 lr 0.000004	 wd 0.0001	time 5.6328 (5.0639)	loss 1.0400 (1.0321)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.312
[2024-12-02 16:50:19 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][20/34]	eta 0:01:09 lr 0.000004	 wd 0.0001	time 4.6827 (4.9918)	loss 1.0398 (0.9864)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-02 16:51:08 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][30/34]	eta 0:00:19 lr 0.000004	 wd 0.0001	time 4.5924 (4.9784)	loss 1.0399 (1.0065)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 16:51:23 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 5 training takes 0:02:48
[2024-12-02 16:51:31 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.410 
[2024-12-02 16:51:31 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 16:51:31 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 16:51:37 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [6/50][0/34]	eta 0:03:19 lr 0.000004	 wd 0.0001	time 5.8790 (5.8790)	loss 1.2132 (1.2132)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.219
[2024-12-02 17:05:06 swin_tiny_patch4_window7_224] (main.py 398): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 17:05:06 swin_tiny_patch4_window7_224] (main.py 399): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 17:05:06 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 17:05:07 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 27498236
[2024-12-02 17:05:07 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.210060288
[2024-12-02 17:05:07 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 17:05:07 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 17:05:46 swin_tiny_patch4_window7_224] (main.py 398): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 17:05:46 swin_tiny_patch4_window7_224] (main.py 399): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 17:05:46 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 17:05:47 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 27498236
[2024-12-02 17:05:47 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.210060288
[2024-12-02 17:05:47 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 17:05:47 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 17:10:13 swin_tiny_patch4_window7_224] (main.py 398): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 17:10:13 swin_tiny_patch4_window7_224] (main.py 399): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 17:10:14 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 17:10:14 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 27498236
[2024-12-02 17:10:14 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.210060288
[2024-12-02 17:10:14 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 17:10:14 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 17:11:45 swin_tiny_patch4_window7_224] (main.py 398): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 17:11:45 swin_tiny_patch4_window7_224] (main.py 399): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 17:11:45 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 17:11:46 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 27498236
[2024-12-02 17:11:46 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.210060288
[2024-12-02 17:11:46 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 17:11:46 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 17:12:35 swin_tiny_patch4_window7_224] (main.py 398): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 17:12:35 swin_tiny_patch4_window7_224] (main.py 399): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 17:12:35 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 17:12:36 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 27498236
[2024-12-02 17:12:36 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.210060288
[2024-12-02 17:12:36 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 17:12:36 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 17:12:56 swin_tiny_patch4_window7_224] (main.py 398): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 17:12:56 swin_tiny_patch4_window7_224] (main.py 399): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 17:12:56 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 17:12:56 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 1192388
[2024-12-02 17:12:56 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.096084288
[2024-12-02 17:12:56 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 17:12:56 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 17:12:56 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][0/34]	eta 0:00:19 lr 0.000004	 wd 0.0001	time 0.5731 (0.5731)	loss 0.9096 (0.9096)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 17:13:01 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][10/34]	eta 0:00:11 lr 0.000004	 wd 0.0001	time 0.5762 (0.4947)	loss 1.0159 (1.0847)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-02 17:13:06 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][20/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 0.3876 (0.4625)	loss 0.9576 (1.0601)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-02 17:13:10 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][30/34]	eta 0:00:01 lr 0.000004	 wd 0.0001	time 0.4258 (0.4464)	loss 0.7830 (1.0552)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.461
[2024-12-02 17:13:11 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 0 training takes 0:00:15
[2024-12-02 17:13:12 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.269 
[2024-12-02 17:13:12 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-02 17:13:12 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.27%
[2024-12-02 17:13:13 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][0/34]	eta 0:00:28 lr 0.000004	 wd 0.0001	time 0.8394 (0.8394)	loss 1.1328 (1.1328)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.266
[2024-12-02 17:13:18 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4233 (0.5284)	loss 1.0429 (1.0148)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.312
[2024-12-02 17:13:40 swin_tiny_patch4_window7_224] (main.py 398): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 17:13:40 swin_tiny_patch4_window7_224] (main.py 399): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 17:13:40 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 17:13:41 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 27498236
[2024-12-02 17:13:41 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.210060288
[2024-12-02 17:13:41 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 17:13:41 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 17:33:11 swin_tiny_patch4_window7_224] (main.py 398): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 17:33:11 swin_tiny_patch4_window7_224] (main.py 399): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 17:33:11 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 17:33:11 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 5039132
[2024-12-02 17:33:11 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.130737024
[2024-12-02 17:33:11 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 17:33:11 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 17:33:28 swin_tiny_patch4_window7_224] (main.py 398): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 4
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 17:33:28 swin_tiny_patch4_window7_224] (main.py 399): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 17:33:28 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 17:33:29 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 2084144
[2024-12-02 17:33:29 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.141317952
[2024-12-02 17:33:29 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 17:33:29 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 17:33:29 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7061 (0.7061)	loss 1.2547 (1.2547)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.875
[2024-12-02 17:33:35 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.4928 (0.5659)	loss 1.0419 (1.0654)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 17:33:41 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.7172 (0.5895)	loss 1.0426 (1.0304)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.375
[2024-12-02 17:33:46 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5035 (0.5737)	loss 1.1294 (1.0199)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.617
[2024-12-02 17:33:48 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 0 training takes 0:00:19
[2024-12-02 17:33:49 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.667 
[2024-12-02 17:33:49 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 17:33:49 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:33:50 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][0/34]	eta 0:00:28 lr 0.000004	 wd 0.0001	time 0.8359 (0.8359)	loss 1.2139 (1.2139)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.734
[2024-12-02 17:33:56 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.5240 (0.6529)	loss 1.0400 (1.0804)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.375
[2024-12-02 17:34:02 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5255 (0.6080)	loss 1.1280 (1.0243)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.422
[2024-12-02 17:34:08 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.6289 (0.6070)	loss 1.0399 (1.0184)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-02 17:34:10 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 1 training takes 0:00:20
[2024-12-02 17:34:11 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.564 
[2024-12-02 17:34:11 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 17:34:11 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:34:11 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7209 (0.7209)	loss 0.7801 (0.7801)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.508
[2024-12-02 17:34:17 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.5275 (0.5373)	loss 0.8670 (0.9852)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:34:23 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5340 (0.5842)	loss 0.6933 (0.9907)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:34:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5451 (0.5612)	loss 1.0400 (1.0122)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-02 17:34:30 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 2 training takes 0:00:19
[2024-12-02 17:34:31 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.603 
[2024-12-02 17:34:31 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 17:34:31 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:34:32 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][0/34]	eta 0:00:31 lr 0.000004	 wd 0.0001	time 0.9175 (0.9175)	loss 0.8666 (0.8666)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 17:34:38 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.4968 (0.6295)	loss 0.9533 (1.0163)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-02 17:34:43 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5395 (0.5787)	loss 1.1267 (1.0028)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-02 17:34:49 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5380 (0.5980)	loss 0.7799 (1.0120)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 17:34:51 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 3 training takes 0:00:20
[2024-12-02 17:34:52 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.474 
[2024-12-02 17:34:52 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 17:34:52 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:34:53 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][0/34]	eta 0:00:28 lr 0.000004	 wd 0.0001	time 0.8382 (0.8382)	loss 1.1268 (1.1268)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.422
[2024-12-02 17:34:59 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.7097 (0.5788)	loss 0.7799 (1.0084)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.516
[2024-12-02 17:35:05 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5331 (0.5917)	loss 0.8665 (0.9904)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 17:35:10 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4887 (0.5659)	loss 0.9531 (0.9980)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:35:12 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 4 training takes 0:00:19
[2024-12-02 17:35:13 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.423 
[2024-12-02 17:35:13 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:35:13 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:35:14 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][0/34]	eta 0:00:33 lr 0.000004	 wd 0.0001	time 0.9864 (0.9864)	loss 0.8665 (0.8665)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 17:35:19 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5369 (0.5685)	loss 0.6065 (0.8981)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:35:25 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.7614 (0.5524)	loss 1.2998 (0.9904)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.773
[2024-12-02 17:35:31 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4946 (0.5691)	loss 1.1265 (1.0007)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:35:32 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 5 training takes 0:00:19
[2024-12-02 17:35:34 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.513 
[2024-12-02 17:35:34 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 17:35:34 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:35:34 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [6/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7218 (0.7218)	loss 1.0398 (1.0398)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 17:35:40 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [6/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.7362 (0.6297)	loss 0.8665 (0.9847)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 17:35:46 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [6/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.4963 (0.5872)	loss 0.8665 (0.9862)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:35:51 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [6/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5027 (0.5611)	loss 0.9532 (1.0091)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-02 17:35:53 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 6 training takes 0:00:19
[2024-12-02 17:35:55 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.449 
[2024-12-02 17:35:55 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:35:55 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:35:55 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [7/50][0/34]	eta 0:00:27 lr 0.000004	 wd 0.0001	time 0.8166 (0.8166)	loss 1.2132 (1.2132)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.359
[2024-12-02 17:36:01 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [7/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.5217 (0.5855)	loss 1.1264 (1.0083)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.734
[2024-12-02 17:36:07 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [7/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.7505 (0.5995)	loss 0.9532 (1.0151)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-02 17:36:12 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [7/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5074 (0.5745)	loss 1.2998 (1.0147)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.719
[2024-12-02 17:36:14 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 7 training takes 0:00:19
[2024-12-02 17:36:15 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.603 
[2024-12-02 17:36:15 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 17:36:15 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:36:16 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [8/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7174 (0.7174)	loss 0.7798 (0.7798)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.508
[2024-12-02 17:36:23 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [8/50][10/34]	eta 0:00:17 lr 0.000004	 wd 0.0001	time 0.6712 (0.7492)	loss 1.2997 (1.0555)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.555
[2024-12-02 17:36:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [8/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5314 (0.6425)	loss 1.0398 (1.0522)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.438
[2024-12-02 17:36:34 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [8/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7432 (0.6230)	loss 0.9531 (1.0286)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-02 17:36:36 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 8 training takes 0:00:21
[2024-12-02 17:36:38 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.372 
[2024-12-02 17:36:38 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:36:38 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:36:38 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [9/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7367 (0.7367)	loss 1.3864 (1.3864)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.375
[2024-12-02 17:36:44 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [9/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5145 (0.5437)	loss 1.0398 (1.0556)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 17:36:50 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [9/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5738 (0.5818)	loss 1.1264 (1.0192)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-02 17:36:55 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [9/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5293 (0.5624)	loss 1.2131 (1.0035)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.453
[2024-12-02 17:36:57 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 9 training takes 0:00:18
[2024-12-02 17:36:58 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.385 
[2024-12-02 17:36:58 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:36:58 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:36:59 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [10/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7263 (0.7263)	loss 1.2131 (1.2131)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:37:05 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [10/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.5383 (0.6510)	loss 1.1265 (1.0634)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.695
[2024-12-02 17:37:10 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [10/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.4884 (0.5821)	loss 0.9531 (1.0439)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-02 17:37:16 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [10/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.6521 (0.5909)	loss 0.6932 (1.0202)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:37:18 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 10 training takes 0:00:19
[2024-12-02 17:37:19 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.397 
[2024-12-02 17:37:19 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:37:19 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:37:20 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [11/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7377 (0.7377)	loss 0.7798 (0.7798)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.453
[2024-12-02 17:37:25 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [11/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.5132 (0.5319)	loss 1.2131 (0.9847)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-02 17:37:31 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [11/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5778 (0.5814)	loss 0.9531 (1.0027)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-02 17:37:36 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [11/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4994 (0.5619)	loss 1.0398 (1.0063)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 17:37:38 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 11 training takes 0:00:19
[2024-12-02 17:37:39 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.436 
[2024-12-02 17:37:39 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:37:39 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:37:40 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [12/50][0/34]	eta 0:00:34 lr 0.000004	 wd 0.0001	time 1.0059 (1.0059)	loss 0.8665 (0.8665)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 17:37:46 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [12/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.5456 (0.6555)	loss 1.0398 (0.9689)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:37:52 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [12/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.4878 (0.5896)	loss 1.0397 (0.9779)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-02 17:37:58 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [12/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5880 (0.5988)	loss 1.1264 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.695
[2024-12-02 17:37:59 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 12 training takes 0:00:20
[2024-12-02 17:38:01 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.526 
[2024-12-02 17:38:01 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 17:38:01 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:38:02 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [13/50][0/34]	eta 0:00:30 lr 0.000004	 wd 0.0001	time 0.9055 (0.9055)	loss 1.2997 (1.2997)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.336
[2024-12-02 17:38:07 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [13/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.7516 (0.5698)	loss 0.7798 (1.0634)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.492
[2024-12-02 17:38:13 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [13/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5333 (0.5838)	loss 0.9531 (1.0522)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:38:18 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [13/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5253 (0.5637)	loss 1.0398 (1.0202)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:38:20 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 13 training takes 0:00:19
[2024-12-02 17:38:22 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.423 
[2024-12-02 17:38:22 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:38:22 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:38:23 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [14/50][0/34]	eta 0:00:36 lr 0.000004	 wd 0.0001	time 1.0723 (1.0723)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-02 17:38:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [14/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.5886 (0.5971)	loss 1.0398 (0.9925)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.375
[2024-12-02 17:38:34 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [14/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.7492 (0.5881)	loss 1.1264 (1.0068)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.422
[2024-12-02 17:38:40 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [14/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5372 (0.5918)	loss 1.0398 (1.0006)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.719
[2024-12-02 17:38:42 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 14 training takes 0:00:19
[2024-12-02 17:38:43 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.513 
[2024-12-02 17:38:43 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 17:38:43 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:38:44 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [15/50][0/34]	eta 0:00:28 lr 0.000004	 wd 0.0001	time 0.8240 (0.8240)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-02 17:38:50 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [15/50][10/34]	eta 0:00:16 lr 0.000004	 wd 0.0001	time 0.5356 (0.6740)	loss 1.0398 (1.0240)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 17:38:56 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [15/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5435 (0.6073)	loss 0.8665 (0.9902)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-02 17:39:01 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [15/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7060 (0.5918)	loss 0.9531 (1.0174)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 17:39:03 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 15 training takes 0:00:20
[2024-12-02 17:39:04 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.474 
[2024-12-02 17:39:04 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 17:39:04 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:39:05 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [16/50][0/34]	eta 0:00:26 lr 0.000004	 wd 0.0001	time 0.7931 (0.7931)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-02 17:39:10 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [16/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5250 (0.5419)	loss 0.7798 (0.9925)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-02 17:39:17 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [16/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5328 (0.5848)	loss 1.0397 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-02 17:39:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [16/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4976 (0.5625)	loss 0.8665 (1.0118)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:39:23 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 16 training takes 0:00:19
[2024-12-02 17:39:25 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.500 
[2024-12-02 17:39:25 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 17:39:25 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:39:25 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [17/50][0/34]	eta 0:00:27 lr 0.000004	 wd 0.0001	time 0.8124 (0.8124)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 17:39:32 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [17/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.4972 (0.6354)	loss 1.2997 (0.9610)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:39:37 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [17/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5401 (0.5806)	loss 0.8665 (0.9944)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 17:39:43 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [17/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7139 (0.5920)	loss 0.9531 (1.0006)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-02 17:39:45 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 17 training takes 0:00:20
[2024-12-02 17:39:46 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.538 
[2024-12-02 17:39:46 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 17:39:46 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:39:47 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [18/50][0/34]	eta 0:00:27 lr 0.000004	 wd 0.0001	time 0.8040 (0.8040)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-02 17:39:52 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [18/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5614 (0.5521)	loss 1.2997 (0.9925)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.664
[2024-12-02 17:39:58 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [18/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.4953 (0.5730)	loss 1.1264 (0.9944)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.461
[2024-12-02 17:40:03 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [18/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5339 (0.5653)	loss 1.1264 (1.0006)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-02 17:40:05 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 18 training takes 0:00:19
[2024-12-02 17:40:06 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.436 
[2024-12-02 17:40:06 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:40:06 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:40:07 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [19/50][0/34]	eta 0:00:39 lr 0.000004	 wd 0.0001	time 1.1709 (1.1709)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.438
[2024-12-02 17:40:13 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [19/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.5033 (0.6437)	loss 1.0397 (0.9689)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 17:40:19 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [19/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5652 (0.5931)	loss 1.2997 (0.9944)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.391
[2024-12-02 17:40:25 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [19/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5245 (0.6018)	loss 0.9531 (1.0034)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-02 17:40:27 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 19 training takes 0:00:20
[2024-12-02 17:40:28 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.590 
[2024-12-02 17:40:28 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 17:40:28 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:40:29 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [20/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7280 (0.7280)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 17:40:34 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [20/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.7575 (0.6077)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-02 17:40:40 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [20/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5425 (0.5946)	loss 0.8664 (1.0067)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 17:40:45 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [20/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4985 (0.5699)	loss 1.2130 (1.0202)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 17:40:47 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 20 training takes 0:00:19
[2024-12-02 17:40:49 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.462 
[2024-12-02 17:40:49 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 17:40:49 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:40:50 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [21/50][0/34]	eta 0:00:28 lr 0.000004	 wd 0.0001	time 0.8408 (0.8408)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-02 17:40:55 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [21/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5648 (0.5682)	loss 1.1264 (0.9137)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.461
[2024-12-02 17:41:01 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [21/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.7524 (0.5790)	loss 0.8664 (0.9655)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:41:07 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [21/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5169 (0.5737)	loss 1.1264 (1.0174)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.461
[2024-12-02 17:41:09 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 21 training takes 0:00:19
[2024-12-02 17:41:10 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.679 
[2024-12-02 17:41:10 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 17:41:10 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-02 17:41:10 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [22/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7316 (0.7316)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-02 17:41:17 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [22/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.4894 (0.6347)	loss 1.2130 (0.9689)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 17:41:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [22/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5268 (0.5721)	loss 0.8664 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.516
[2024-12-02 17:41:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [22/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7835 (0.5813)	loss 1.2130 (1.0230)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-02 17:41:30 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 22 training takes 0:00:20
[2024-12-02 17:41:31 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.308 
[2024-12-02 17:41:31 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-02 17:41:31 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-02 17:41:32 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [23/50][0/34]	eta 0:00:26 lr 0.000004	 wd 0.0001	time 0.7718 (0.7718)	loss 0.8665 (0.8665)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-02 17:41:37 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [23/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5612 (0.5553)	loss 0.7798 (0.9767)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.492
[2024-12-02 17:41:43 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [23/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5674 (0.5908)	loss 1.1264 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-02 17:41:48 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [23/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5619 (0.5700)	loss 1.0398 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.625
[2024-12-02 17:41:50 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 23 training takes 0:00:19
[2024-12-02 17:41:51 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.564 
[2024-12-02 17:41:51 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 17:41:51 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-02 17:41:52 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [24/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7355 (0.7355)	loss 0.7798 (0.7798)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.508
[2024-12-02 17:41:58 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [24/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.5735 (0.6577)	loss 1.0397 (1.0004)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-02 17:42:04 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [24/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.7177 (0.6331)	loss 1.2130 (1.0109)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-02 17:42:11 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [24/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5494 (0.6281)	loss 0.9531 (1.0174)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 17:42:12 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 24 training takes 0:00:21
[2024-12-02 17:42:13 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.679 
[2024-12-02 17:42:13 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 17:42:13 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-02 17:42:14 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [25/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7596 (0.7596)	loss 1.2130 (1.2130)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.734
[2024-12-02 17:42:19 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [25/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.5822 (0.5357)	loss 1.1264 (1.0555)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.617
[2024-12-02 17:42:25 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [25/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.5503 (0.5709)	loss 1.1264 (1.0315)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.383
[2024-12-02 17:42:31 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [25/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5288 (0.5587)	loss 0.9531 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 17:42:32 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 25 training takes 0:00:18
[2024-12-02 17:42:34 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.513 
[2024-12-02 17:42:34 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 17:42:34 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-02 17:42:35 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [26/50][0/34]	eta 0:00:36 lr 0.000004	 wd 0.0001	time 1.0863 (1.0863)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-02 17:42:41 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [26/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5084 (0.5830)	loss 1.0397 (1.0003)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 17:42:46 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [26/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.6334 (0.5662)	loss 1.1264 (0.9655)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-02 17:42:52 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [26/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5555 (0.5915)	loss 1.1264 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-02 17:42:54 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 26 training takes 0:00:20
[2024-12-02 17:42:55 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.615 
[2024-12-02 17:42:55 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 17:42:55 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-02 17:42:56 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [27/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7274 (0.7274)	loss 0.6932 (0.6932)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:43:02 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [27/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.7346 (0.6442)	loss 1.1264 (0.9610)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.695
[2024-12-02 17:43:08 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [27/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5561 (0.5876)	loss 0.9531 (0.9985)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-02 17:43:13 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [27/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7392 (0.5751)	loss 1.2130 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 17:43:15 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 27 training takes 0:00:20
[2024-12-02 17:43:17 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.410 
[2024-12-02 17:43:17 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:43:17 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-02 17:43:17 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [28/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7575 (0.7575)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:43:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [28/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4718 (0.5379)	loss 0.5199 (1.0082)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-02 17:43:29 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [28/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.7259 (0.5780)	loss 1.1264 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.734
[2024-12-02 17:43:34 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [28/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4812 (0.5599)	loss 0.8665 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:43:36 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 28 training takes 0:00:19
[2024-12-02 17:43:37 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.615 
[2024-12-02 17:43:37 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 17:43:37 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-02 17:43:38 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [29/50][0/34]	eta 0:00:26 lr 0.000004	 wd 0.0001	time 0.7878 (0.7878)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 17:43:44 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [29/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4865 (0.6225)	loss 1.0397 (1.0240)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 17:43:49 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [29/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5493 (0.5769)	loss 1.0397 (0.9820)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 17:43:55 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [29/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7336 (0.5917)	loss 1.2130 (1.0174)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.641
[2024-12-02 17:43:57 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 29 training takes 0:00:20
[2024-12-02 17:43:58 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.385 
[2024-12-02 17:43:58 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:43:58 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-02 17:43:59 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [30/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7540 (0.7540)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 17:44:04 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [30/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5032 (0.5473)	loss 0.6065 (0.9373)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 17:44:10 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [30/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5412 (0.5814)	loss 1.0397 (0.9613)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 17:44:16 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [30/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5381 (0.5673)	loss 1.0398 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.250
[2024-12-02 17:44:17 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 30 training takes 0:00:19
[2024-12-02 17:44:19 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.718 
[2024-12-02 17:44:19 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 17:44:19 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 17:44:20 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [31/50][0/34]	eta 0:00:37 lr 0.000004	 wd 0.0001	time 1.1015 (1.1015)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-02 17:44:26 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [31/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.5426 (0.6502)	loss 1.2997 (1.0161)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:44:31 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [31/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5563 (0.5891)	loss 1.0397 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 17:44:37 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [31/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5050 (0.6035)	loss 0.8664 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-02 17:44:39 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 31 training takes 0:00:20
[2024-12-02 17:44:40 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.397 
[2024-12-02 17:44:40 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:44:40 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 17:44:41 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [32/50][0/34]	eta 0:00:27 lr 0.000004	 wd 0.0001	time 0.8096 (0.8096)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.430
[2024-12-02 17:44:46 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [32/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.7267 (0.5873)	loss 0.8664 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 17:44:52 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [32/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5392 (0.5858)	loss 1.0397 (0.9902)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 17:44:58 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [32/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5549 (0.5680)	loss 0.9531 (1.0118)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-02 17:44:59 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 32 training takes 0:00:19
[2024-12-02 17:45:01 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.551 
[2024-12-02 17:45:01 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 17:45:01 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 17:45:02 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [33/50][0/34]	eta 0:00:31 lr 0.000004	 wd 0.0001	time 0.9298 (0.9298)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:45:08 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [33/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5737 (0.5772)	loss 0.9531 (0.9925)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-02 17:45:13 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [33/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.7547 (0.5779)	loss 0.8664 (0.9943)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-02 17:45:19 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [33/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5097 (0.5729)	loss 0.7798 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:45:21 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 33 training takes 0:00:19
[2024-12-02 17:45:22 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.641 
[2024-12-02 17:45:22 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 17:45:22 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 17:45:23 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [34/50][0/34]	eta 0:00:27 lr 0.000004	 wd 0.0001	time 0.8123 (0.8123)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 17:45:29 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [34/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.5436 (0.6496)	loss 0.8664 (1.0555)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 17:45:34 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [34/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5019 (0.5973)	loss 0.9531 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.570
[2024-12-02 17:45:40 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [34/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7203 (0.5839)	loss 0.6931 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:45:42 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 34 training takes 0:00:20
[2024-12-02 17:45:43 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.436 
[2024-12-02 17:45:43 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:45:43 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 17:45:44 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [35/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7504 (0.7504)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.422
[2024-12-02 17:45:50 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [35/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5261 (0.5723)	loss 0.9531 (0.9846)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-02 17:45:56 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [35/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5383 (0.5938)	loss 1.0397 (0.9861)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 17:46:01 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [35/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5549 (0.5739)	loss 1.0397 (1.0174)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:46:03 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 35 training takes 0:00:19
[2024-12-02 17:46:04 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.333 
[2024-12-02 17:46:04 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-02 17:46:04 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 17:46:05 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [36/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7528 (0.7528)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.438
[2024-12-02 17:46:11 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [36/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.4974 (0.6365)	loss 1.1264 (1.0712)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-02 17:46:16 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [36/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5226 (0.5857)	loss 0.7798 (1.0232)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.508
[2024-12-02 17:46:23 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [36/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5045 (0.6015)	loss 0.9531 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:46:24 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 36 training takes 0:00:20
[2024-12-02 17:46:25 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.462 
[2024-12-02 17:46:25 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 17:46:25 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 17:46:26 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [37/50][0/34]	eta 0:00:27 lr 0.000004	 wd 0.0001	time 0.8017 (0.8017)	loss 0.6065 (0.6065)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.508
[2024-12-02 17:46:32 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [37/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5410 (0.5726)	loss 1.2130 (1.0003)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-02 17:46:38 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [37/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.4898 (0.5982)	loss 0.9531 (1.0067)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.430
[2024-12-02 17:46:43 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [37/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5321 (0.5702)	loss 1.2130 (1.0118)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.359
[2024-12-02 17:46:45 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 37 training takes 0:00:19
[2024-12-02 17:46:46 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.577 
[2024-12-02 17:46:46 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 17:46:46 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 17:46:47 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [38/50][0/34]	eta 0:00:36 lr 0.000004	 wd 0.0001	time 1.0612 (1.0612)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.625
[2024-12-02 17:46:53 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [38/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.5003 (0.5997)	loss 1.1264 (0.9925)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-02 17:46:58 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [38/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.5441 (0.5667)	loss 0.9531 (1.0026)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-02 17:47:04 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [38/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4994 (0.5825)	loss 0.7798 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-02 17:47:06 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 38 training takes 0:00:19
[2024-12-02 17:47:07 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.590 
[2024-12-02 17:47:07 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 17:47:07 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 17:47:07 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [39/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7206 (0.7206)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.453
[2024-12-02 17:47:13 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [39/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.7918 (0.5515)	loss 0.7798 (0.9452)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 17:47:19 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [39/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.5004 (0.5646)	loss 1.0397 (1.0067)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-02 17:47:24 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [39/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4965 (0.5422)	loss 1.0397 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:47:25 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 39 training takes 0:00:18
[2024-12-02 17:47:27 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.410 
[2024-12-02 17:47:27 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:47:27 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 17:47:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [40/50][0/34]	eta 0:00:36 lr 0.000004	 wd 0.0001	time 1.0844 (1.0844)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:47:33 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [40/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.5518 (0.5986)	loss 0.9531 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-02 17:47:39 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [40/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.7833 (0.5814)	loss 1.0397 (1.0356)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.375
[2024-12-02 17:47:45 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [40/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5499 (0.5875)	loss 0.7798 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 17:47:47 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 40 training takes 0:00:19
[2024-12-02 17:47:48 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.679 
[2024-12-02 17:47:48 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 17:47:48 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 17:47:49 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [41/50][0/34]	eta 0:00:26 lr 0.000004	 wd 0.0001	time 0.7692 (0.7692)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 17:47:55 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [41/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.5992 (0.6460)	loss 1.1264 (1.0240)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-02 17:48:00 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [41/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5552 (0.5902)	loss 0.8664 (0.9861)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.453
[2024-12-02 17:48:06 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [41/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.8078 (0.5842)	loss 1.0397 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:48:08 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 41 training takes 0:00:20
[2024-12-02 17:48:09 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.731 
[2024-12-02 17:48:09 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 17:48:09 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 17:48:10 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [42/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7518 (0.7518)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.617
[2024-12-02 17:48:15 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [42/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.5598 (0.5372)	loss 1.1264 (1.0082)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-02 17:48:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [42/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.6505 (0.5873)	loss 0.8664 (1.0026)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 17:48:27 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [42/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5426 (0.5669)	loss 0.9531 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 17:48:29 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 42 training takes 0:00:19
[2024-12-02 17:48:30 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.654 
[2024-12-02 17:48:30 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 17:48:30 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 17:48:31 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [43/50][0/34]	eta 0:00:26 lr 0.000004	 wd 0.0001	time 0.7791 (0.7791)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 17:48:37 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [43/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.4906 (0.6389)	loss 1.1264 (0.9767)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.617
[2024-12-02 17:48:42 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [43/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.4890 (0.5766)	loss 1.2130 (1.0191)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.781
[2024-12-02 17:48:48 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [43/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7941 (0.5964)	loss 1.1264 (1.0118)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.734
[2024-12-02 17:48:50 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 43 training takes 0:00:20
[2024-12-02 17:48:51 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.321 
[2024-12-02 17:48:51 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-02 17:48:51 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 17:50:53 swin_tiny_patch4_window7_224] (main.py 398): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 4
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-02 17:50:53 swin_tiny_patch4_window7_224] (main.py 399): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-02 17:50:53 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-02 17:50:53 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 2084144
[2024-12-02 17:50:53 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.141317952
[2024-12-02 17:50:53 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-02 17:50:53 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-02 17:50:54 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][0/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 0.6546 (0.6546)	loss 1.2547 (1.2547)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.875
[2024-12-02 17:51:00 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.6837 (0.6340)	loss 1.0419 (1.0654)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 17:51:05 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5005 (0.5800)	loss 1.0426 (1.0304)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.375
[2024-12-02 17:51:11 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.6784 (0.5577)	loss 1.1294 (1.0199)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.617
[2024-12-02 17:51:13 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 0 training takes 0:00:19
[2024-12-02 17:51:14 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.667 
[2024-12-02 17:51:14 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 17:51:14 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:51:15 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7417 (0.7417)	loss 1.2139 (1.2139)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.734
[2024-12-02 17:51:20 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4906 (0.5332)	loss 1.0400 (1.0804)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.375
[2024-12-02 17:51:26 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.7394 (0.5497)	loss 1.1280 (1.0243)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.422
[2024-12-02 17:51:31 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5615 (0.5496)	loss 1.0399 (1.0184)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-02 17:51:33 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 1 training takes 0:00:18
[2024-12-02 17:51:34 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.564 
[2024-12-02 17:51:34 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 17:51:34 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:51:35 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][0/34]	eta 0:00:26 lr 0.000004	 wd 0.0001	time 0.7887 (0.7887)	loss 0.7801 (0.7801)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.508
[2024-12-02 17:51:41 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.4903 (0.6532)	loss 0.8670 (0.9852)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:51:47 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5302 (0.5989)	loss 0.6933 (0.9907)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:51:53 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7688 (0.6078)	loss 1.0400 (1.0122)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-02 17:51:55 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 2 training takes 0:00:20
[2024-12-02 17:51:56 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.603 
[2024-12-02 17:51:56 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 17:51:56 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:51:57 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7234 (0.7234)	loss 0.8666 (0.8666)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 17:52:02 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5645 (0.5723)	loss 0.9533 (1.0163)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-02 17:52:09 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5184 (0.6056)	loss 1.1267 (1.0028)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-02 17:52:14 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4816 (0.5806)	loss 0.7799 (1.0120)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 17:52:15 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 3 training takes 0:00:19
[2024-12-02 17:52:17 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.474 
[2024-12-02 17:52:17 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 17:52:17 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:52:18 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][0/34]	eta 0:00:36 lr 0.000004	 wd 0.0001	time 1.0870 (1.0870)	loss 1.1268 (1.1268)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.422
[2024-12-02 17:52:24 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.5294 (0.6601)	loss 0.7799 (1.0084)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.516
[2024-12-02 17:52:29 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.4908 (0.5994)	loss 0.8665 (0.9904)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 17:52:36 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4880 (0.6133)	loss 0.9531 (0.9980)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:52:37 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 4 training takes 0:00:20
[2024-12-02 17:52:38 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.423 
[2024-12-02 17:52:38 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:52:38 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:52:39 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7355 (0.7355)	loss 0.8665 (0.8665)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 17:52:45 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.7929 (0.6041)	loss 0.6065 (0.8981)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:52:51 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5103 (0.5876)	loss 1.2998 (0.9904)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.773
[2024-12-02 17:52:56 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5592 (0.5748)	loss 1.1265 (1.0007)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:52:58 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 5 training takes 0:00:19
[2024-12-02 17:53:00 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.513 
[2024-12-02 17:53:00 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 17:53:00 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:53:01 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [6/50][0/34]	eta 0:00:28 lr 0.000004	 wd 0.0001	time 0.8461 (0.8461)	loss 1.0398 (1.0398)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 17:53:06 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [6/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.4887 (0.5787)	loss 0.8665 (0.9847)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 17:53:12 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [6/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.8328 (0.5901)	loss 0.8665 (0.9862)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:53:18 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [6/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5873 (0.5905)	loss 0.9532 (1.0091)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-02 17:53:20 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 6 training takes 0:00:19
[2024-12-02 17:53:21 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.449 
[2024-12-02 17:53:21 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:53:21 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:53:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [7/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7363 (0.7363)	loss 1.2132 (1.2132)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.359
[2024-12-02 17:53:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [7/50][10/34]	eta 0:00:16 lr 0.000004	 wd 0.0001	time 0.5588 (0.6800)	loss 1.1264 (1.0083)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.734
[2024-12-02 17:53:34 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [7/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5570 (0.6227)	loss 0.9532 (1.0151)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-02 17:53:40 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [7/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7426 (0.6218)	loss 1.2998 (1.0147)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.719
[2024-12-02 17:53:42 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 7 training takes 0:00:20
[2024-12-02 17:53:43 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.603 
[2024-12-02 17:53:43 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 17:53:43 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:53:44 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [8/50][0/34]	eta 0:00:40 lr 0.000004	 wd 0.0001	time 1.1955 (1.1955)	loss 0.7798 (0.7798)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.508
[2024-12-02 17:53:51 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [8/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.6328 (0.6642)	loss 1.2997 (1.0555)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.555
[2024-12-02 17:53:57 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [8/50][20/34]	eta 0:00:09 lr 0.000004	 wd 0.0001	time 0.5361 (0.6554)	loss 1.0398 (1.0522)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.438
[2024-12-02 17:54:02 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [8/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5161 (0.6167)	loss 0.9531 (1.0286)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-02 17:54:04 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 8 training takes 0:00:20
[2024-12-02 17:54:06 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.372 
[2024-12-02 17:54:06 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:54:06 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:54:07 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [9/50][0/34]	eta 0:00:38 lr 0.000004	 wd 0.0001	time 1.1372 (1.1372)	loss 1.3864 (1.3864)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.375
[2024-12-02 17:54:12 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [9/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.5138 (0.6022)	loss 1.0398 (1.0556)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 17:54:18 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [9/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.7745 (0.5878)	loss 1.1264 (1.0192)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-02 17:54:24 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [9/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5088 (0.5918)	loss 1.2131 (1.0035)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.453
[2024-12-02 17:54:26 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 9 training takes 0:00:20
[2024-12-02 17:54:27 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.385 
[2024-12-02 17:54:27 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:54:27 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:54:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [10/50][0/34]	eta 0:00:29 lr 0.000004	 wd 0.0001	time 0.8647 (0.8647)	loss 1.2131 (1.2131)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:54:34 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [10/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.7063 (0.6571)	loss 1.1265 (1.0634)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.695
[2024-12-02 17:54:40 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [10/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5791 (0.6084)	loss 0.9531 (1.0439)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-02 17:54:46 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [10/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.8409 (0.6084)	loss 0.6932 (1.0202)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:54:48 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 10 training takes 0:00:20
[2024-12-02 17:54:49 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.397 
[2024-12-02 17:54:49 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:54:49 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:54:50 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [11/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7431 (0.7431)	loss 0.7798 (0.7798)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.453
[2024-12-02 17:54:55 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [11/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5098 (0.5503)	loss 1.2131 (0.9847)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-02 17:55:01 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [11/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5187 (0.5917)	loss 0.9531 (1.0027)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-02 17:55:07 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [11/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5064 (0.5790)	loss 1.0398 (1.0063)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 17:55:09 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 11 training takes 0:00:19
[2024-12-02 17:55:10 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.436 
[2024-12-02 17:55:10 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:55:10 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:55:10 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [12/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7533 (0.7533)	loss 0.8665 (0.8665)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 17:55:17 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [12/50][10/34]	eta 0:00:16 lr 0.000004	 wd 0.0001	time 0.5250 (0.6810)	loss 1.0398 (0.9689)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:55:23 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [12/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5894 (0.6249)	loss 1.0397 (0.9779)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-02 17:55:29 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [12/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5057 (0.6343)	loss 1.1264 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.695
[2024-12-02 17:55:31 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 12 training takes 0:00:21
[2024-12-02 17:55:32 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.526 
[2024-12-02 17:55:32 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 17:55:32 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:55:33 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [13/50][0/34]	eta 0:00:28 lr 0.000004	 wd 0.0001	time 0.8454 (0.8454)	loss 1.2997 (1.2997)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.336
[2024-12-02 17:55:39 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [13/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.7315 (0.6086)	loss 0.7798 (1.0634)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.492
[2024-12-02 17:55:45 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [13/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.4945 (0.6060)	loss 0.9531 (1.0522)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:55:50 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [13/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5496 (0.5847)	loss 1.0398 (1.0202)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:55:53 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 13 training takes 0:00:20
[2024-12-02 17:55:54 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.423 
[2024-12-02 17:55:54 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:55:54 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:55:55 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [14/50][0/34]	eta 0:00:27 lr 0.000004	 wd 0.0001	time 0.7943 (0.7943)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-02 17:56:01 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [14/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.6042 (0.5821)	loss 1.0398 (0.9925)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.375
[2024-12-02 17:56:07 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [14/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.8323 (0.5929)	loss 1.1264 (1.0068)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.422
[2024-12-02 17:56:12 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [14/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5548 (0.5856)	loss 1.0398 (1.0006)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.719
[2024-12-02 17:56:14 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 14 training takes 0:00:19
[2024-12-02 17:56:15 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.513 
[2024-12-02 17:56:15 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 17:56:15 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:56:16 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [15/50][0/34]	eta 0:00:27 lr 0.000004	 wd 0.0001	time 0.8173 (0.8173)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-02 17:56:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [15/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.5384 (0.6644)	loss 1.0398 (1.0240)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 17:56:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [15/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.4905 (0.5935)	loss 0.8665 (0.9902)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-02 17:56:34 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [15/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7701 (0.5978)	loss 0.9531 (1.0174)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 17:56:36 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 15 training takes 0:00:20
[2024-12-02 17:56:37 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.474 
[2024-12-02 17:56:37 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 17:56:37 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:56:38 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [16/50][0/34]	eta 0:00:26 lr 0.000004	 wd 0.0001	time 0.7851 (0.7851)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-02 17:56:43 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [16/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5281 (0.5767)	loss 0.7798 (0.9925)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-02 17:56:50 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [16/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5425 (0.6159)	loss 1.0397 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-02 17:56:55 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [16/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5216 (0.5956)	loss 0.8665 (1.0118)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:56:57 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 16 training takes 0:00:20
[2024-12-02 17:56:58 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.500 
[2024-12-02 17:56:58 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 17:56:58 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:56:59 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [17/50][0/34]	eta 0:00:33 lr 0.000004	 wd 0.0001	time 0.9799 (0.9799)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 17:57:05 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [17/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.5704 (0.6353)	loss 1.2997 (0.9610)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:57:10 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [17/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5247 (0.5887)	loss 0.8665 (0.9944)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 17:57:17 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [17/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5311 (0.6015)	loss 0.9531 (1.0006)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-02 17:57:18 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 17 training takes 0:00:20
[2024-12-02 17:57:20 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.538 
[2024-12-02 17:57:20 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 17:57:20 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:57:20 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [18/50][0/34]	eta 0:00:27 lr 0.000004	 wd 0.0001	time 0.8168 (0.8168)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-02 17:57:26 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [18/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.7805 (0.6002)	loss 1.2997 (0.9925)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.664
[2024-12-02 17:57:32 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [18/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5616 (0.5893)	loss 1.1264 (0.9944)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.461
[2024-12-02 17:57:37 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [18/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5634 (0.5756)	loss 1.1264 (1.0006)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-02 17:57:40 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 18 training takes 0:00:20
[2024-12-02 17:57:41 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.436 
[2024-12-02 17:57:41 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 17:57:41 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:57:42 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [19/50][0/34]	eta 0:00:26 lr 0.000004	 wd 0.0001	time 0.7792 (0.7792)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.438
[2024-12-02 17:57:48 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [19/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.5247 (0.5895)	loss 1.0397 (0.9689)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 17:57:54 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [19/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.7401 (0.5923)	loss 1.2997 (0.9944)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.391
[2024-12-02 17:57:59 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [19/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5935 (0.5841)	loss 0.9531 (1.0034)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-02 17:58:01 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 19 training takes 0:00:19
[2024-12-02 17:58:02 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.590 
[2024-12-02 17:58:02 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 17:58:02 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:58:03 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [20/50][0/34]	eta 0:00:28 lr 0.000004	 wd 0.0001	time 0.8407 (0.8407)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 17:58:10 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [20/50][10/34]	eta 0:00:16 lr 0.000004	 wd 0.0001	time 0.5427 (0.7024)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-02 17:58:16 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [20/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.4900 (0.6204)	loss 0.8664 (1.0067)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 17:58:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [20/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7871 (0.6245)	loss 1.2130 (1.0202)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 17:58:23 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 20 training takes 0:00:20
[2024-12-02 17:58:25 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.462 
[2024-12-02 17:58:25 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 17:58:25 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-02 17:58:25 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [21/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7525 (0.7525)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-02 17:58:31 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [21/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4989 (0.5411)	loss 1.1264 (0.9137)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.461
[2024-12-02 17:58:37 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [21/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5491 (0.5969)	loss 0.8664 (0.9655)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 17:58:43 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [21/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5859 (0.5814)	loss 1.1264 (1.0174)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.461
[2024-12-02 17:58:44 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 21 training takes 0:00:19
[2024-12-02 17:58:46 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.679 
[2024-12-02 17:58:46 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 17:58:46 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-02 17:58:47 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [22/50][0/34]	eta 0:00:39 lr 0.000004	 wd 0.0001	time 1.1558 (1.1558)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-02 17:58:53 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [22/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.5620 (0.6585)	loss 1.2130 (0.9689)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 17:58:58 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [22/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5477 (0.6060)	loss 0.8664 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.516
[2024-12-02 17:59:05 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [22/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5589 (0.6181)	loss 1.2130 (1.0230)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-02 17:59:06 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 22 training takes 0:00:20
[2024-12-02 17:59:08 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.308 
[2024-12-02 17:59:08 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-02 17:59:08 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-02 17:59:08 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [23/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7595 (0.7595)	loss 0.8665 (0.8665)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-02 17:59:14 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [23/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.7848 (0.6174)	loss 0.7798 (0.9767)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.492
[2024-12-02 17:59:20 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [23/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5572 (0.6050)	loss 1.1264 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-02 17:59:26 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [23/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5396 (0.5802)	loss 1.0398 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.625
[2024-12-02 17:59:28 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 23 training takes 0:00:20
[2024-12-02 17:59:30 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.564 
[2024-12-02 17:59:30 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 17:59:30 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-02 17:59:31 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [24/50][0/34]	eta 0:00:35 lr 0.000004	 wd 0.0001	time 1.0535 (1.0535)	loss 0.7798 (0.7798)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.508
[2024-12-02 17:59:37 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [24/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4955 (0.6141)	loss 1.0397 (1.0004)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-02 17:59:43 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [24/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.8367 (0.6121)	loss 1.2130 (1.0109)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-02 17:59:48 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [24/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5159 (0.5909)	loss 0.9531 (1.0174)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 17:59:50 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 24 training takes 0:00:19
[2024-12-02 17:59:51 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.679 
[2024-12-02 17:59:51 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 17:59:51 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-02 17:59:52 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [25/50][0/34]	eta 0:00:27 lr 0.000004	 wd 0.0001	time 0.8228 (0.8228)	loss 1.2130 (1.2130)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.734
[2024-12-02 17:59:58 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [25/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.5727 (0.6491)	loss 1.1264 (1.0555)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.617
[2024-12-02 18:00:04 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [25/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5497 (0.6346)	loss 1.1264 (1.0315)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.383
[2024-12-02 18:00:11 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [25/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5676 (0.6451)	loss 0.9531 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 18:00:13 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 25 training takes 0:00:21
[2024-12-02 18:00:14 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.513 
[2024-12-02 18:00:14 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 18:00:14 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-02 18:00:15 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [26/50][0/34]	eta 0:00:27 lr 0.000004	 wd 0.0001	time 0.8033 (0.8033)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-02 18:00:20 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [26/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5391 (0.5564)	loss 1.0397 (1.0003)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 18:00:26 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [26/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5559 (0.5913)	loss 1.1264 (0.9655)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-02 18:00:32 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [26/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5586 (0.5723)	loss 1.1264 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-02 18:00:33 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 26 training takes 0:00:19
[2024-12-02 18:00:35 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.615 
[2024-12-02 18:00:35 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 18:00:35 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-02 18:00:36 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [27/50][0/34]	eta 0:00:40 lr 0.000004	 wd 0.0001	time 1.1996 (1.1996)	loss 0.6932 (0.6932)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 18:00:42 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [27/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.5598 (0.6114)	loss 1.1264 (0.9610)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.695
[2024-12-02 18:00:47 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [27/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.7384 (0.5871)	loss 0.9531 (0.9985)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-02 18:00:54 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [27/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5345 (0.5997)	loss 1.2130 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 18:00:55 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 27 training takes 0:00:20
[2024-12-02 18:00:57 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.410 
[2024-12-02 18:00:57 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 18:00:57 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-02 18:00:57 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [28/50][0/34]	eta 0:00:28 lr 0.000004	 wd 0.0001	time 0.8407 (0.8407)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 18:01:04 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [28/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.6577 (0.6326)	loss 0.5199 (1.0082)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-02 18:01:09 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [28/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5715 (0.5913)	loss 1.1264 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.734
[2024-12-02 18:01:15 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [28/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7270 (0.5868)	loss 0.8665 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 18:01:17 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 28 training takes 0:00:20
[2024-12-02 18:01:18 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.615 
[2024-12-02 18:01:18 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 18:01:18 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-02 18:01:19 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [29/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7575 (0.7575)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 18:01:24 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [29/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5519 (0.5641)	loss 1.0397 (1.0240)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 18:01:31 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [29/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5367 (0.6117)	loss 1.0397 (0.9820)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 18:01:37 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [29/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5150 (0.5906)	loss 1.2130 (1.0174)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.641
[2024-12-02 18:01:38 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 29 training takes 0:00:20
[2024-12-02 18:01:40 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.385 
[2024-12-02 18:01:40 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 18:01:40 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-02 18:01:40 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [30/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7267 (0.7267)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 18:01:47 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [30/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.5445 (0.6547)	loss 0.6065 (0.9373)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 18:01:52 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [30/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5101 (0.5846)	loss 1.0397 (0.9613)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 18:01:58 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [30/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5999 (0.6106)	loss 1.0398 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.250
[2024-12-02 18:02:00 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 30 training takes 0:00:20
[2024-12-02 18:02:01 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.718 
[2024-12-02 18:02:01 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 18:02:01 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 18:02:02 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [31/50][0/34]	eta 0:00:28 lr 0.000004	 wd 0.0001	time 0.8431 (0.8431)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-02 18:02:08 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [31/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.8182 (0.6296)	loss 1.2997 (1.0161)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 18:02:15 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [31/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5434 (0.6249)	loss 1.0397 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 18:02:20 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [31/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5358 (0.5959)	loss 0.8664 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-02 18:02:22 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 31 training takes 0:00:20
[2024-12-02 18:02:23 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.397 
[2024-12-02 18:02:23 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 18:02:23 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 18:02:25 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [32/50][0/34]	eta 0:00:35 lr 0.000004	 wd 0.0001	time 1.0316 (1.0316)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.430
[2024-12-02 18:02:30 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [32/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.5962 (0.5878)	loss 0.8664 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 18:02:36 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [32/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.8306 (0.5894)	loss 1.0397 (0.9902)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 18:02:42 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [32/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.6009 (0.5970)	loss 0.9531 (1.0118)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-02 18:02:44 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 32 training takes 0:00:20
[2024-12-02 18:02:45 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.551 
[2024-12-02 18:02:45 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 18:02:45 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 18:02:46 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [33/50][0/34]	eta 0:00:28 lr 0.000004	 wd 0.0001	time 0.8357 (0.8357)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 18:02:52 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [33/50][10/34]	eta 0:00:16 lr 0.000004	 wd 0.0001	time 0.5143 (0.6815)	loss 0.9531 (0.9925)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-02 18:02:58 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [33/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5568 (0.6160)	loss 0.8664 (0.9943)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-02 18:03:04 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [33/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7683 (0.6187)	loss 0.7798 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 18:03:06 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 33 training takes 0:00:20
[2024-12-02 18:03:07 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.641 
[2024-12-02 18:03:07 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 18:03:07 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 18:03:08 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [34/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7499 (0.7499)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 18:03:13 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [34/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.4890 (0.5547)	loss 0.8664 (1.0555)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 18:03:20 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [34/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.4961 (0.5986)	loss 0.9531 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.570
[2024-12-02 18:03:25 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [34/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4851 (0.5697)	loss 0.6931 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 18:03:26 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 34 training takes 0:00:19
[2024-12-02 18:03:27 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.436 
[2024-12-02 18:03:27 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 18:03:27 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 18:03:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [35/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7378 (0.7378)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.422
[2024-12-02 18:03:35 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [35/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.5541 (0.6595)	loss 0.9531 (0.9846)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-02 18:03:40 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [35/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5346 (0.5912)	loss 1.0397 (0.9861)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 18:03:46 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [35/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5067 (0.6055)	loss 1.0397 (1.0174)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 18:03:48 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 35 training takes 0:00:20
[2024-12-02 18:03:49 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.333 
[2024-12-02 18:03:49 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-02 18:03:49 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 18:03:50 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [36/50][0/34]	eta 0:00:27 lr 0.000004	 wd 0.0001	time 0.7991 (0.7991)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.438
[2024-12-02 18:03:55 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [36/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5385 (0.5610)	loss 1.1264 (1.0712)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-02 18:04:01 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [36/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5027 (0.5957)	loss 0.7798 (1.0232)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.508
[2024-12-02 18:04:07 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [36/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4912 (0.5709)	loss 0.9531 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 18:04:08 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 36 training takes 0:00:19
[2024-12-02 18:04:10 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.462 
[2024-12-02 18:04:10 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-02 18:04:10 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 18:04:11 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [37/50][0/34]	eta 0:00:38 lr 0.000004	 wd 0.0001	time 1.1442 (1.1442)	loss 0.6065 (0.6065)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.508
[2024-12-02 18:04:17 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [37/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.5052 (0.6156)	loss 1.2130 (1.0003)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-02 18:04:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [37/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4846 (0.5700)	loss 0.9531 (1.0067)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.430
[2024-12-02 18:04:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [37/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5057 (0.5889)	loss 1.2130 (1.0118)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.359
[2024-12-02 18:04:30 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 37 training takes 0:00:19
[2024-12-02 18:04:31 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.577 
[2024-12-02 18:04:31 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 18:04:31 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 18:04:32 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [38/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7458 (0.7458)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.625
[2024-12-02 18:04:38 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [38/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.8553 (0.6199)	loss 1.1264 (0.9925)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-02 18:04:44 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [38/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5692 (0.5942)	loss 0.9531 (1.0026)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-02 18:04:49 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [38/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5670 (0.5771)	loss 0.7798 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-02 18:04:51 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 38 training takes 0:00:20
[2024-12-02 18:04:53 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.590 
[2024-12-02 18:04:53 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-02 18:04:53 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 18:04:54 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [39/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7492 (0.7492)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.453
[2024-12-02 18:04:59 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [39/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5407 (0.5592)	loss 0.7798 (0.9452)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-02 18:05:05 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [39/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.8910 (0.6049)	loss 1.0397 (1.0067)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-02 18:05:12 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [39/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5189 (0.6186)	loss 1.0397 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 18:05:14 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 39 training takes 0:00:20
[2024-12-02 18:05:15 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.410 
[2024-12-02 18:05:15 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-02 18:05:15 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 18:05:16 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [40/50][0/34]	eta 0:00:25 lr 0.000004	 wd 0.0001	time 0.7609 (0.7609)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 18:05:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [40/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.4897 (0.6538)	loss 0.9531 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-02 18:05:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [40/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.6115 (0.6060)	loss 1.0397 (1.0356)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.375
[2024-12-02 18:05:34 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [40/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5176 (0.6126)	loss 0.7798 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 18:05:35 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 40 training takes 0:00:20
[2024-12-02 18:05:37 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.679 
[2024-12-02 18:05:37 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 18:05:37 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-02 18:05:37 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [41/50][0/34]	eta 0:00:26 lr 0.000004	 wd 0.0001	time 0.7778 (0.7778)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-02 18:05:43 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [41/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5196 (0.5649)	loss 1.1264 (1.0240)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-02 18:05:49 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [41/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.4990 (0.6027)	loss 0.8664 (0.9861)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.453
[2024-12-02 18:05:55 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [41/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5092 (0.5828)	loss 1.0397 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-02 18:05:56 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 41 training takes 0:00:19
[2024-12-02 18:05:58 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.731 
[2024-12-02 18:05:58 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 18:05:58 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 18:05:59 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [42/50][0/34]	eta 0:00:36 lr 0.000004	 wd 0.0001	time 1.0839 (1.0839)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.617
[2024-12-02 18:06:05 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [42/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.5261 (0.6023)	loss 1.1264 (1.0082)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-02 18:06:10 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [42/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.6132 (0.5772)	loss 0.8664 (1.0026)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 18:06:16 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [42/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.5455 (0.5912)	loss 0.9531 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-02 18:06:18 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 42 training takes 0:00:19
[2024-12-02 18:06:19 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.654 
[2024-12-02 18:06:19 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-02 18:06:19 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-02 18:06:20 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [43/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7320 (0.7320)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-02 18:06:26 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [43/50][10/34]	eta 0:00:15 lr 0.000004	 wd 0.0001	time 0.8100 (0.6515)	loss 1.1264 (0.9767)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.617
[2024-12-02 18:06:32 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [43/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.5507 (0.6057)	loss 1.2130 (1.0191)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.781
[2024-12-02 18:06:37 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [43/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7307 (0.5872)	loss 1.1264 (1.0118)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.734
[2024-12-02 18:06:40 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 43 training takes 0:00:20
[2024-12-02 18:06:41 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.321 
[2024-12-02 18:06:41 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-02 18:06:41 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:21:00 swin_tiny_patch4_window7_224] (main.py 398): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 4
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-07 10:21:00 swin_tiny_patch4_window7_224] (main.py 399): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-07 10:21:00 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-07 10:21:01 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 2084144
[2024-12-07 10:21:01 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.141317952
[2024-12-07 10:21:01 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-07 10:21:01 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-07 10:21:02 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][0/34]	eta 0:00:40 lr 0.000004	 wd 0.0001	time 1.2028 (1.2028)	loss 1.2547 (1.2547)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.875
[2024-12-07 10:21:07 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.4703 (0.5773)	loss 1.0419 (1.0654)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-07 10:21:12 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4690 (0.5243)	loss 1.0426 (1.0304)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.375
[2024-12-07 10:21:17 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4568 (0.5377)	loss 1.1294 (1.0199)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.617
[2024-12-07 10:21:19 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 0 training takes 0:00:18
[2024-12-07 10:21:20 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.667 
[2024-12-07 10:21:20 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-07 10:21:20 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:21:20 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6918 (0.6918)	loss 1.2139 (1.2139)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.734
[2024-12-07 10:21:25 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][10/34]	eta 0:00:11 lr 0.000004	 wd 0.0001	time 0.4643 (0.4943)	loss 1.0400 (1.0804)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.375
[2024-12-07 10:21:31 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4712 (0.5280)	loss 1.1280 (1.0243)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.422
[2024-12-07 10:21:36 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4726 (0.5096)	loss 1.0399 (1.0184)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-07 10:21:37 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 1 training takes 0:00:17
[2024-12-07 10:21:38 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.564 
[2024-12-07 10:21:38 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:21:38 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:21:39 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6811 (0.6811)	loss 0.7801 (0.7801)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.508
[2024-12-07 10:21:44 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.4564 (0.5793)	loss 0.8670 (0.9852)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:21:49 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4608 (0.5276)	loss 0.6933 (0.9907)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:21:54 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.6988 (0.5248)	loss 1.0400 (1.0122)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-07 10:21:56 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 2 training takes 0:00:18
[2024-12-07 10:21:57 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.603 
[2024-12-07 10:21:57 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:21:57 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:21:58 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][0/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 0.6742 (0.6742)	loss 0.8666 (0.8666)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:22:02 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][10/34]	eta 0:00:11 lr 0.000004	 wd 0.0001	time 0.4674 (0.4907)	loss 0.9533 (1.0163)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-07 10:22:08 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.7600 (0.5378)	loss 1.1267 (1.0028)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-07 10:22:14 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4517 (0.5431)	loss 0.7799 (1.0120)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-07 10:22:15 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 3 training takes 0:00:18
[2024-12-07 10:22:16 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.474 
[2024-12-07 10:22:16 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:22:16 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:22:17 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][0/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 0.6691 (0.6691)	loss 1.1268 (1.1268)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.422
[2024-12-07 10:22:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.6898 (0.5362)	loss 0.7799 (1.0084)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.516
[2024-12-07 10:22:27 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4574 (0.5280)	loss 0.8665 (0.9904)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:22:32 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4592 (0.5109)	loss 0.9531 (0.9980)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:22:34 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 4 training takes 0:00:17
[2024-12-07 10:22:35 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.423 
[2024-12-07 10:22:35 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:22:35 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:22:36 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][0/34]	eta 0:00:33 lr 0.000004	 wd 0.0001	time 0.9749 (0.9749)	loss 0.8665 (0.8665)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-07 10:22:41 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4538 (0.5414)	loss 0.6065 (0.8981)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:22:46 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4780 (0.5093)	loss 1.2998 (0.9904)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.773
[2024-12-07 10:22:51 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4669 (0.5279)	loss 1.1265 (1.0007)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:22:53 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 5 training takes 0:00:17
[2024-12-07 10:22:54 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.513 
[2024-12-07 10:22:54 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:22:54 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:22:55 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [6/50][0/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 0.6652 (0.6652)	loss 1.0398 (1.0398)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-07 10:22:59 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [6/50][10/34]	eta 0:00:11 lr 0.000004	 wd 0.0001	time 0.4530 (0.4865)	loss 0.8665 (0.9847)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-07 10:23:05 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [6/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4580 (0.5272)	loss 0.8665 (0.9862)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:23:10 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [6/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4617 (0.5093)	loss 0.9532 (1.0091)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-07 10:23:11 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 6 training takes 0:00:17
[2024-12-07 10:23:12 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.449 
[2024-12-07 10:23:12 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:23:12 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:23:13 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [7/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6853 (0.6853)	loss 1.2132 (1.2132)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.359
[2024-12-07 10:23:19 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [7/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4775 (0.5885)	loss 1.1264 (1.0083)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.734
[2024-12-07 10:23:23 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [7/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4716 (0.5333)	loss 0.9532 (1.0151)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-07 10:23:29 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [7/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.6876 (0.5352)	loss 1.2998 (1.0147)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.719
[2024-12-07 10:23:30 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 7 training takes 0:00:18
[2024-12-07 10:23:31 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.603 
[2024-12-07 10:23:31 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:23:31 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:23:32 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [8/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7274 (0.7274)	loss 0.7798 (0.7798)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.508
[2024-12-07 10:23:37 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [8/50][10/34]	eta 0:00:11 lr 0.000004	 wd 0.0001	time 0.4802 (0.4999)	loss 1.2997 (1.0555)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.555
[2024-12-07 10:23:43 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [8/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.6742 (0.5286)	loss 1.0398 (1.0522)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.438
[2024-12-07 10:23:47 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [8/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4695 (0.5117)	loss 0.9531 (1.0286)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-07 10:23:49 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 8 training takes 0:00:17
[2024-12-07 10:23:50 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.372 
[2024-12-07 10:23:50 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:23:50 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:23:50 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [9/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6858 (0.6858)	loss 1.3864 (1.3864)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.375
[2024-12-07 10:23:56 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [9/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.5758 (0.5872)	loss 1.0398 (1.0556)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-07 10:24:01 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [9/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4691 (0.5282)	loss 1.1264 (1.0192)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-07 10:24:06 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [9/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4783 (0.5097)	loss 1.2131 (1.0035)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.453
[2024-12-07 10:24:07 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 9 training takes 0:00:17
[2024-12-07 10:24:09 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.385 
[2024-12-07 10:24:09 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:24:09 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:24:10 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [10/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6809 (0.6809)	loss 1.2131 (1.2131)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:24:14 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [10/50][10/34]	eta 0:00:11 lr 0.000004	 wd 0.0001	time 0.4745 (0.4996)	loss 1.1265 (1.0634)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.695
[2024-12-07 10:24:19 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [10/50][20/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 0.6030 (0.4912)	loss 0.9531 (1.0439)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-07 10:24:25 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [10/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4450 (0.5112)	loss 0.6932 (1.0202)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:24:26 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 10 training takes 0:00:17
[2024-12-07 10:24:27 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.397 
[2024-12-07 10:24:27 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:24:27 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:24:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [11/50][0/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 0.6715 (0.6715)	loss 0.7798 (0.7798)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.453
[2024-12-07 10:24:33 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [11/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.7056 (0.5415)	loss 1.2131 (0.9847)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-07 10:24:38 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [11/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4575 (0.5362)	loss 0.9531 (1.0027)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-07 10:24:43 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [11/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4783 (0.5136)	loss 1.0398 (1.0063)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-07 10:24:45 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 11 training takes 0:00:17
[2024-12-07 10:24:46 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.436 
[2024-12-07 10:24:46 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:24:46 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:24:47 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [12/50][0/34]	eta 0:00:33 lr 0.000004	 wd 0.0001	time 0.9712 (0.9712)	loss 0.8665 (0.8665)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-07 10:24:52 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [12/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.4605 (0.5652)	loss 1.0398 (0.9689)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:24:57 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [12/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4897 (0.5198)	loss 1.0397 (0.9779)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-07 10:25:05 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [12/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4788 (0.6158)	loss 1.1264 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.695
[2024-12-07 10:25:06 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 12 training takes 0:00:20
[2024-12-07 10:25:07 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.526 
[2024-12-07 10:25:07 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:25:07 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:25:08 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [13/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6975 (0.6975)	loss 1.2997 (1.2997)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.336
[2024-12-07 10:25:13 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [13/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.6612 (0.5282)	loss 0.7798 (1.0634)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.492
[2024-12-07 10:25:19 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [13/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4873 (0.5343)	loss 0.9531 (1.0522)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:25:24 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [13/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4818 (0.5476)	loss 1.0398 (1.0202)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:25:26 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 13 training takes 0:00:18
[2024-12-07 10:25:27 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.423 
[2024-12-07 10:25:27 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:25:27 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:25:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [14/50][0/34]	eta 0:00:31 lr 0.000004	 wd 0.0001	time 0.9374 (0.9374)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-07 10:25:33 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [14/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.5263 (0.5226)	loss 1.0398 (0.9925)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.375
[2024-12-07 10:25:38 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [14/50][20/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 0.4736 (0.4983)	loss 1.1264 (1.0068)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.422
[2024-12-07 10:25:44 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [14/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4771 (0.5251)	loss 1.0398 (1.0006)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.719
[2024-12-07 10:25:45 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 14 training takes 0:00:17
[2024-12-07 10:25:46 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.513 
[2024-12-07 10:25:46 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:25:46 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:25:47 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [15/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.7045 (0.7045)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-07 10:25:52 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [15/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.6666 (0.5228)	loss 1.0398 (1.0240)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-07 10:25:58 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [15/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4551 (0.5391)	loss 0.8665 (0.9902)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-07 10:26:02 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [15/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4700 (0.5180)	loss 0.9531 (1.0174)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:26:04 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 15 training takes 0:00:17
[2024-12-07 10:26:05 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.474 
[2024-12-07 10:26:05 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:26:05 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:26:06 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [16/50][0/34]	eta 0:00:33 lr 0.000004	 wd 0.0001	time 0.9725 (0.9725)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-07 10:26:11 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [16/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.4537 (0.5711)	loss 0.7798 (0.9925)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-07 10:26:16 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [16/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4795 (0.5246)	loss 1.0397 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-07 10:26:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [16/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4664 (0.5411)	loss 0.8665 (1.0118)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:26:23 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 16 training takes 0:00:18
[2024-12-07 10:26:24 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.500 
[2024-12-07 10:26:24 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:26:24 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:26:25 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [17/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7120 (0.7120)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-07 10:26:30 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [17/50][10/34]	eta 0:00:11 lr 0.000004	 wd 0.0001	time 0.4726 (0.4943)	loss 1.2997 (0.9610)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:26:35 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [17/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4646 (0.5331)	loss 0.8665 (0.9944)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-07 10:26:40 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [17/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4577 (0.5118)	loss 0.9531 (1.0006)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-07 10:26:41 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 17 training takes 0:00:17
[2024-12-07 10:26:43 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.538 
[2024-12-07 10:26:43 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:26:43 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:26:43 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [18/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7256 (0.7256)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-07 10:26:49 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [18/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4792 (0.5902)	loss 1.2997 (0.9925)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.664
[2024-12-07 10:26:54 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [18/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4683 (0.5332)	loss 1.1264 (0.9944)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.461
[2024-12-07 10:26:59 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [18/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7081 (0.5269)	loss 1.1264 (1.0006)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-07 10:27:01 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 18 training takes 0:00:18
[2024-12-07 10:27:02 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.436 
[2024-12-07 10:27:02 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:27:02 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:27:03 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [19/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6835 (0.6835)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.438
[2024-12-07 10:27:07 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [19/50][10/34]	eta 0:00:11 lr 0.000004	 wd 0.0001	time 0.4534 (0.4891)	loss 1.0397 (0.9689)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-07 10:27:13 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [19/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.7011 (0.5127)	loss 1.2997 (0.9944)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.391
[2024-12-07 10:27:18 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [19/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4601 (0.5095)	loss 0.9531 (1.0034)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-07 10:27:19 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 19 training takes 0:00:17
[2024-12-07 10:27:20 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.590 
[2024-12-07 10:27:20 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:27:20 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:27:21 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [20/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6793 (0.6793)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-07 10:27:27 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [20/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.7004 (0.5819)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-07 10:27:31 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [20/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4534 (0.5289)	loss 0.8664 (1.0067)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-07 10:27:36 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [20/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4802 (0.5111)	loss 1.2130 (1.0202)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:27:38 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 20 training takes 0:00:17
[2024-12-07 10:27:39 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.462 
[2024-12-07 10:27:39 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:27:39 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:27:40 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [21/50][0/34]	eta 0:00:28 lr 0.000004	 wd 0.0001	time 0.8405 (0.8405)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-07 10:27:45 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [21/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4818 (0.5061)	loss 1.1264 (0.9137)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.461
[2024-12-07 10:27:50 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [21/50][20/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 0.4572 (0.4883)	loss 0.8664 (0.9655)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:27:55 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [21/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4635 (0.5161)	loss 1.1264 (1.0174)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.461
[2024-12-07 10:27:57 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 21 training takes 0:00:17
[2024-12-07 10:27:58 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.679 
[2024-12-07 10:27:58 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-07 10:27:58 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-07 10:27:58 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [22/50][0/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 0.6718 (0.6718)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-07 10:28:03 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [22/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.5854 (0.5002)	loss 1.2130 (0.9689)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:28:09 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [22/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4600 (0.5282)	loss 0.8664 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.516
[2024-12-07 10:28:14 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [22/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4792 (0.5082)	loss 1.2130 (1.0230)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-07 10:28:15 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 22 training takes 0:00:17
[2024-12-07 10:28:16 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.308 
[2024-12-07 10:28:16 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-07 10:28:16 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-07 10:28:17 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [23/50][0/34]	eta 0:00:30 lr 0.000004	 wd 0.0001	time 0.9098 (0.9098)	loss 0.8665 (0.8665)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-07 10:28:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [23/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4659 (0.5905)	loss 0.7798 (0.9767)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.492
[2024-12-07 10:28:27 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [23/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4643 (0.5344)	loss 1.1264 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-07 10:28:33 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [23/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4848 (0.5459)	loss 1.0398 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.625
[2024-12-07 10:28:35 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 23 training takes 0:00:18
[2024-12-07 10:28:36 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.564 
[2024-12-07 10:28:36 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:28:36 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-07 10:28:37 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [24/50][0/34]	eta 0:00:31 lr 0.000004	 wd 0.0001	time 0.9158 (0.9158)	loss 0.7798 (0.7798)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.508
[2024-12-07 10:28:42 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [24/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4611 (0.5160)	loss 1.0397 (1.0004)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-07 10:28:48 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [24/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4750 (0.5419)	loss 1.2130 (1.0109)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-07 10:28:52 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [24/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4629 (0.5183)	loss 0.9531 (1.0174)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:28:54 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 24 training takes 0:00:17
[2024-12-07 10:28:55 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.679 
[2024-12-07 10:28:55 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-07 10:28:55 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-07 10:28:55 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [25/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6930 (0.6930)	loss 1.2130 (1.2130)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.734
[2024-12-07 10:29:01 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [25/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4664 (0.5854)	loss 1.1264 (1.0555)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.617
[2024-12-07 10:29:06 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [25/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4829 (0.5327)	loss 1.1264 (1.0315)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.383
[2024-12-07 10:29:12 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [25/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.6806 (0.5432)	loss 0.9531 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:29:13 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 25 training takes 0:00:18
[2024-12-07 10:29:14 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.513 
[2024-12-07 10:29:14 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:29:14 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-07 10:29:15 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [26/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6798 (0.6798)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-07 10:29:20 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [26/50][10/34]	eta 0:00:11 lr 0.000004	 wd 0.0001	time 0.4662 (0.4995)	loss 1.0397 (1.0003)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-07 10:29:25 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [26/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4842 (0.5334)	loss 1.1264 (0.9655)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-07 10:29:30 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [26/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4666 (0.5134)	loss 1.1264 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-07 10:29:31 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 26 training takes 0:00:17
[2024-12-07 10:29:32 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.615 
[2024-12-07 10:29:32 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:29:32 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-07 10:29:33 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [27/50][0/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 0.6664 (0.6664)	loss 0.6932 (0.6932)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:29:39 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [27/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4726 (0.5910)	loss 1.1264 (0.9610)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.695
[2024-12-07 10:29:44 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [27/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4812 (0.5360)	loss 0.9531 (0.9985)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-07 10:29:49 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [27/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.6958 (0.5218)	loss 1.2130 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:29:51 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 27 training takes 0:00:18
[2024-12-07 10:29:52 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.410 
[2024-12-07 10:29:52 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:29:52 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-07 10:29:53 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [28/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6912 (0.6912)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:29:57 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [28/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4572 (0.5088)	loss 0.5199 (1.0082)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-07 10:30:03 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [28/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.7254 (0.5225)	loss 1.1264 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.734
[2024-12-07 10:30:08 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [28/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4716 (0.5219)	loss 0.8665 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:30:10 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 28 training takes 0:00:17
[2024-12-07 10:30:11 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.615 
[2024-12-07 10:30:11 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:30:11 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-07 10:30:11 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [29/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6829 (0.6829)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:30:17 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [29/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.6802 (0.6045)	loss 1.0397 (1.0240)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-07 10:30:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [29/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4699 (0.5448)	loss 1.0397 (0.9820)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-07 10:30:27 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [29/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4786 (0.5252)	loss 1.2130 (1.0174)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.641
[2024-12-07 10:30:29 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 29 training takes 0:00:18
[2024-12-07 10:30:30 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.385 
[2024-12-07 10:30:30 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:30:30 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.68%
[2024-12-07 10:30:31 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [30/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.7016 (0.7016)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-07 10:30:36 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [30/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.5091 (0.5022)	loss 0.6065 (0.9373)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-07 10:30:41 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [30/50][20/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 0.6494 (0.4949)	loss 1.0397 (0.9613)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-07 10:30:46 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [30/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4679 (0.5172)	loss 1.0398 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.250
[2024-12-07 10:30:48 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 30 training takes 0:00:17
[2024-12-07 10:30:49 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.718 
[2024-12-07 10:30:49 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-07 10:30:49 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-07 10:30:50 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [31/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6961 (0.6961)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-07 10:30:55 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [31/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.7333 (0.5639)	loss 1.2997 (1.0161)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:31:00 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [31/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4707 (0.5397)	loss 1.0397 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-07 10:31:05 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [31/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4695 (0.5175)	loss 0.8664 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-07 10:31:06 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 31 training takes 0:00:17
[2024-12-07 10:31:08 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.397 
[2024-12-07 10:31:08 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:31:08 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-07 10:31:09 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [32/50][0/34]	eta 0:00:33 lr 0.000004	 wd 0.0001	time 0.9931 (0.9931)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.430
[2024-12-07 10:31:14 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [32/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.4672 (0.5510)	loss 0.8664 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-07 10:31:19 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [32/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4923 (0.5125)	loss 1.0397 (0.9902)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-07 10:31:24 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [32/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4595 (0.5307)	loss 0.9531 (1.0118)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-07 10:31:26 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 32 training takes 0:00:17
[2024-12-07 10:31:27 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.551 
[2024-12-07 10:31:27 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:31:27 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-07 10:31:27 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [33/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6767 (0.6767)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:31:32 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [33/50][10/34]	eta 0:00:11 lr 0.000004	 wd 0.0001	time 0.4675 (0.4919)	loss 0.9531 (0.9925)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-07 10:31:38 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [33/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4626 (0.5323)	loss 0.8664 (0.9943)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-07 10:31:43 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [33/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4700 (0.5129)	loss 0.7798 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:31:44 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 33 training takes 0:00:17
[2024-12-07 10:31:45 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.641 
[2024-12-07 10:31:45 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:31:45 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-07 10:31:46 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [34/50][0/34]	eta 0:00:27 lr 0.000004	 wd 0.0001	time 0.7995 (0.7995)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-07 10:31:53 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [34/50][10/34]	eta 0:00:16 lr 0.000004	 wd 0.0001	time 0.6224 (0.6844)	loss 0.8664 (1.0555)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-07 10:31:57 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [34/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.4659 (0.5845)	loss 0.9531 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.570
[2024-12-07 10:32:03 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [34/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4626 (0.5797)	loss 0.6931 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:32:05 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 34 training takes 0:00:19
[2024-12-07 10:32:06 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.436 
[2024-12-07 10:32:06 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:32:06 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-07 10:32:06 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [35/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6774 (0.6774)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.422
[2024-12-07 10:32:11 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [35/50][10/34]	eta 0:00:11 lr 0.000004	 wd 0.0001	time 0.4602 (0.4941)	loss 0.9531 (0.9846)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-07 10:32:17 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [35/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4652 (0.5294)	loss 1.0397 (0.9861)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-07 10:32:21 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [35/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4683 (0.5115)	loss 1.0397 (1.0174)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:32:23 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 35 training takes 0:00:17
[2024-12-07 10:32:24 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.333 
[2024-12-07 10:32:24 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-07 10:32:24 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-07 10:32:25 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [36/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6834 (0.6834)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.438
[2024-12-07 10:32:30 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [36/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4771 (0.5865)	loss 1.1264 (1.0712)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-07 10:32:35 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [36/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4545 (0.5307)	loss 0.7798 (1.0232)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.508
[2024-12-07 10:32:41 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [36/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.6953 (0.5410)	loss 0.9531 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:32:42 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 36 training takes 0:00:18
[2024-12-07 10:32:43 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.462 
[2024-12-07 10:32:43 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:32:43 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-07 10:32:44 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [37/50][0/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 0.6739 (0.6739)	loss 0.6065 (0.6065)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.508
[2024-12-07 10:32:49 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [37/50][10/34]	eta 0:00:11 lr 0.000004	 wd 0.0001	time 0.4822 (0.4991)	loss 1.2130 (1.0003)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-07 10:32:55 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [37/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.5499 (0.5319)	loss 0.9531 (1.0067)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.430
[2024-12-07 10:32:59 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [37/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4686 (0.5119)	loss 1.2130 (1.0118)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.359
[2024-12-07 10:33:01 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 37 training takes 0:00:17
[2024-12-07 10:33:02 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.577 
[2024-12-07 10:33:02 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:33:02 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-07 10:33:02 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [38/50][0/34]	eta 0:00:22 lr 0.000004	 wd 0.0001	time 0.6704 (0.6704)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.625
[2024-12-07 10:33:08 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [38/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4686 (0.5866)	loss 1.1264 (0.9925)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-07 10:33:13 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [38/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4698 (0.5313)	loss 0.9531 (1.0026)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-07 10:33:18 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [38/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4796 (0.5123)	loss 0.7798 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-07 10:33:20 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 38 training takes 0:00:18
[2024-12-07 10:33:21 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.590 
[2024-12-07 10:33:21 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:33:21 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-07 10:33:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [39/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6938 (0.6938)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.453
[2024-12-07 10:33:27 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [39/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4842 (0.5135)	loss 0.7798 (0.9452)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-07 10:33:32 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [39/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.7147 (0.5174)	loss 1.0397 (1.0067)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-07 10:33:37 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [39/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4873 (0.5218)	loss 1.0397 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:33:39 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 39 training takes 0:00:17
[2024-12-07 10:33:40 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.410 
[2024-12-07 10:33:40 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:33:40 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-07 10:33:40 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [40/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6910 (0.6910)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:33:46 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [40/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.6904 (0.5673)	loss 0.9531 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-07 10:33:51 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [40/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4645 (0.5334)	loss 1.0397 (1.0356)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.375
[2024-12-07 10:33:56 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [40/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4778 (0.5123)	loss 0.7798 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-07 10:33:57 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 40 training takes 0:00:17
[2024-12-07 10:33:59 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.679 
[2024-12-07 10:33:59 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-07 10:33:59 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.72%
[2024-12-07 10:34:00 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [41/50][0/34]	eta 0:00:33 lr 0.000004	 wd 0.0001	time 0.9843 (0.9843)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-07 10:34:05 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [41/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.4955 (0.5508)	loss 1.1264 (1.0240)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-07 10:34:10 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [41/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4725 (0.5262)	loss 0.8664 (0.9861)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.453
[2024-12-07 10:34:15 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [41/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4713 (0.5410)	loss 1.0397 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:34:17 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 41 training takes 0:00:18
[2024-12-07 10:34:18 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.731 
[2024-12-07 10:34:18 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-07 10:34:18 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:34:19 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [42/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6792 (0.6792)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.617
[2024-12-07 10:34:24 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [42/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.6824 (0.5228)	loss 1.1264 (1.0082)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-07 10:34:29 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [42/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4662 (0.5331)	loss 0.8664 (1.0026)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:34:34 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [42/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4755 (0.5141)	loss 0.9531 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:34:35 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 42 training takes 0:00:17
[2024-12-07 10:34:36 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.654 
[2024-12-07 10:34:36 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-07 10:34:36 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:34:37 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [43/50][0/34]	eta 0:00:32 lr 0.000004	 wd 0.0001	time 0.9630 (0.9630)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-07 10:34:43 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [43/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.4631 (0.5717)	loss 1.1264 (0.9767)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.617
[2024-12-07 10:34:47 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [43/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4677 (0.5246)	loss 1.2130 (1.0191)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.781
[2024-12-07 10:34:53 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [43/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4809 (0.5407)	loss 1.1264 (1.0118)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.734
[2024-12-07 10:34:55 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 43 training takes 0:00:18
[2024-12-07 10:34:56 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.321 
[2024-12-07 10:34:56 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-07 10:34:56 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:37:07 swin_tiny_patch4_window7_224] (main.py 398): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: 'no'
  DATASET: imagenet
  DATA_PATH: data/ImageNet-Zip
  IMG_SIZE: 28
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: true
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 2
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 4
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 1
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 2
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 1
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 2
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 4.348145121276739e-06
  CLIP_GRAD: 5.0
  EPOCHS: 50
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.348145121276739e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 4.348145121276739e-06
  WEIGHT_DECAY: 0.0001

[2024-12-07 10:37:07 swin_tiny_patch4_window7_224] (main.py 399): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224.yaml", "opts": null, "batch_size": 16, "data_path": "data/ImageNet-Zip", "zip": true, "cache_mode": "no", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-12-07 10:37:07 swin_tiny_patch4_window7_224] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224
[2024-12-07 10:37:07 swin_tiny_patch4_window7_224] (main.py 98): INFO number of params: 2084144
[2024-12-07 10:37:07 swin_tiny_patch4_window7_224] (main.py 101): INFO number of GFLOPs: 0.141317952
[2024-12-07 10:37:07 swin_tiny_patch4_window7_224] (main.py 138): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/default, ignoring auto resume
[2024-12-07 10:37:07 swin_tiny_patch4_window7_224] (main.py 156): INFO Start training
[2024-12-07 10:37:08 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][0/34]	eta 0:00:28 lr 0.000004	 wd 0.0001	time 0.8305 (0.8305)	loss 0.9830 (0.9830)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-07 10:37:14 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.7643 (0.5883)	loss 1.3025 (1.0092)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.391
[2024-12-07 10:37:19 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4720 (0.5529)	loss 0.9546 (0.9969)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-07 10:37:24 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [0/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4743 (0.5290)	loss 1.2162 (1.0198)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-07 10:37:25 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 0 training takes 0:00:17
[2024-12-07 10:37:27 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.615 
[2024-12-07 10:37:27 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:37:27 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.62%
[2024-12-07 10:37:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][0/34]	eta 0:00:37 lr 0.000004	 wd 0.0001	time 1.1004 (1.1004)	loss 1.1272 (1.1272)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.695
[2024-12-07 10:37:33 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.4795 (0.5548)	loss 1.2135 (1.0565)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.312
[2024-12-07 10:37:38 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4795 (0.5194)	loss 1.2146 (1.0200)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:37:43 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [1/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4611 (0.5410)	loss 0.6936 (1.0099)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:37:45 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 1 training takes 0:00:18
[2024-12-07 10:37:46 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.526 
[2024-12-07 10:37:46 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:37:46 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.62%
[2024-12-07 10:37:47 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6810 (0.6810)	loss 1.0400 (1.0400)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-07 10:37:52 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4905 (0.5013)	loss 1.0404 (1.0483)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-07 10:37:57 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4697 (0.5370)	loss 1.0400 (1.0155)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-07 10:38:02 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [2/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4724 (0.5167)	loss 0.6937 (0.9926)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:38:04 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 2 training takes 0:00:17
[2024-12-07 10:38:05 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.628 
[2024-12-07 10:38:05 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:38:05 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.63%
[2024-12-07 10:38:05 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][0/34]	eta 0:00:30 lr 0.000004	 wd 0.0001	time 0.9022 (0.9022)	loss 1.2133 (1.2133)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.641
[2024-12-07 10:38:11 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4806 (0.5956)	loss 0.9533 (1.0794)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-07 10:38:16 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4682 (0.5355)	loss 0.9534 (1.0276)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:38:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [3/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7298 (0.5683)	loss 0.8665 (1.0176)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:38:24 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 3 training takes 0:00:19
[2024-12-07 10:38:25 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.308 
[2024-12-07 10:38:25 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-07 10:38:25 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.63%
[2024-12-07 10:38:26 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6975 (0.6975)	loss 1.0402 (1.0402)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-07 10:38:31 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][10/34]	eta 0:00:11 lr 0.000004	 wd 0.0001	time 0.4747 (0.4977)	loss 1.1265 (0.9690)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-07 10:38:37 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4761 (0.5384)	loss 0.9532 (0.9821)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:38:41 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [4/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4813 (0.5218)	loss 1.0398 (1.0175)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-07 10:38:43 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 4 training takes 0:00:17
[2024-12-07 10:38:44 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.385 
[2024-12-07 10:38:44 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:38:44 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.63%
[2024-12-07 10:38:45 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7140 (0.7140)	loss 0.7799 (0.7799)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.508
[2024-12-07 10:38:51 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4670 (0.5975)	loss 0.8665 (1.0635)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-07 10:38:55 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4773 (0.5390)	loss 1.0398 (1.0151)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-07 10:39:01 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [5/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7377 (0.5368)	loss 1.1265 (1.0091)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-07 10:39:02 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 5 training takes 0:00:18
[2024-12-07 10:39:04 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.513 
[2024-12-07 10:39:04 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:39:04 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.63%
[2024-12-07 10:39:04 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [6/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6803 (0.6803)	loss 1.0399 (1.0399)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-07 10:39:09 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [6/50][10/34]	eta 0:00:11 lr 0.000004	 wd 0.0001	time 0.4728 (0.4982)	loss 1.1266 (1.0477)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.383
[2024-12-07 10:39:15 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [6/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.6993 (0.5336)	loss 0.7799 (1.0316)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.508
[2024-12-07 10:39:20 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [6/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4694 (0.5191)	loss 0.8665 (1.0258)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-07 10:39:21 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 6 training takes 0:00:17
[2024-12-07 10:39:22 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.462 
[2024-12-07 10:39:22 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:39:22 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.63%
[2024-12-07 10:39:23 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [7/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6897 (0.6897)	loss 1.2998 (1.2998)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.336
[2024-12-07 10:39:29 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [7/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4950 (0.5914)	loss 0.7799 (0.9926)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-07 10:39:33 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [7/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4778 (0.5369)	loss 0.8665 (1.0109)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.516
[2024-12-07 10:39:38 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [7/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4874 (0.5197)	loss 1.0398 (1.0119)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-07 10:39:40 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 7 training takes 0:00:18
[2024-12-07 10:39:42 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.603 
[2024-12-07 10:39:42 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:39:42 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.63%
[2024-12-07 10:39:42 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [8/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.7056 (0.7056)	loss 1.0398 (1.0398)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-07 10:39:47 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [8/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4726 (0.5045)	loss 1.0398 (1.0241)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-07 10:39:52 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [8/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.6880 (0.5083)	loss 0.9531 (1.0357)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:39:58 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [8/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4756 (0.5192)	loss 0.7799 (1.0035)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.516
[2024-12-07 10:39:59 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 8 training takes 0:00:17
[2024-12-07 10:40:00 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.372 
[2024-12-07 10:40:00 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:40:00 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.63%
[2024-12-07 10:40:01 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [9/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6886 (0.6886)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:40:07 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [9/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.6932 (0.5636)	loss 1.0398 (1.0083)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-07 10:40:12 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [9/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4754 (0.5425)	loss 1.1264 (1.0109)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-07 10:40:17 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [9/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4763 (0.5211)	loss 1.0398 (1.0202)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:40:18 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 9 training takes 0:00:17
[2024-12-07 10:40:20 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.385 
[2024-12-07 10:40:20 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:40:20 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.63%
[2024-12-07 10:40:21 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [10/50][0/34]	eta 0:00:33 lr 0.000004	 wd 0.0001	time 0.9911 (0.9911)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-07 10:40:25 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [10/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4897 (0.5395)	loss 1.2131 (1.0004)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.734
[2024-12-07 10:40:30 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [10/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4703 (0.5085)	loss 1.1264 (1.0109)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.422
[2024-12-07 10:40:36 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [10/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4795 (0.5308)	loss 1.1264 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-07 10:40:37 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 10 training takes 0:00:17
[2024-12-07 10:40:39 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.449 
[2024-12-07 10:40:39 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:40:39 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.63%
[2024-12-07 10:40:39 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [11/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7144 (0.7144)	loss 0.8665 (0.8665)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-07 10:40:44 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [11/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4929 (0.5117)	loss 0.8665 (0.9925)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:40:50 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [11/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4853 (0.5438)	loss 0.8665 (1.0233)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.516
[2024-12-07 10:40:55 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [11/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4836 (0.5216)	loss 1.1264 (1.0118)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-07 10:40:56 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 11 training takes 0:00:17
[2024-12-07 10:40:57 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.474 
[2024-12-07 10:40:57 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:40:57 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.63%
[2024-12-07 10:40:58 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [12/50][0/34]	eta 0:00:34 lr 0.000004	 wd 0.0001	time 1.0035 (1.0035)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-07 10:41:04 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [12/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4665 (0.5932)	loss 0.8665 (0.9925)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.516
[2024-12-07 10:41:08 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [12/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4960 (0.5380)	loss 1.1264 (1.0026)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-07 10:41:14 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [12/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4709 (0.5526)	loss 1.2131 (1.0006)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-07 10:41:16 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 12 training takes 0:00:18
[2024-12-07 10:41:17 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.538 
[2024-12-07 10:41:17 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:41:17 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.63%
[2024-12-07 10:41:18 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [13/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.7058 (0.7058)	loss 1.3864 (1.3864)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.438
[2024-12-07 10:41:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [13/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4704 (0.5058)	loss 1.0397 (1.1028)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:41:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [13/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4749 (0.5407)	loss 0.8665 (1.0439)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:41:33 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [13/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4603 (0.5186)	loss 0.7798 (1.0118)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-07 10:41:34 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 13 training takes 0:00:17
[2024-12-07 10:41:36 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.423 
[2024-12-07 10:41:36 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:41:36 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.63%
[2024-12-07 10:41:37 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [14/50][0/34]	eta 0:00:34 lr 0.000004	 wd 0.0001	time 1.0194 (1.0194)	loss 0.8665 (0.8665)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-07 10:41:43 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [14/50][10/34]	eta 0:00:16 lr 0.000004	 wd 0.0001	time 0.4749 (0.6965)	loss 1.0397 (1.0319)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-07 10:41:48 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [14/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.4889 (0.5939)	loss 1.0397 (1.0356)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-07 10:41:53 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [14/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7119 (0.5776)	loss 0.9531 (1.0174)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:41:55 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 14 training takes 0:00:19
[2024-12-07 10:41:56 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.513 
[2024-12-07 10:41:56 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:41:56 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.63%
[2024-12-07 10:41:57 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [15/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.7046 (0.7046)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-07 10:42:02 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [15/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4901 (0.5038)	loss 1.2130 (1.0319)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.359
[2024-12-07 10:42:08 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [15/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.6992 (0.5385)	loss 1.0397 (1.0191)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-07 10:42:12 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [15/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4768 (0.5201)	loss 1.2131 (1.0174)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-07 10:42:14 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 15 training takes 0:00:17
[2024-12-07 10:42:15 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.615 
[2024-12-07 10:42:15 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:42:15 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.63%
[2024-12-07 10:42:16 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [16/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7100 (0.7100)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-07 10:42:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [16/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4987 (0.5952)	loss 1.2130 (1.0634)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.453
[2024-12-07 10:42:26 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [16/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4644 (0.5371)	loss 0.9531 (1.0026)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:42:31 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [16/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.6069 (0.5224)	loss 0.8665 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:42:33 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 16 training takes 0:00:18
[2024-12-07 10:42:35 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.564 
[2024-12-07 10:42:35 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:42:35 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.63%
[2024-12-07 10:42:35 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [17/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.7009 (0.7009)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-07 10:42:40 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [17/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4804 (0.5009)	loss 1.1264 (1.0161)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-07 10:42:45 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [17/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.7012 (0.5124)	loss 0.9532 (1.0274)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-07 10:42:51 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [17/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4847 (0.5197)	loss 1.1264 (1.0202)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.461
[2024-12-07 10:42:52 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 17 training takes 0:00:17
[2024-12-07 10:42:53 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.436 
[2024-12-07 10:42:53 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:42:53 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.63%
[2024-12-07 10:42:54 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [18/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6923 (0.6923)	loss 0.6932 (0.6932)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:42:59 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [18/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.7000 (0.5695)	loss 0.8665 (0.9689)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-07 10:43:04 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [18/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4765 (0.5360)	loss 1.1264 (0.9737)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-07 10:43:09 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [18/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4975 (0.5177)	loss 1.2998 (0.9978)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.555
[2024-12-07 10:43:11 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 18 training takes 0:00:17
[2024-12-07 10:43:12 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.474 
[2024-12-07 10:43:12 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:43:12 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.63%
[2024-12-07 10:43:13 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [19/50][0/34]	eta 0:00:33 lr 0.000004	 wd 0.0001	time 0.9934 (0.9934)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.570
[2024-12-07 10:43:18 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [19/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4682 (0.5286)	loss 0.9531 (1.0476)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-07 10:43:23 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [19/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4822 (0.5053)	loss 0.8665 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.438
[2024-12-07 10:43:29 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [19/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4728 (0.5282)	loss 0.8665 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-07 10:43:30 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 19 training takes 0:00:17
[2024-12-07 10:43:31 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.615 
[2024-12-07 10:43:31 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:43:31 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.63%
[2024-12-07 10:43:32 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [20/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7081 (0.7081)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-07 10:43:37 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [20/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.6675 (0.5227)	loss 0.7798 (1.0004)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-07 10:43:43 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [20/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4734 (0.5445)	loss 1.2131 (1.0521)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:43:48 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [20/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4909 (0.5231)	loss 1.2130 (1.0258)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:43:49 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 20 training takes 0:00:17
[2024-12-07 10:43:50 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.654 
[2024-12-07 10:43:50 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-07 10:43:50 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.65%
[2024-12-07 10:43:51 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [21/50][0/34]	eta 0:00:34 lr 0.000004	 wd 0.0001	time 1.0046 (1.0046)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-07 10:43:56 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [21/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.4548 (0.5653)	loss 0.6932 (0.9452)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:44:01 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [21/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4837 (0.5222)	loss 1.2130 (0.9861)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:44:07 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [21/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4689 (0.5390)	loss 1.0397 (0.9894)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-07 10:44:08 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 21 training takes 0:00:18
[2024-12-07 10:44:09 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.667 
[2024-12-07 10:44:09 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-07 10:44:09 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:44:10 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [22/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6840 (0.6840)	loss 0.8665 (0.8665)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:44:15 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [22/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4765 (0.5026)	loss 0.9531 (1.0082)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:44:21 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [22/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4843 (0.5397)	loss 1.2130 (1.0315)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:44:26 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [22/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4778 (0.5200)	loss 0.6065 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-07 10:44:27 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 22 training takes 0:00:17
[2024-12-07 10:44:28 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.269 
[2024-12-07 10:44:28 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-07 10:44:28 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:44:29 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [23/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6847 (0.6847)	loss 0.7799 (0.7799)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.461
[2024-12-07 10:44:35 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [23/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4947 (0.5959)	loss 1.1264 (1.0398)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.383
[2024-12-07 10:44:39 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [23/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4860 (0.5413)	loss 1.1264 (1.0398)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-07 10:44:45 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [23/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.6968 (0.5531)	loss 1.0397 (1.0202)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-07 10:44:47 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 23 training takes 0:00:18
[2024-12-07 10:44:48 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.474 
[2024-12-07 10:44:48 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:44:48 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.67%
[2024-12-07 10:44:48 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [24/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6939 (0.6939)	loss 1.0398 (1.0398)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-07 10:44:54 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [24/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.5134 (0.6003)	loss 1.1264 (1.0476)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-07 10:45:00 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [24/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.4641 (0.5888)	loss 0.8665 (1.0026)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-07 10:45:05 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [24/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4936 (0.5553)	loss 1.2997 (1.0202)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.555
[2024-12-07 10:45:06 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 24 training takes 0:00:18
[2024-12-07 10:45:08 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.731 
[2024-12-07 10:45:08 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-07 10:45:08 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:45:08 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [25/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.7037 (0.7037)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-07 10:45:14 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [25/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4753 (0.6002)	loss 0.8665 (1.0161)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-07 10:45:19 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [25/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4932 (0.5455)	loss 0.8664 (1.0232)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.453
[2024-12-07 10:45:25 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [25/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.6978 (0.5557)	loss 0.9531 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:45:26 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 25 training takes 0:00:18
[2024-12-07 10:45:27 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.333 
[2024-12-07 10:45:27 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-07 10:45:27 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:45:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [26/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7068 (0.7068)	loss 0.7798 (0.7798)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-07 10:45:33 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [26/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4800 (0.5030)	loss 0.9531 (0.9925)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-07 10:45:39 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [26/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4675 (0.5399)	loss 0.8665 (0.9943)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-07 10:45:43 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [26/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4721 (0.5209)	loss 1.0397 (1.0006)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-07 10:45:45 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 26 training takes 0:00:17
[2024-12-07 10:45:46 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.603 
[2024-12-07 10:45:46 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:45:46 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:45:47 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [27/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6935 (0.6935)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-07 10:45:53 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [27/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4774 (0.5971)	loss 0.7798 (1.0634)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:45:57 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [27/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4790 (0.5382)	loss 1.2130 (1.0232)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.641
[2024-12-07 10:46:03 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [27/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.7090 (0.5316)	loss 1.2130 (1.0174)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.453
[2024-12-07 10:46:05 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 27 training takes 0:00:18
[2024-12-07 10:46:06 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.590 
[2024-12-07 10:46:06 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:46:06 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:46:06 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [28/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6946 (0.6946)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-07 10:46:11 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [28/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4692 (0.5031)	loss 0.8664 (1.0634)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:46:17 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [28/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.7151 (0.5280)	loss 1.2130 (1.0562)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.734
[2024-12-07 10:46:22 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [28/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4816 (0.5184)	loss 0.7798 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-07 10:46:23 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 28 training takes 0:00:17
[2024-12-07 10:46:24 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.526 
[2024-12-07 10:46:24 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:46:24 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:46:25 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [29/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7252 (0.7252)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-07 10:46:31 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [29/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.5862 (0.6007)	loss 0.6932 (0.9925)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:46:36 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [29/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4688 (0.5408)	loss 0.8664 (0.9902)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.516
[2024-12-07 10:46:40 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [29/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4866 (0.5202)	loss 1.1264 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-07 10:46:42 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 29 training takes 0:00:18
[2024-12-07 10:46:44 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.462 
[2024-12-07 10:46:44 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:46:44 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:46:45 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [30/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7086 (0.7086)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:46:49 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [30/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4794 (0.5030)	loss 1.1264 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-07 10:46:54 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [30/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.6668 (0.5021)	loss 1.1264 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.461
[2024-12-07 10:47:00 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [30/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4942 (0.5211)	loss 1.1264 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-07 10:47:01 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 30 training takes 0:00:17
[2024-12-07 10:47:03 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.731 
[2024-12-07 10:47:03 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-07 10:47:03 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:47:03 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [31/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.7035 (0.7035)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.625
[2024-12-07 10:47:09 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [31/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.7089 (0.5515)	loss 1.1264 (1.0791)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-07 10:47:14 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [31/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.5094 (0.5406)	loss 1.1264 (1.0232)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-07 10:47:19 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [31/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4883 (0.5211)	loss 1.1264 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-07 10:47:20 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 31 training takes 0:00:17
[2024-12-07 10:47:22 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.269 
[2024-12-07 10:47:22 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-07 10:47:22 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:47:23 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [32/50][0/34]	eta 0:00:34 lr 0.000004	 wd 0.0001	time 1.0025 (1.0025)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.391
[2024-12-07 10:47:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [32/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4793 (0.5352)	loss 1.2130 (1.0555)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.359
[2024-12-07 10:47:32 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [32/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4879 (0.5067)	loss 1.2130 (1.0067)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:47:38 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [32/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4758 (0.5304)	loss 1.0397 (1.0034)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.438
[2024-12-07 10:47:40 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 32 training takes 0:00:17
[2024-12-07 10:47:41 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.603 
[2024-12-07 10:47:41 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:47:41 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:47:41 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [33/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6892 (0.6892)	loss 0.7798 (0.7798)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:47:46 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [33/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.5246 (0.5134)	loss 0.9531 (1.0319)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.570
[2024-12-07 10:47:52 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [33/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4865 (0.5400)	loss 1.1264 (0.9985)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:47:57 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [33/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4728 (0.5192)	loss 1.0397 (1.0006)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-07 10:47:58 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 33 training takes 0:00:17
[2024-12-07 10:47:59 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.551 
[2024-12-07 10:47:59 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:47:59 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:48:00 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [34/50][0/34]	eta 0:00:34 lr 0.000004	 wd 0.0001	time 1.0219 (1.0219)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-07 10:48:06 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [34/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4843 (0.5924)	loss 1.0397 (1.0634)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.625
[2024-12-07 10:48:12 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [34/50][20/34]	eta 0:00:08 lr 0.000004	 wd 0.0001	time 0.4745 (0.5862)	loss 1.0397 (1.0315)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.625
[2024-12-07 10:48:17 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [34/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4635 (0.5835)	loss 0.9531 (1.0286)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.453
[2024-12-07 10:48:19 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 34 training takes 0:00:19
[2024-12-07 10:48:20 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.449 
[2024-12-07 10:48:20 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:48:20 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:48:21 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [35/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7098 (0.7098)	loss 1.1264 (1.1264)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:48:26 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [35/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4763 (0.5076)	loss 1.0397 (1.0161)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:48:31 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [35/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4625 (0.5394)	loss 1.0397 (1.0026)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-07 10:48:36 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [35/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4741 (0.5200)	loss 1.0397 (0.9950)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-07 10:48:38 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 35 training takes 0:00:17
[2024-12-07 10:48:39 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.346 
[2024-12-07 10:48:39 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-07 10:48:39 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:48:40 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [36/50][0/34]	eta 0:00:30 lr 0.000004	 wd 0.0001	time 0.9082 (0.9082)	loss 1.2997 (1.2997)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.336
[2024-12-07 10:48:45 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [36/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4827 (0.5951)	loss 0.7798 (1.0319)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.469
[2024-12-07 10:48:50 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [36/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4765 (0.5387)	loss 1.1264 (1.0356)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.617
[2024-12-07 10:48:56 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [36/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4862 (0.5517)	loss 0.7798 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:48:57 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 36 training takes 0:00:18
[2024-12-07 10:48:58 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.333 
[2024-12-07 10:48:58 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-07 10:48:58 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:48:59 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [37/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.7006 (0.7006)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-07 10:49:04 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [37/50][10/34]	eta 0:00:11 lr 0.000004	 wd 0.0001	time 0.4924 (0.4992)	loss 1.2130 (0.9610)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.641
[2024-12-07 10:49:10 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [37/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4780 (0.5345)	loss 1.2997 (0.9820)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.555
[2024-12-07 10:49:14 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [37/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4834 (0.5157)	loss 0.8664 (1.0034)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-07 10:49:16 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 37 training takes 0:00:17
[2024-12-07 10:49:17 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.423 
[2024-12-07 10:49:17 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:49:17 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:49:18 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [38/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.7047 (0.7047)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-07 10:49:23 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [38/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4876 (0.5960)	loss 1.2997 (1.0240)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.445
[2024-12-07 10:49:28 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [38/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4640 (0.5389)	loss 1.0397 (1.0274)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-07 10:49:34 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [38/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.6769 (0.5420)	loss 0.7798 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-07 10:49:35 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 38 training takes 0:00:18
[2024-12-07 10:49:36 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.474 
[2024-12-07 10:49:36 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:49:36 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:49:37 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [39/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6946 (0.6946)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.438
[2024-12-07 10:49:42 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [39/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4671 (0.5027)	loss 0.8664 (1.0319)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-07 10:49:48 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [39/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.6872 (0.5407)	loss 0.9531 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.664
[2024-12-07 10:49:53 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [39/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4820 (0.5214)	loss 1.0397 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-07 10:49:54 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 39 training takes 0:00:17
[2024-12-07 10:49:55 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.397 
[2024-12-07 10:49:55 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:49:55 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:49:56 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [40/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.7006 (0.7006)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.453
[2024-12-07 10:50:02 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [40/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4846 (0.5941)	loss 1.1264 (0.9767)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-07 10:50:07 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [40/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4705 (0.5414)	loss 0.8664 (1.0150)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.484
[2024-12-07 10:50:11 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [40/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.6326 (0.5258)	loss 1.1264 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.422
[2024-12-07 10:50:14 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 40 training takes 0:00:18
[2024-12-07 10:50:15 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.705 
[2024-12-07 10:50:15 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-07 10:50:15 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:50:16 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [41/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.7020 (0.7020)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.523
[2024-12-07 10:50:20 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [41/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4641 (0.5010)	loss 1.0397 (1.0161)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.688
[2024-12-07 10:50:26 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [41/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.6932 (0.5140)	loss 0.7798 (1.0232)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.492
[2024-12-07 10:50:31 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [41/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4733 (0.5190)	loss 0.8664 (1.0230)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:50:32 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 41 training takes 0:00:17
[2024-12-07 10:50:33 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.731 
[2024-12-07 10:50:33 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-07 10:50:33 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:50:34 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [42/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7308 (0.7308)	loss 1.2130 (1.2130)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.781
[2024-12-07 10:50:40 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [42/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.7325 (0.5851)	loss 1.1264 (1.1342)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:50:45 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [42/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4840 (0.5409)	loss 0.8664 (1.0521)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:50:50 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [42/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4780 (0.5217)	loss 0.9531 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-07 10:50:51 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 42 training takes 0:00:17
[2024-12-07 10:50:53 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.731 
[2024-12-07 10:50:53 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-07 10:50:53 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:50:54 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [43/50][0/34]	eta 0:00:29 lr 0.000004	 wd 0.0001	time 0.8586 (0.8586)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.594
[2024-12-07 10:50:59 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [43/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4543 (0.5171)	loss 0.6932 (1.0161)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:51:03 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [43/50][20/34]	eta 0:00:06 lr 0.000004	 wd 0.0001	time 0.4683 (0.4985)	loss 0.8664 (1.0067)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.562
[2024-12-07 10:51:09 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [43/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4710 (0.5268)	loss 1.1264 (1.0230)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.656
[2024-12-07 10:51:11 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 43 training takes 0:00:17
[2024-12-07 10:51:12 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.333 
[2024-12-07 10:51:12 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.3%
[2024-12-07 10:51:12 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:51:12 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [44/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.6912 (0.6912)	loss 1.0397 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-07 10:51:18 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [44/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.6937 (0.5323)	loss 0.8664 (0.9767)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.516
[2024-12-07 10:51:23 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [44/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4790 (0.5413)	loss 0.9531 (0.9985)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:51:29 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [44/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4781 (0.5535)	loss 1.1264 (1.0202)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.422
[2024-12-07 10:51:31 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 44 training takes 0:00:18
[2024-12-07 10:51:32 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.615 
[2024-12-07 10:51:32 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:51:32 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:51:33 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [45/50][0/34]	eta 0:00:30 lr 0.000004	 wd 0.0001	time 0.9105 (0.9105)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.477
[2024-12-07 10:51:38 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [45/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4944 (0.5214)	loss 1.0397 (0.9373)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.344
[2024-12-07 10:51:43 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [45/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4724 (0.5018)	loss 0.9531 (0.9820)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.453
[2024-12-07 10:51:49 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [45/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4684 (0.5286)	loss 1.1264 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.383
[2024-12-07 10:51:50 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 45 training takes 0:00:17
[2024-12-07 10:51:51 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.615 
[2024-12-07 10:51:51 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:51:51 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:51:52 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [46/50][0/34]	eta 0:00:23 lr 0.000004	 wd 0.0001	time 0.7036 (0.7036)	loss 0.8664 (0.8664)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:51:57 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [46/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.6890 (0.5194)	loss 1.2130 (1.0003)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.500
[2024-12-07 10:52:02 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [46/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4656 (0.5372)	loss 0.7798 (0.9778)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.531
[2024-12-07 10:52:07 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [46/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4689 (0.5180)	loss 1.1264 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.461
[2024-12-07 10:52:09 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 46 training takes 0:00:17
[2024-12-07 10:52:10 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.423 
[2024-12-07 10:52:10 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.4%
[2024-12-07 10:52:10 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:52:11 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [47/50][0/34]	eta 0:00:34 lr 0.000004	 wd 0.0001	time 1.0190 (1.0190)	loss 1.2130 (1.2130)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:52:16 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [47/50][10/34]	eta 0:00:13 lr 0.000004	 wd 0.0001	time 0.5085 (0.5779)	loss 0.9531 (1.0397)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.453
[2024-12-07 10:52:21 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [47/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4888 (0.5298)	loss 1.1264 (1.0191)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-07 10:52:27 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [47/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4924 (0.5454)	loss 1.1264 (1.0090)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.578
[2024-12-07 10:52:28 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 47 training takes 0:00:18
[2024-12-07 10:52:29 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.628 
[2024-12-07 10:52:29 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.6%
[2024-12-07 10:52:29 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:52:30 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [48/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7287 (0.7287)	loss 1.2997 (1.2997)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.336
[2024-12-07 10:52:35 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [48/50][10/34]	eta 0:00:12 lr 0.000004	 wd 0.0001	time 0.4803 (0.5035)	loss 1.0397 (1.0476)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.406
[2024-12-07 10:52:41 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [48/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4867 (0.5406)	loss 1.1264 (0.9902)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.539
[2024-12-07 10:52:45 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [48/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.4866 (0.5205)	loss 1.1264 (1.0062)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.383
[2024-12-07 10:52:47 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 48 training takes 0:00:17
[2024-12-07 10:52:48 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.679 
[2024-12-07 10:52:48 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.7%
[2024-12-07 10:52:48 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:52:49 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [49/50][0/34]	eta 0:00:24 lr 0.000004	 wd 0.0001	time 0.7240 (0.7240)	loss 0.9531 (0.9531)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.570
[2024-12-07 10:52:54 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [49/50][10/34]	eta 0:00:14 lr 0.000004	 wd 0.0001	time 0.4748 (0.5898)	loss 1.2130 (0.9058)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.266
[2024-12-07 10:52:59 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [49/50][20/34]	eta 0:00:07 lr 0.000004	 wd 0.0001	time 0.4794 (0.5361)	loss 1.2997 (0.9820)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.172
[2024-12-07 10:53:05 swin_tiny_patch4_window7_224] (main.py 259): INFO Train: [49/50][30/34]	eta 0:00:02 lr 0.000004	 wd 0.0001	time 0.6549 (0.5502)	loss 1.2130 (1.0146)	grad_norm 0.000000 (0.0000)	loss_scale 0.0000 (0.0000)	accuracy training 0.547
[2024-12-07 10:53:06 swin_tiny_patch4_window7_224] (main.py 268): INFO EPOCH 49 training takes 0:00:18
[2024-12-07 10:53:08 swin_tiny_patch4_window7_224] (main.py 321): INFO  * Accuracy validation@ 0.538 
[2024-12-07 10:53:08 swin_tiny_patch4_window7_224] (main.py 169): INFO Accuracy of the network on the 78 test images: 0.5%
[2024-12-07 10:53:08 swin_tiny_patch4_window7_224] (main.py 171): INFO Max accuracy: 0.73%
[2024-12-07 10:53:08 swin_tiny_patch4_window7_224] (main.py 175): INFO Training time 0:16:00
